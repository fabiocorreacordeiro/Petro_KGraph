{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para filtrar o arquivo conllu em sentenças que contenham pelo menos duas URIS anotadas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from conllu import parse\n",
    "import pandas as pd\n",
    "from conllu import parse_incr\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def get_start(x, offset):\n",
    "    start = str(int(x[\"start_char\"]) - offset)\n",
    "    return start\n",
    "def get_end(x, offset):\n",
    "    end = str(int(x[\"end_char\"]) - offset)\n",
    "    return end\n",
    "def correct_start_end(df):\n",
    "    offset = int(df.iloc[0][\"misc\"].get(\"start_char\"))\n",
    "    df['start'] = df['misc'].apply(lambda x: get_start(x, offset))\n",
    "    df['end'] = df['misc'].apply(lambda x: get_end(x, offset))\n",
    "    return df\n",
    "\n",
    "def remove_null_tokens(df):#retira tokens nulos na sentenca\n",
    "    df = df[df['misc'].notna()]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df[df['deps'].notna()]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def ents_uris_counter_prefiltering(sentence,ents,uris): #contagem de entidades e uris pre-filtragem\n",
    "    entidades_sentenca_old, uris_sentenca_old = ents, uris\n",
    "    ents_list, uris_list = ents, uris\n",
    "    for idx_row in range(len(sentence)):\n",
    "        token = sentence.iloc[idx_row]['deps']\n",
    "        grafo = sentence.iloc[idx_row]['misc'].get('grafo')\n",
    "        if \"B=\" in token:\n",
    "            ents_list.append(token)\n",
    "        if grafo:\n",
    "            uris_list.append(grafo)     \n",
    "    return ents_list, uris_list\n",
    "\n",
    "def verify_sentence_is_functioning(df_sentence):\n",
    "    if df_sentence.iloc[0][\"misc\"].get('eliminar') == '':#retirar linha com eliminar\n",
    "        print('eliminada')\n",
    "        return 0\n",
    "    for idx_row in range(len(df_sentence)):\n",
    "        if df_sentence.iloc[idx_row]['misc'].get('start_char') == None:#problema com start_char\n",
    "            print('start_char com problema')\n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "def get_text_sentence(df):\n",
    "    size_sentence = int(df.iloc[-1][\"end\"])\n",
    "    #size_sentence = int(df_sentence.iloc[-1][\"end\"])\n",
    "    text = \" \"*size_sentence\n",
    "    #for index, row in df_sentence.iterrows():\n",
    "    for index, row in df.iterrows():\n",
    "        text = text[:int(row[\"start\"])] + row[\"form\"] +text[int(row[\"end\"]):]\n",
    "    return text \n",
    "\n",
    "def get_word_join(df, index):\n",
    "    entity = df.iloc[index]['deps']\n",
    "    entity_I = entity.replace(\"B=\",\"I=\")\n",
    "    count = 1\n",
    "    word_join = \"\"\n",
    "    row_main = df.iloc[index]\n",
    "    start_word, end_word = row_main['start'], row_main['end'] \n",
    "    word_join = \" \".join([word_join, row_main['form']])\n",
    "    while index+count != len(df) and (df.iloc[index+count][\"deps\"] == entity_I or check_I_entities(df, index+count,entity_I)):\n",
    "        row = df.iloc[index+count]\n",
    "        word_join = \" \".join([word_join, row['form']])\n",
    "        end_word = row['end']\n",
    "        count+=1\n",
    "    return word_join, start_word, end_word\n",
    "\n",
    "def check_I_entities(df, i,entity):\n",
    "    #verifica se ainda está dentro da entidade usando o sistema BIO de tokens\n",
    "    next_entity_is_I = (df.iloc[i][\"deps\"] == entity) or (df.iloc[i][\"deps\"] == None and df.iloc[i+1][\"deps\"] == entity)\n",
    "    return next_entity_is_I\n",
    "\n",
    "def check_sentence_for_pair_uris(df_sentence, ents, uris):\n",
    "    df_sent_new = pd.DataFrame()\n",
    "    ents_list_old, uris_list_old = ents, uris\n",
    "    ents_list, uris_list = ents, uris\n",
    "    \n",
    "    countURIs = 0\n",
    "    for idx_row in range(len(df_sentence)):\n",
    "        token = df_sentence.iloc[idx_row]['deps']\n",
    "        grafo = df_sentence.iloc[idx_row]['misc'].get('grafo')\n",
    "        if 'B=' in token and grafo:\n",
    "            countURIs+=1\n",
    "            ents_list = ents_list + [token]\n",
    "            uris_list = uris_list + [grafo]\n",
    "            \n",
    "            df_sentence.loc[idx_row,'grafo'] = grafo\n",
    "            df_sentence.loc[idx_row,'text'] = get_text_sentence(df_sentence)\n",
    "            word_join, start_word_join, end_word_join = get_word_join(df_sentence, idx_row)\n",
    "            df_sentence.loc[idx_row,'word_join'] = word_join\n",
    "            df_sentence.loc[idx_row,'word_join_start'] = start_word_join\n",
    "            df_sentence.loc[idx_row,'word_join_end'] = end_word_join\n",
    "            df_sentence.loc[idx_row,'index_e'] = int(idx_row)\n",
    "            \n",
    "            df_sent_new = df_sent_new.append(df_sentence.iloc[idx_row],ignore_index = True)\n",
    "#             raise SystemExit(\"Stop right there!\")   \n",
    "    if countURIs > 1:\n",
    "        return 1, df_sent_new, ents_list, uris_list\n",
    "    return 0, df_sentence, ents_list_old, uris_list_old,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ler conllu para filtrar sentencas que contenham pelo menos duas URIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in conllu -> 17987\n",
      "Total number of sentences in conllu -> 2467\n",
      "Total number of sentences in conllu -> 3581\n"
     ]
    }
   ],
   "source": [
    "#CONLLU_PATH = \"../PetroNER/petroner-uri-2023-06-20.conllu\"\n",
    "CONLLU_PATH_treino = \"../PetroNER/petroner-uri-treino.conllu\"\n",
    "CONLLU_PATH_valid = \"../PetroNER/petroner-uri-validação.conllu\"\n",
    "CONLLU_PATH_teste = \"../PetroNER/petroner-uri-teste.conllu\"\n",
    "\n",
    "def sentence_parser(CONLLU_PATH):\n",
    "    data_file = open(CONLLU_PATH, \"r\", encoding=\"utf-8\")\n",
    "    sentences=[]\n",
    "    for tokenlist in parse_incr(data_file):\n",
    "        sentences.append(tokenlist)\n",
    "    print('Total number of sentences in conllu ->',len(sentences))\n",
    "    return sentences\n",
    "    \n",
    "sentences_treino = sentence_parser(CONLLU_PATH_treino)\n",
    "sentences_valid = sentence_parser(CONLLU_PATH_valid)\n",
    "sentences_teste = sentence_parser(CONLLU_PATH_teste)\n",
    "    \n",
    "#algum problema nessas sentencas em diferentes petroner\n",
    "sentences_with_issues = [12408, 13636, 15264, 21023, 21122, 23920, 24017]\n",
    "# sentences_with_issues = [1749, 4747, 4935, 5066, 5235, 5798, 6802, 7858, 8881, 9271, 10689, 10691, 10695, 10750, \n",
    "#                          11144, 12408, 13393, 13636, 15264, 17494, 18462, 18476, 19346, 21023, 21105, 21122, \n",
    "#                         22163, 23920, 24017]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotina para contar entidades e URIs no conllu (é necessário rodar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def contar_URI(sentences, pickle_name, csv_name):\n",
    "    df_new = pd.DataFrame()\n",
    "    contSentences = 0\n",
    "    ents, uris = [], []\n",
    "    for idxSentence in range(0,len(sentences)):\n",
    "        #if idxSentence not in sentences_with_issues:\n",
    "        try:\n",
    "            #print('sentence = ' ,idxSentence)\n",
    "            sentence = sentences[idxSentence]\n",
    "            json_sent = json.dumps(sentence)\n",
    "            df_sentence = pd.read_json(json_sent)\n",
    "            df_sentence = remove_null_tokens(df_sentence) \n",
    "            if verify_sentence_is_functioning(df_sentence):\n",
    "                df_sentence = correct_start_end(df_sentence)    \n",
    "\n",
    "                checkQtdTokenURI, df_test, ents, uris = check_sentence_for_pair_uris(df_sentence,ents,uris)\n",
    "                if checkQtdTokenURI == 1:\n",
    "                    contSentences+= 1\n",
    "                    df_test['sentence'] = contSentences\n",
    "                    df_test['#sentence_original'] = idxSentence\n",
    "                    df_new = df_new.append(df_test)\n",
    "\n",
    "                    #print('sentences after filtering ->', contSentences)\n",
    "    #                 raise SystemExit(\"Stop right there!\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "\n",
    "    #pickle.dump(df_new, open('df_filtred_petroner_uri_2023_04_05.conllu.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #df_new.to_csv('df_filtred_petroner_uri_2023_04_05_conllu.csv',encoding = 'utf-8',index=False)\n",
    "\n",
    "    df_new.drop(columns=[\"start\",\"end\",\"head\",\"id\",\"feats\",\"upos\",\"lemma\",\"xpos\"], inplace=True)\n",
    "    \n",
    "    pickle.dump(df_new, open(pickle_name, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    df_new.to_csv(csv_name, encoding = 'utf-8',index=False)\n",
    "\n",
    "\n",
    "    #entidades é a lista de diferentes tipos de entidades que aparecem após filtragem das sentencas\n",
    "    entidades, numb_ents = np.unique(ents, return_counts = True)\n",
    "    #grafos é a lista de diferentes URIs anotadas que aparecem após filtragem das sentencas\n",
    "    grafos, numb_grafos = np.unique(uris, return_counts = True)\n",
    "    print('------------')\n",
    "    print('lista de diferentes tipos de entidades pós filtragem')\n",
    "    print(entidades.tolist())\n",
    "    print('------------')\n",
    "    print('quantidades de diferentes tipos de entidades pós filtragem')\n",
    "    print(numb_ents.tolist())\n",
    "    print('------------')\n",
    "    print('Total de URIs pós filtragem ->', sum(numb_grafos.tolist()))\n",
    "    \n",
    "\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "------------\n",
      "lista de diferentes tipos de entidades pós filtragem\n",
      "['B=BACIA', 'B=CAMPO', 'B=ELEMENTO_PETRO', 'B=ESTRUTURA_FÍSICA', 'B=EVENTO_PETRO', 'B=FLUIDO', 'B=FLUIDODATERRA_i', 'B=FLUIDODATERRA_o', 'B=NÃOCONSOLID', 'B=POÇO', 'B=POÇO_Q', 'B=POÇO_R', 'B=POÇO_T', 'B=ROCHA', 'B=TEXTURA', 'B=TIPO_POROSIDADE', 'B=UNIDADE_CRONO', 'B=UNIDADE_LITO']\n",
      "------------\n",
      "quantidades de diferentes tipos de entidades pós filtragem\n",
      "[2160, 441, 100, 1286, 134, 91, 756, 129, 563, 737, 8, 2, 32, 1708, 86, 7, 1640, 921]\n",
      "------------\n",
      "Total de URIs pós filtragem -> 10801\n"
     ]
    }
   ],
   "source": [
    "contar_URI(sentences_treino, 'df_filtred_petroner_uri_treino.conllu.pkl', 'df_filtred_petroner_uri_treino_conllu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "------------\n",
      "lista de diferentes tipos de entidades pós filtragem\n",
      "['B=BACIA', 'B=CAMPO', 'B=ELEMENTO_PETRO', 'B=ESTRUTURA_FÍSICA', 'B=EVENTO_PETRO', 'B=FLUIDO', 'B=FLUIDODATERRA_i', 'B=FLUIDODATERRA_o', 'B=NÃOCONSOLID', 'B=POÇO', 'B=POÇO_Q', 'B=POÇO_T', 'B=ROCHA', 'B=TEXTURA', 'B=TIPO_POROSIDADE', 'B=UNIDADE_CRONO', 'B=UNIDADE_LITO']\n",
      "------------\n",
      "quantidades de diferentes tipos de entidades pós filtragem\n",
      "[279, 41, 2, 105, 21, 8, 41, 8, 106, 98, 1, 1, 219, 13, 12, 283, 82]\n",
      "------------\n",
      "Total de URIs pós filtragem -> 1320\n"
     ]
    }
   ],
   "source": [
    "contar_URI(sentences_valid, 'df_filtred_petroner_uri_valid.conllu.pkl', 'df_filtred_petroner_uri_valid_conllu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "------------\n",
      "lista de diferentes tipos de entidades pós filtragem\n",
      "['B=BACIA', 'B=CAMPO', 'B=ELEMENTO_PETRO', 'B=ESTRUTURA_FÍSICA', 'B=EVENTO_PETRO', 'B=FLUIDO', 'B=FLUIDODATERRA_i', 'B=FLUIDODATERRA_o', 'B=NÃOCONSOLID', 'B=POÇO', 'B=POÇO_R', 'B=ROCHA', 'B=TEXTURA', 'B=UNIDADE_CRONO', 'B=UNIDADE_LITO']\n",
      "------------\n",
      "quantidades de diferentes tipos de entidades pós filtragem\n",
      "[564, 103, 92, 149, 69, 6, 233, 19, 117, 82, 1, 292, 4, 514, 266]\n",
      "------------\n",
      "Total de URIs pós filtragem -> 2511\n"
     ]
    }
   ],
   "source": [
    "contar_URI(sentences_teste, 'df_filtred_petroner_uri_teste.conllu.pkl', 'df_filtred_petroner_uri_teste_conllu.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotina para contar entidades e URIS no arquivo conllu (não é necessário rodar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "start_char com problema\n",
      "eliminada\n",
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "start_char com problema\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "eliminada\n",
      "------------\n",
      "lista de diferentes tipos de entidades no conllu\n",
      "['B=BACIA', 'B=CAMPO', 'B=ESTRUTURA_FÍSICA', 'B=EVENTO_PETRO', 'B=FLUIDO', 'B=FLUIDODATERRA_i', 'B=FLUIDODATERRA_o', 'B=NÃOCONSOLID', 'B=POÇO', 'B=POÇO_Q', 'B=POÇO_R', 'B=POÇO_T', 'B=ROCHA', 'B=TEXTURA', 'B=TIPO_POROSIDADE', 'B=UNIDADE_CRONO', 'B=UNIDADE_LITO']\n",
      "------------\n",
      "quantidades de diferentes tipos de entidades no conllu\n",
      "[3992, 702, 2029, 492, 174, 1372, 245, 1034, 1200, 9, 3, 35, 2768, 139, 22, 2906, 1486]\n",
      "------------\n",
      "Total de URIs no Conllu original -> 17941\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ents_pre = []\n",
    "uris_pre = []\n",
    "for idxGroups in range(0,len(sentences)):\n",
    "    if idxGroups not in sentences_with_issues: \n",
    "        #print('sentence = ' ,idxGroups)\n",
    "        sentence = sentences[idxGroups]\n",
    "        json_temp = json.dumps(sentence)\n",
    "        df_get_start_end = pd.read_json(json_temp)\n",
    "        df_get_start_end = remove_null_tokens(df_get_start_end) \n",
    "        if verify_sentence_is_functioning(df_get_start_end):\n",
    "            df_get_start_end = correct_start_end(df_get_start_end) \n",
    "        \n",
    "            ents_pre, uris_pre = ents_uris_counter_prefiltering(df_get_start_end,ents_pre,uris_pre)\n",
    "        \n",
    "entidades_pre, numb_ents_pre = np.unique(ents_pre, return_counts = True)\n",
    "grafos_pre, numb_grafos_pre = np.unique(uris_pre, return_counts = True)\n",
    "print('------------')\n",
    "print('lista de diferentes tipos de entidades no conllu')\n",
    "print(entidades_pre.tolist())\n",
    "print('------------')\n",
    "print('quantidades de diferentes tipos de entidades no conllu')\n",
    "print(numb_ents_pre.tolist())\n",
    "print('------------')\n",
    "print('Total de URIs no Conllu original ->',sum(numb_grafos_pre.tolist()))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
