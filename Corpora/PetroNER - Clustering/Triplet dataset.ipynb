{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5392406-540c-4768-82d6-8c4e96d12a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from owlready2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4247118c-963b-42fd-af23-c060d1090251",
   "metadata": {},
   "source": [
    "Load ontology and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced4ddb7-7efb-4943-88f3-61375dc6ae5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_ontology(\"http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OntoGeoLógica populada (OntoGeoLogicaPopulada.owl)\n",
    "onto = get_ontology(\"../../KnowledgeGraph/OntoGeoLogicaInstanciasRelacoes.owl\")\n",
    "onto.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b83a703-656e-4384-9142-2e18a79e7a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo OWL2Vec - \n",
    "#PetroOntoVec = gensim.models.Word2Vec.load(\"../../Embeddings/PetroOntoVec/Base/outputontology.embeddings\")\n",
    "PetroOntoVec = gensim.models.Word2Vec.load(\"../../Embeddings/PetroOntoVec/Petrovec-OeG_NP2/outputontology.embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0a49d-85b6-4ec4-9829-2c9995fe578f",
   "metadata": {},
   "source": [
    "### Carregando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af296cc0-bd22-412a-b5c5-3123f0e24343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas de treino\n",
    "text_treino = np.load('../PetroNER-LinkedEntity/treino - sentences.npy')\n",
    "entities_treino = np.load('../PetroNER-LinkedEntity/treino - entities.npy')\n",
    "classes_treino = np.load('../PetroNER-LinkedEntity/treino - classes.npy')\n",
    "URI_treino = np.load('../PetroNER-LinkedEntity/treino - URI.npy', allow_pickle=True)\n",
    "#URIvec_treino = np.load('../../Corpora/PetroNER-LinkedEntity/treino - URI_vectors.npy')\n",
    "\n",
    "# Listas de validação\n",
    "text_valid = np.load('../PetroNER-LinkedEntity/valid - sentences.npy')\n",
    "entities_valid = np.load('../PetroNER-LinkedEntity/valid - entities.npy')\n",
    "classes_valid = np.load('../PetroNER-LinkedEntity/valid - classes.npy')\n",
    "URI_valid = np.load('../PetroNER-LinkedEntity/valid - URI.npy', allow_pickle=True)\n",
    "#URIvec_valid = np.load('../../Corpora/PetroNER-LinkedEntity/valid - URI_vectors.npy')\n",
    "\n",
    "# Listas de teste\n",
    "text_teste = np.load('../PetroNER-LinkedEntity/teste - sentences.npy')\n",
    "entities_teste = np.load('../PetroNER-LinkedEntity/teste - entities.npy')\n",
    "classes_teste = np.load('../PetroNER-LinkedEntity/teste - classes.npy')\n",
    "URI_teste = np.load('../PetroNER-LinkedEntity/teste - URI.npy', allow_pickle=True)\n",
    "#URIvec_teste = np.load('../../Corpora/PetroNER-LinkedEntity/teste - URI_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a87f85bf-ef71-40be-8e8f-c8cb0744cb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino\n",
      "Texto:  14171\n",
      "Entidades:  14171\n",
      "Classes:  14171\n",
      "URI:  14171\n",
      "\n",
      " Validação\n",
      "Texto:  1775\n",
      "Entidades:  1775\n",
      "Classes:  1775\n",
      "URI:  1775\n",
      "\n",
      " Teste\n",
      "Texto:  3191\n",
      "Entidades:  3191\n",
      "Classes:  3191\n",
      "URI:  3191\n"
     ]
    }
   ],
   "source": [
    "print ('Treino')\n",
    "print('Texto: ', len(text_treino))\n",
    "print('Entidades: ', len(entities_treino))\n",
    "print('Classes: ', len(classes_treino))\n",
    "print('URI: ', len(URI_treino))\n",
    "#print('URIvec: :', len(URIvec_treino))\n",
    "\n",
    "print ('\\n Validação')\n",
    "print('Texto: ', len(text_valid))\n",
    "print('Entidades: ', len(entities_valid))\n",
    "print('Classes: ', len(classes_valid))\n",
    "print('URI: ', len(URI_valid))\n",
    "#print('URIvec: :', len(URIvec_valid))\n",
    "\n",
    "print ('\\n Teste')\n",
    "print('Texto: ', len(text_teste))\n",
    "print('Entidades: ', len(entities_teste))\n",
    "print('Classes: ', len(classes_teste))\n",
    "print('URI: ', len(URI_teste))\n",
    "#print('URIvec: :', len(URIvec_teste))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f72b9b-e81f-43fd-b5f1-c2727fdae9a9",
   "metadata": {},
   "source": [
    "### Buscando vetores do PetroOntoVec para cada URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85695dc5-db5f-4fd0-bc22-09c38f133e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscando_URI_vec(text, entities, classes_NER, ENT_URI):\n",
    "\n",
    "    # Separando as listas de sentenças, entidades, classes, URI e vetores\n",
    "    text_new = []\n",
    "    entities_new = []\n",
    "    classes_NER_new = [] \n",
    "    ENT_URI_new = []\n",
    "    URI_Vec_new = []\n",
    "\n",
    "    URI_ausente_Onto = []\n",
    "\n",
    "    n = 0 \n",
    "    #m = 0\n",
    "    #Iterando por todas as sentenças, desconsiderando aquelas cujas entidades não possuem URI\n",
    "    for i in range(len(text)):\n",
    "        n = n + 1\n",
    "        if ENT_URI[i] != None:\n",
    "            # Quebrando as entidades que possuem mais de um URI (ex: \"#Aptian,#Miocene\" e \"#Cenozoic,#Mesozoic\")\n",
    "            ENT_URI_n = ENT_URI[i].split(',#')\n",
    "\n",
    "            # Iterando pelas ENT_URI após serem quebradas e acrescentando '#\" naquelas que perderam na operação de split \n",
    "            for ent in ENT_URI_n:\n",
    "                if ent[0] != '#':\n",
    "                    ent = '#' + ent\n",
    "                try:\n",
    "                    #Acrescentar a URI da entidade e o vetor da URI nas listas\n",
    "                    URI_Vec_new.append(PetroOntoVec['http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2' + ent])\n",
    "                    ENT_URI_new.append(ent)\n",
    "\n",
    "                    # O texto, a entidade no texto e a classe são repetidas quando existem multiplas URI para uma mesma entidade\n",
    "                    text_new.append(text[i])\n",
    "                    entities_new.append(entities[i])\n",
    "                    classes_NER_new.append(classes_NER[i])\n",
    "                except:\n",
    "                    #print(\"URI presente no arquivo CONLLU mas ausente na Ontologia: \", ent)\n",
    "                    URI_ausente_Onto.append(ent)\n",
    "\n",
    "    # Transformando as listas em Numpy array\n",
    "    text = np.array(text_new)\n",
    "    entities = np.array(entities_new)\n",
    "    classes_NER = np.array(classes_NER_new) \n",
    "    ENT_URI = np.array(ENT_URI_new)\n",
    "    URI_Vec = np.array(URI_Vec_new)\n",
    "    URI_ausente_Onto = np.array(URI_ausente_Onto)\n",
    "\n",
    "    print('Total de sentenças anotadas: ', n)\n",
    "    print('N° de sentenças: ', len(text))\n",
    "    print ('N° de entidades: ', len(entities))\n",
    "    print('N° de classes: ', len(classes_NER))\n",
    "    print('N° de entidades_ID: ', len(ENT_URI))       \n",
    "    print('N° de vetores de URI: ', len(URI_Vec))       \n",
    "    \n",
    "    return(text, entities, classes_NER, ENT_URI, URI_Vec, URI_ausente_Onto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "863b7d63-2a58-4de4-959b-c4b491f916f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de sentenças anotadas:  14171\n",
      "N° de sentenças:  13465\n",
      "N° de entidades:  13465\n",
      "N° de classes:  13465\n",
      "N° de entidades_ID:  13465\n",
      "N° de vetores de URI:  13465\n",
      "Total de sentenças anotadas:  1775\n",
      "N° de sentenças:  1690\n",
      "N° de entidades:  1690\n",
      "N° de classes:  1690\n",
      "N° de entidades_ID:  1690\n",
      "N° de vetores de URI:  1690\n",
      "Total de sentenças anotadas:  3191\n",
      "N° de sentenças:  2997\n",
      "N° de entidades:  2997\n",
      "N° de classes:  2997\n",
      "N° de entidades_ID:  2997\n",
      "N° de vetores de URI:  2997\n"
     ]
    }
   ],
   "source": [
    "#Treino\n",
    "(text_treino,\n",
    " entities_treino,\n",
    " classes_treino,\n",
    " URI_treino,\n",
    " URIvec_treino,\n",
    " URI_ausente_Onto_treino) = buscando_URI_vec(text_treino,\n",
    "                                             entities_treino,\n",
    "                                             classes_treino,\n",
    "                                             URI_treino)\n",
    "\n",
    "#Validação\n",
    "(text_valid,\n",
    " entities_valid,\n",
    " classes_valid,\n",
    " URI_valid,\n",
    " URIvec_valid,\n",
    " URI_ausente_Onto_valid) = buscando_URI_vec(text_valid,\n",
    "                                            entities_valid,\n",
    "                                            classes_valid,\n",
    "                                            URI_valid)\n",
    "\n",
    "#Teste\n",
    "(text_teste,\n",
    " entities_teste,\n",
    " classes_teste,\n",
    " URI_teste,\n",
    " URIvec_teste,\n",
    " URI_ausente_Onto_teste) = buscando_URI_vec(text_teste,\n",
    "                                            entities_teste,\n",
    "                                            classes_teste,\n",
    "                                            URI_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c38ac84-4d11-4a31-a3d3-6320396e60c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI no PetroNER_Treino ausentes da ontologia: \n",
      "{'#inter_formacao_020', '#Rio_da_serra_Age', '#explotatory_well', '#INTER_BACIA_020', '#externo_textura_006', '#inter_grupo_002', '#porosidade_22', '#externo_textura_008', '#INTER_BACIA_008', '#EXTERNO_EST_FISICA_ROCHA_003', '#formation', '#appraisal_well', '#INTER_BACIA_021', '#INTER_BACIA_014', '#inter_membro_013', '#stratigraphic_well', '#special_well', '#EXTERNO_EST_FISICA_ROCHA_005', '#externo_formacao_003', '#externo_grupo_003', '#externo_formacao_004', '#porosidade_05', '#porosidade_24', '#INTER_BACIA_019', '#externo_grupo_005', '#INTER_BACIA_003', '#INTER_BACIA_015', '#INTER_BACIA_013', '#EXTERNO_EST_FISICA_ROCHA_006', '#inter_formacao_027', '#CAMP_CD_CAMPO_0817', '#inter_membro_011', '#inter_formacao_021', '#discovery_well', '#INTER_BACIA_005', '#grupo_05', '#inter_formacao_019', '#externo_textura_002', '#formation_fluid', '#generic_anthropogenic_fluid', '#dry_hole', '#inter_formacao_023', '#externo_membro_001', '#INTER_BACIA_011', '#deeper_prospect_well', '#geochronologic_age', '#supergrupo_001', '#inter_membro_010', '#EXTERNO_EST_FISICA_ROCHA_004', '#INTER_CAMPO_0001', '#inter_formacao_017', '#inter_membro_008', '#externo_grupo_004', '#externo_textura_004', '#INTER_BACIA_009', '#inter_membro_012', '#generic_earth_fluid', '#EXTERNO_EST_FISICA_ROCHA_007', '#INTER_BACIA_004', '#INTER_BACIA_010', '#INTER_BACIA_006', '#subcommercial_well', '#externo_formacao_001', '#externo_textura_005', '#INTER_BACIA_023', '#externo_textura_007', '#Neo-Alagoas_Subage', '#porosidade_21', '#EXTERNO_EST_FISICA_ROCHA_002', '#calcilutite', '#Permiano', '#INTER_BACIA_018', '#inter_formacao_028', '#externo_grupo_002', '#INTER_BACIA_017', '#INTER_BACIA_016', '#inter_formacao_018', '#wildcat_well', '#shallower_prospect_well', '#production_well', '#inter_formacao_024', '#INTER_BACIA_024', '#INTER_BACIA_025', '#INTER_BACIA_026', '#inter_grupo_001', '#INTER_BACIA_007', '#INTER_BACIA_022', '#EXTERNO_EST_FISICA_ROCHA_001', '#inter_membro_009'}\n",
      "\n",
      "URI no PetroNER_Validação ausentes da ontologia: \n",
      "{'#inter_formacao_015', '#externo_textura_005', '#porosidade_05', '#INTER_CAMPO_0002', '#porosidade_10', '#Lampasan_age', '#porosidade_21', '#INTER_CAMPO_003', '#Lampasan_Age', '#externo_textura_006', '#calcilutite', '#porosidade_18', '#Permiano', '#Bendian_Age', '#EXTERNO_EST_FISICA_ROCHA_004', '#wildcat_well', '#INTER_CAMPO_0001', '#INTER_BACIA_001', '#EXTERNO_EST_FISICA_ROCHA_003', '#formation', '#inter_formacao_022', '#generic_earth_fluid', '#EXTERNO_EST_FISICA_ROCHA_007', '#inter_formacao_016', '#generic_anthropogenic_fluid', '#externo_textura_001', '#dry_hole'}\n",
      "\n",
      "URI no PetroNER_Teste ausentes da ontologia: \n",
      "{'#inter_formacao_010', '#inter_membro_003', '#inter_membro_002', '#Rio_da_serra_Age', '#inter_membro_004', '#inter_formacao_003', '#inter_formacao_002', '#inter_formacao_001', '#INTER_BACIA_003', '#INTER_BACIA_015', '#INTER_BACIA_012', '#inter_membro_007', '#inter_formacao_013', '#calcilutite', '#INTER_BACIA_002', '#inter_formacao_021', '#inter_formacao_004', '#INTER_BACIA_017', '#INTER_BACIA_016', '#EXTERNO_EST_FISICA_ROCHA_004', '#Eoriodaserra_Age', '#inter_formacao_012', '#inter_formacao_025', '#CAMPO_GENERICO', '#geochronologic_epoch', '#inter_formacao_014', '#Mesoriodaserra_Age', '#production_well', '#grupo_05', '#inter_formacao_011', '#inter_formacao_026', '#inter_membro_005', '#formation', '#generic_earth_fluid', '#inter_formacao_007', '#inter_membro_006', '#inter_membro_001', '#inter_formacao_006', '#EXTERNO_EST_FISICA_ROCHA_007', '#inter_formacao_009', '#inter_formacao_008', '#INTER_BACIA_006', '#generic_anthropogenic_fluid', '#inter_formacao_005'}\n"
     ]
    }
   ],
   "source": [
    "print('URI no PetroNER_Treino ausentes da ontologia: ')\n",
    "print(set(URI_ausente_Onto_treino))\n",
    "print('')\n",
    "print('URI no PetroNER_Validação ausentes da ontologia: ')\n",
    "print(set(URI_ausente_Onto_valid))\n",
    "print('')\n",
    "print('URI no PetroNER_Teste ausentes da ontologia: ')\n",
    "print(set(URI_ausente_Onto_teste))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221b7ef-55fd-4a7e-8c9f-98346bceb0d4",
   "metadata": {},
   "source": [
    "### Preparando dataset para treinar rede Siamesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba642040-def9-43b9-b05f-a87dda6c64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe com as URI, sentenças e classes\n",
    "\n",
    "def dataset_siamesa(classes, URI, URIvec, text, x_easy, x_hard):\n",
    "       \n",
    "    df = pd.DataFrame({'Classe': classes, 'URI': URI, 'Sentença': text}) #, 'URIvec': URIvec})\n",
    "    \n",
    "    # Arrays para armazenar as sentenças âncoras, positivas e negativas \n",
    "    URI_anchor = np.array([])\n",
    "    URIvec_anchor = np.array([])\n",
    "    dataset_anchor = np.array([])\n",
    "    dataset_positive = np.array([])\n",
    "    dataset_negative = np.array([])\n",
    "\n",
    "    #Quantidade de exemplos negativos fáceis e difíceis\n",
    "    #x_easy = 20\n",
    "    #x_hard = 30\n",
    "\n",
    "    # iterando por todas as URI\n",
    "    for i in set(URI):\n",
    "\n",
    "        try:\n",
    "            \n",
    "            # Subgrupo apenas com a URI selecionada\n",
    "            df_URI = df[df['URI'] == i]\n",
    "\n",
    "            # Classe da URI selecionada\n",
    "            classe_da_URI = df_URI['Classe'].values[0]\n",
    "            # Vetor da URI selecionada\n",
    "            vetor_da_URI = URIvec[df_URI.index[0]]\n",
    "            # Subgrupo com sentenças negativa 'fácil' (URI diferente e classe diferente).\n",
    "            df_negative_easy = df[(df['URI'] != i) & (df['Classe'] != classe_da_URI)]\n",
    "            # Subgrupo com sentenças negativa 'difícil' (URI diferente mas da mesma classe).\n",
    "            df_negative_hard = df[(df['URI'] != i) & (df['Classe'] == classe_da_URI)]\n",
    "\n",
    "            # Pular os casos onde só existe uma sentença para a URI selecionada\n",
    "            if (len(df_URI) > 1) & (len(df_negative_easy) > 1) & (len(df_negative_hard) > 1):\n",
    "\n",
    "                ### Construindo triplets com exemplos negativos fáceis\n",
    "\n",
    "                # Selecionando as sentenças âncoras \n",
    "                # (verificamos o número de combinações possíveis entre sentenças com mesma URI \"len(df_URI)*(len(df_URI) - 1)\".\n",
    "                #  Se X_easy for maior que esse valor, amostramos apenas essa quantidade de amostras.\n",
    "                df_ancora = df_URI.sample(min(len(df_URI)*(len(df_URI) - 1) , x_easy), replace=True)\n",
    "\n",
    "                # itera pelas senteças âncoras\n",
    "                for row in df_ancora.iterrows():\n",
    "\n",
    "                    anchor = row[1]['Sentença']\n",
    "                    # Seleciona uma exemplo positivo diferente da sentença âncora\n",
    "                    positive = df_URI[df_URI['Sentença'] != anchor].sample(1)['Sentença'].values[0]\n",
    "\n",
    "                    # Seleciona uma sentença negativa\n",
    "                    negative_easy = df_negative_easy.sample(1)['Sentença'].values[0]\n",
    "\n",
    "                    # Concatena com as arrays que armazenam as senteças e URI\n",
    "                    URI_anchor = np.concatenate((URI_anchor, np.array([np.str_(i)])), axis=0)\n",
    "                    URIvec_anchor = np.concatenate((URIvec_anchor, vetor_da_URI), axis=0)\n",
    "                    dataset_anchor = np.concatenate((dataset_anchor, np.array([anchor])), axis=0)\n",
    "                    #dataset_anchor  = np.concatenate((dataset_anchor, anchor), axis=0)\n",
    "                    dataset_positive = np.concatenate((dataset_positive, np.array([positive])), axis=0)\n",
    "                    dataset_negative = np.concatenate((dataset_negative, np.array([negative_easy])), axis=0)\n",
    "\n",
    "                ### Construindo triplets com exemplos negativos difíceis\n",
    "\n",
    "                # Selecionando as sentenças âncoras \n",
    "                # (verificamos o número de combinações possíveis entre sentenças com mesma URI \"len(df_URI)*(len(df_URI) - 1)\".\n",
    "                #  Se X_hard for maior que esse valor, amostramos apenas essa quantidade de amostras.\n",
    "                \n",
    "                df_ancora = df_URI.sample(min(len(df_URI)*(len(df_URI) - 1), x_hard), replace=True)\n",
    "\n",
    "                # itera pelas senteças âncoras\n",
    "                for row in df_ancora.iterrows():\n",
    "                    anchor = row[1]['Sentença']\n",
    "\n",
    "                    # Seleciona uma exemplo positivo diferente da sentença âncora\n",
    "                    positive = df_URI[df_URI['Sentença'] != anchor].sample(1)['Sentença'].values[0]\n",
    "\n",
    "                    # Seleciona uma sentença negativa\n",
    "                    negative_hard = df_negative_hard.sample(1)['Sentença'].values[0]\n",
    "\n",
    "                    # Concatena com as arrays que armazenam as senteças\n",
    "                    URI_anchor = np.concatenate((URI_anchor, np.array([np.str_(i)])), axis=1)\n",
    "                    URIvec_anchor = np.concatenate((URIvec_anchor, vetor_da_URI), axis=0)\n",
    "                    dataset_anchor  = np.concatenate((dataset_anchor, np.array([anchor])), axis=0)\n",
    "                    #dataset_anchor  = np.concatenate((dataset_anchor, anchor), axis=0)\n",
    "                    dataset_positive  = np.concatenate((dataset_positive, np.array([positive])), axis=0)\n",
    "                    dataset_negative  = np.concatenate((dataset_negative, np.array([negative_hard])), axis=0)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #Ajustando o formato da array URIvec_anchor\n",
    "    URIvec_anchor = np.reshape(URIvec_anchor, (-1, len(URIvec[0])))\n",
    "    \n",
    "    return(URI_anchor, URIvec_anchor, dataset_anchor, dataset_positive, dataset_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e57fee6-b02e-4af5-b7c1-ac17f3cff98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(URI_anchor_treino,\n",
    " URIvec_anchor_treino,\n",
    " dataset_anchor_treino,\n",
    " dataset_positive_treino,\n",
    " dataset_negative_treino) = dataset_siamesa(classes_treino,\n",
    "                                            URI_treino,\n",
    "                                            URIvec_treino,\n",
    "                                            text_treino, 200, 200)\n",
    "\n",
    "(URI_anchor_valid, \n",
    " URIvec_anchor_valid,\n",
    " dataset_anchor_valid,\n",
    " dataset_positive_valid,\n",
    " dataset_negative_valid) = dataset_siamesa(classes_valid,\n",
    "                                           URI_valid,\n",
    "                                           URIvec_valid,\n",
    "                                           text_valid, 200, 200)\n",
    "\n",
    "(URI_anchor_teste,\n",
    " URIvec_anchor_teste,\n",
    " dataset_anchor_teste,\n",
    " dataset_positive_teste,\n",
    " dataset_negative_teste) = dataset_siamesa(classes_teste,\n",
    "                                           URI_teste,\n",
    "                                           URIvec_teste,   \n",
    "                                           text_teste, 200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73d04755-6eb6-46d9-ac39-d1374864e15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset treino:  34826\n",
      "Dataset validação:  9368\n",
      "Dataset teste:  13686\n"
     ]
    }
   ],
   "source": [
    "print('Dataset treino: ', len(URI_anchor_treino))\n",
    "print('Dataset validação: ', len(dataset_anchor_valid))\n",
    "print('Dataset teste: ', len(dataset_anchor_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c5ce99e-46ef-4642-a6a6-109f6c97ae29",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-2214c4b690cb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-2214c4b690cb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Dataset treino:  12448\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Dataset treino:  12448\n",
    "Dataset validação:  3282\n",
    "Dataset teste:  4948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6d7c4b4-5a02-4185-9d8b-825b1d771485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ UNIDADE_CRONO ] Paleoceno | Nota-se, também, a ocorrência de grandes discordâncias angulares na base da Formação Calumbi( Santoniano/Turoniano), e próximas ao topo do [ E ] Paleoceno [ / E ]..\n",
      "[ UNIDADE_CRONO ] Paleoceno | Apesar disso, na fase inícial da progradação, as paleobatimetrias aumentam até o [ E ] Paleoceno [ / E ], como resultado de uma subsidência que ainda continua maior do que o aporte sedimentar( fase final da subsidência térmica”..\n",
      "[ ROCHA ] arenito | Becker et al.( 1989) identificaram seis litofácies: conglomerado arenoso( Ca), [ E ] arenito [ / E ] conglomerático( Ac), arenito maciço( Am), interlaminado( 1), arenito estratificado( Ae) e arenito argiloso deformado( D)..\n",
      "#Paleocene\n",
      "[-0.28884128  0.29546309 -0.17015468  0.20688121 -0.34036124 -0.72685421\n",
      " -1.48428321 -0.85393631  0.03652421  0.46401012 -0.68416041 -0.768749\n",
      " -0.03291129  1.92501605  0.20827916 -0.10501655 -0.2687076   0.08026255\n",
      "  0.97931963 -0.37341329  0.43900031  0.22620183 -0.51032269  1.3114686\n",
      " -1.27340698  0.61767972  0.13469614  0.75422168  0.11758894  0.98666698\n",
      "  1.03829741  0.52202439 -0.8685531  -0.31458664  1.50167131  0.6359455\n",
      "  0.83441472  0.93792021  0.12272659  0.80019301  1.13913083 -1.18436217\n",
      "  0.07985532  0.36501867  0.01909667  0.78531832 -0.75025803 -1.28123903\n",
      " -0.14961603  0.2193858   0.77463764 -1.80367565 -0.41247076  1.10328436\n",
      " -1.03342128  0.34497851  0.32602888  0.93724614 -0.64549083 -1.38126183\n",
      " -0.37369815  0.83506083  0.83411378 -0.49405134  0.64720243 -0.09241151\n",
      " -0.7523123   0.2433389  -0.3550176  -0.15746318 -0.07686608 -0.20257808\n",
      "  1.69380546  2.02699041 -1.65274966  0.62882346  0.59677392 -0.7197659\n",
      "  0.51947743  0.50625849  0.07039969  1.04705787  0.61825413  1.71247125\n",
      "  0.64885598  0.63647079 -1.62128413  0.4648945  -1.07707763 -1.74347115\n",
      "  0.23399721 -0.06237499  0.28780288  0.23256932 -0.21042879 -0.32619542\n",
      " -0.92419976  1.14587331  0.46773097 -0.76740831]\n"
     ]
    }
   ],
   "source": [
    "n = 51\n",
    "\n",
    "print(dataset_anchor_treino[n])\n",
    "print(dataset_positive_treino[n])\n",
    "print(dataset_negative_treino[n])\n",
    "print(URI_anchor_treino[n])\n",
    "print(URIvec_anchor_treino[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b1136-76a6-46ea-934c-4ab5b85d46ea",
   "metadata": {},
   "source": [
    "### Salvando dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa474a98-d03a-40c6-be0b-1e6f4694c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treino\n",
    "with open('URI_anchor_treino.npy', 'wb') as f:\n",
    "    np.save(f, URI_anchor_treino)\n",
    "with open('URIvec_anchor_treino.npy', 'wb') as f:\n",
    "    np.save(f, URIvec_anchor_treino)\n",
    "with open('anchor_treino.npy', 'wb') as f:\n",
    "    np.save(f, dataset_anchor_treino)\n",
    "with open('positive_treino.npy', 'wb') as f:\n",
    "    np.save(f, dataset_positive_treino)\n",
    "with open('negative_treino.npy', 'wb') as f:\n",
    "    np.save(f, dataset_negative_treino)\n",
    "\n",
    "    \n",
    "#Validação\n",
    "with open('URI_anchor_valid.npy', 'wb') as f:\n",
    "    np.save(f, URI_anchor_valid)\n",
    "with open('URIvec_anchor_valid.npy', 'wb') as f:\n",
    "    np.save(f, URIvec_anchor_valid)\n",
    "with open('anchor_valid.npy', 'wb') as f:\n",
    "    np.save(f, dataset_anchor_valid)\n",
    "with open('positive_valid.npy', 'wb') as f:\n",
    "    np.save(f, dataset_positive_valid)\n",
    "with open('negative_valid.npy', 'wb') as f:\n",
    "    np.save(f, dataset_negative_valid)\n",
    "    \n",
    "#Teste\n",
    "with open('URI_anchor_teste.npy', 'wb') as f:\n",
    "    np.save(f, URI_anchor_teste)\n",
    "with open('URIvec_anchor_teste.npy', 'wb') as f:\n",
    "    np.save(f, URIvec_anchor_teste)\n",
    "with open('anchor_teste.npy', 'wb') as f:\n",
    "    np.save(f, dataset_anchor_teste)\n",
    "with open('positive_teste.npy', 'wb') as f:\n",
    "    np.save(f, dataset_positive_teste)\n",
    "with open('negative_teste.npy', 'wb') as f:\n",
    "    np.save(f, dataset_negative_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8337c-dbe9-4d6b-a2c6-3b4094dc4d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
