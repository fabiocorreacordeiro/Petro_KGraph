{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data='/share_gamma/nethome/cristian/projects/nlp/NER/distant_supervision/data_full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: stanza in /prj/parceirosbr/jose.castro2/.local/lib/python3.5/site-packages (0.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2\r\n"
     ]
    }
   ],
   "source": [
    "# stanza.download('pt')\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/parceirosbr/bigoilict/share/NLP/tokenizer\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"main.py\", line 1, in <module>\r\n",
      "    from inf_stanza.inference import stanza_inference\r\n",
      "  File \"/scratch/parceirosbr/bigoilict/share/NLP/tokenizer/inf_stanza/inference.py\", line 174\r\n",
      "    doc.MISC: f'start_char={start_char}|end_char={end_char}'}\r\n",
      "                                                           ^\r\n",
      "SyntaxError: invalid syntax\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py --idir '../../NLP/Datasets/400_tesis/Tokenization_jose_11_01_21/Tornando_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import stanza\n",
    "from stanza import Pipeline\n",
    "from stanza.utils.conll import CoNLL\n",
    "from utils.stanza_utils import list_to_conllu_text, text_preprocessing, portuguese_sentenciation\n",
    "from stanza.models.common import doc\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_pattern = r'''al\\.|[.,;:\"“”'?():_`]|\\w*(?:[^;\"“”'?():_`\\s]*\\w+)+[%º°?']?|[\\S]'''\n",
    "\n",
    "def spans(txt, tokens):\n",
    "    offset = 0\n",
    "    all_spans = []\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        all_spans.append((token, offset, offset + len(token)))\n",
    "        offset += len(token)\n",
    "    return all_spans\n",
    "\n",
    "def get_mwts(token_sentence, tokens_by_id):\n",
    "    mwts = []\n",
    "    for i in range(len(token_sentence)):\n",
    "        token = token_sentence[i]\n",
    "        if isinstance(token['id'], tuple):\n",
    "            id_start = token['id'][0]\n",
    "            id_end = token['id'][1]+1\n",
    "            ids = range(id_start,id_end)\n",
    "            mwts.append((token['text'] , [tokens_by_id[idx] for idx in ids]))\n",
    "    return mwts\n",
    "\n",
    "def merge_stanza_token_sentences(stanza_token_sentences):\n",
    "    offset=0\n",
    "    for i in range(len(stanza_token_sentences)):\n",
    "        for j in range(len(stanza_token_sentences[i])):\n",
    "            if 'head' in stanza_token_sentences[i][j]:\n",
    "                stanza_token_sentences[i][j]['head']=int(stanza_token_sentences[i][j]['head'])+offset\n",
    "                \n",
    "            if  isinstance(stanza_token_sentences[i][j]['id'], tuple):\n",
    "                ids = stanza_token_sentences[i][j]['id']\n",
    "                #stanza_token_sentences[i][j]['id'] = '-'.join([str(int(id)+offset) for id in ids])\n",
    "                stanza_token_sentences[i][j]['id'] = tuple(list(int(id)+offset for id in ids))\n",
    "            else:\n",
    "                stanza_token_sentences[i][j]['id'] = int(stanza_token_sentences[i][j]['id']) + offset\n",
    "\n",
    "        offset=int(stanza_token_sentences[i][-1]['id'])\n",
    "    stanza_token_sentences = list(itertools.chain.from_iterable(stanza_token_sentences))\n",
    "    return stanza_token_sentences\n",
    "\n",
    "def get_stanza_2_tagged(stanza_token_sentence, tagged_sentence):\n",
    "    filtered_stanza_token_sentence = []\n",
    "    skip_tokens_id = []\n",
    "    for token in stanza_token_sentence:\n",
    "        if token['id'] in skip_tokens_id:\n",
    "            continue\n",
    "        if isinstance(token['id'], tuple):#'-' in token['id']:\n",
    "            id_start = token['id'][0]\n",
    "            id_end = token['id'][1]+1\n",
    "            skip_tokens_id =[idx for idx in range(id_start,id_end)]#.split('-')\n",
    "        filtered_stanza_token_sentence.append(token)\n",
    "\n",
    "    for i,token in enumerate(filtered_stanza_token_sentence):\n",
    "        filtered_stanza_token_sentence[i]['start']=int(token['misc'].split('|')[0][11:])\n",
    "\n",
    "    for i,token in enumerate(tagged_sentence):\n",
    "        tagged_sentence[i]['start']=int(token['misc'].split('|')[0][11:])\n",
    "\n",
    "    stanza_2_tagged = {}\n",
    "    stanza_2_tagged[0]=0\n",
    "    for ftoken in filtered_stanza_token_sentence:\n",
    "        compare=[]\n",
    "        for i,ttoken in enumerate(tagged_sentence):\n",
    "            ftext = set(ftoken['text'])\n",
    "            ttext =set(ttoken['text'])\n",
    "            if min(len(ftext-ttext),len(ttext-ftext))==0:\n",
    "                compare.append((i, abs(ftoken['start'] - ttoken['start'])))\n",
    "        if len(compare)==0:\n",
    "            continue\n",
    "            \n",
    "        idx = min(compare, key=lambda x:x[1])[0]\n",
    "        \n",
    "        if isinstance(ftoken['id'],int):\n",
    "            ids = [ftoken['id']]\n",
    "        else:\n",
    "            id_start = ftoken['id'][0]\n",
    "            id_end = ftoken['id'][1]+1\n",
    "            ids = range(id_start, id_end)\n",
    "            \n",
    "        for f_id in ids:#.split('-')\n",
    "            stanza_2_tagged[f_id] = tagged_sentence[idx]['id']\n",
    "    \n",
    "    return stanza_2_tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_path='/share_gamma/nethome/cristian/projects/nlp/NER/distant_supervision/data_full/UFBA__24311.txt'\n",
    "tok_nlp = Pipeline(lang='pt', processors='tokenize,mwt,pos,lemma,depparse',tokenize_pretokenized=False, use_gpu=True)#processors='tokenize,mwt', \n",
    "tag_nlp = Pipeline(lang='pt', tokenize_pretokenized=True, use_gpu=True)#processors='tokenize,pos,lemma,depparse',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('r', 'r')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0],a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading file: {}\".format(input_path))\n",
    "#input_path='data/{}.txt'.format(input_filename)\n",
    "try:\n",
    "    infile = open(input_path, encoding='utf-16')\n",
    "    text = infile.read()\n",
    "    print('first')\n",
    "except:\n",
    "    try:\n",
    "        infile = open(input_path, encoding='utf-16')\n",
    "        text = infile.read()\n",
    "        print('second')\n",
    "    except Exception as e:\n",
    "        infile = open(input_path, encoding='latin-1')\n",
    "        text = infile.read()\n",
    "        print('thrid')\n",
    "        print(e)\n",
    "\n",
    "text = text.replace('<title>','')\n",
    "print('*'*500)\n",
    "#text = infile.read().decode('utf-16', errors='replace')\n",
    "#if text == \"\" or text is None:\n",
    "#    text = infile.read().decode('utf-32', errors='ignore')\n",
    "#print('text',text)\n",
    "preprocessed_text = text_preprocessing(text)\n",
    "#print('preprocessed_text',preprocessed_text)\n",
    "#sentences = portuguese_sentenciation(preprocessed_text)\n",
    "#tok_nlp = Pipeline(lang='pt', tokenize_pretokenized=False, use_gpu=False)#processors='tokenize,mwt', \n",
    "#tag_nlp = Pipeline(lang='pt', tokenize_pretokenized=True, use_gpu=False)#processors='tokenize,pos,lemma,depparse',\n",
    "\n",
    "sentences = portuguese_sentenciation(preprocessed_text)\n",
    "\n",
    "new_document=[]\n",
    "raw_texts=[]\n",
    "phrases = []\n",
    "for phrase,rphrase in tqdm(list(sentences)):\n",
    "    if not phrase.strip()=='':\n",
    "        phrases.append(rphrase)\n",
    "        stanza_sentence = tok_nlp(phrase)\n",
    "        stanza_token_sentences2 = stanza_sentence.to_dict()\n",
    "        stanza_token_sentence = merge_stanza_token_sentences(stanza_token_sentences2)\n",
    "        tokens_by_id = {token['id']:token for token in stanza_token_sentence}\n",
    "        mwt_cases = get_mwts(stanza_token_sentence, tokens_by_id)\n",
    "        sentence = re.findall(tokenization_pattern, phrase)\n",
    "        tagged_sentences = tag_nlp([sentence])     \n",
    "        tagged_sentences = tagged_sentences.to_dict()\n",
    "        tagged_sentence = tagged_sentences[0]\n",
    "        assert len(tagged_sentence)==len(sentence)\n",
    "\n",
    "        sentence = spans(rphrase,sentence)\n",
    "\n",
    "        stanza_2_tagged = get_stanza_2_tagged(stanza_token_sentence, tagged_sentence)\n",
    "\n",
    "        max_str = sentence[-1][-1]\n",
    "        raw_text =\" \"*max_str\n",
    "        sent = []\n",
    "        ant2new = {}\n",
    "        ant2new[0]=0\n",
    "        token_id=1\n",
    "        for (token,start_char,end_char),tagged_token in zip(sentence,tagged_sentence):\n",
    "            idx=tagged_token['id']\n",
    "            ant2new[idx]=token_id\n",
    "            tagged_token.pop('key',None)\n",
    "            tagged_token.pop('misc',None)\n",
    "            tagged_token.pop('id',None)\n",
    "            tagged_token.pop('text',None)\n",
    "\n",
    "            if not len(mwt_cases)==0:\n",
    "                mwt_token_key = mwt_cases[0][0]\n",
    "                mwt_tokens = mwt_cases[0][1]\n",
    "\n",
    "                if token == mwt_token_key:\n",
    "                    token_id0 = token_id\n",
    "                    token_id = token_id + len(mwt_tokens)-1\n",
    "                    token_information = {doc.ID: (token_id0, token_id), doc.TEXT: token, \\\n",
    "                                 doc.MISC: f'start_char={start_char}|end_char={end_char}'}\n",
    "                    token_information = {**token_information, **tagged_token}\n",
    "                    sent.append(token_information)\n",
    "                    #print(mwt_tokens)\n",
    "                    for i,c in enumerate(mwt_tokens):\n",
    "                        #print('c:',c)\n",
    "                        try:\n",
    "                            c.pop('id')\n",
    "                        except: \n",
    "                            print('except:',c)\n",
    "                            raise\n",
    "                        #print(stanza_2_tagged)\n",
    "                        #print(c)\n",
    "                        c['head'] = stanza_2_tagged[c['head']]\n",
    "                        token_information = {doc.ID: token_id0 + i}\n",
    "                        sent.append({**token_information, **c})\n",
    "                    mwt_cases = mwt_cases[1:]\n",
    "                else:\n",
    "                    token_information = {doc.ID: token_id, doc.TEXT: token, \\\n",
    "                             doc.MISC: f'start_char={start_char}|end_char={end_char}'}\n",
    "                    token_information = {**token_information, **tagged_token}\n",
    "                    sent.append(token_information)\n",
    "\n",
    "            else:\n",
    "                token_information = {doc.ID: token_id, doc.TEXT: token, \\\n",
    "                             doc.MISC: f'start_char={start_char}|end_char={end_char}'}\n",
    "                token_information = {**token_information, **tagged_token}\n",
    "                sent.append(token_information)\n",
    "\n",
    "            token_id+=1\n",
    "\n",
    "            raw_text = raw_text[:start_char] + token + raw_text[end_char:]\n",
    "        raw_texts.append(raw_text)\n",
    "        #print(ant2new)\n",
    "        for i in range(len(sent)):\n",
    "            new_head = ant2new[sent[i]['head']]\n",
    "            sent[i]['head'] = int(new_head)\n",
    "\n",
    "        new_document.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts='\\n\\n'.join(raw_texts)\n",
    "phrases_str = '\\n\\n'.join(phrases)\n",
    "print(new_document)\n",
    "print(phrases_str)\n",
    "new_document=doc.Document(new_document, phrases_str)\n",
    "segmented_text = new_document.to_dict()\n",
    "conllu = CoNLL.convert_dict(segmented_text)\n",
    "output = list_to_conllu_text('out.conllu', conllu, phrases)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('out.conllu', \"w\", encoding='utf-8')\n",
    "file.write(output)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat  ~/.local/lib/python3.7/site-packages/stanza/models/common/doc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
