{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e826cdb2-9525-4d74-b93f-aaf5def3b151",
   "metadata": {},
   "source": [
    "### Notebook para realizar a predição NER em documentos novos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea3319b2-5a72-40ea-a921-e2d49314ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Senha:  ··········\n"
     ]
    }
   ],
   "source": [
    "# Configurando Proxy\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "chave  = os.getenv('USER')\n",
    "senha  = getpass('Senha: ')\n",
    "\n",
    "os.environ['HTTP_PROXY']  = f'http://{chave}:{senha}@inet-sys.petrobras.com.br:804'\n",
    "os.environ['HTTPS_PROXY'] = f'http://{chave}:{senha}@inet-sys.petrobras.com.br:804'\n",
    "os.environ['NO_PROXY']    = '127.0.0.1, localhost, petrobras.com.br, petrobras.biz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27abee1b-3959-474a-8e2b-34b843ab63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_incr, parse\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37303186-7dd6-4e59-a50b-c768d09213c3",
   "metadata": {},
   "source": [
    "### Ajustando o dataset para predição do NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f3ffa-b348-49c3-b75f-f77c843ee42e",
   "metadata": {},
   "source": [
    "Usar as mesmos nome de classes e encoder usados no treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e55762-4612-4b43-8a68-4c833f45192d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B=BACIA' 'B=CAMPO' 'B=ELEMENTO_PETRO' 'B=ESTRUTURA_FÍSICA'\n",
      " 'B=EVENTO_PETRO' 'B=FLUIDO' 'B=FLUIDODATERRA_i' 'B=FLUIDODATERRA_o'\n",
      " 'B=NÃOCONSOLID' 'B=POÇO' 'B=ROCHA' 'B=TEXTURA' 'B=UNIDADE_CRONO'\n",
      " 'B=UNIDADE_LITO' 'I=BACIA' 'I=CAMPO' 'I=ELEMENTO_PETRO'\n",
      " 'I=ESTRUTURA_FÍSICA' 'I=EVENTO_PETRO' 'I=FLUIDO' 'I=FLUIDODATERRA_i'\n",
      " 'I=FLUIDODATERRA_o' 'I=NÃOCONSOLID' 'I=POÇO' 'I=ROCHA' 'I=TEXTURA'\n",
      " 'I=UNIDADE_CRONO' 'I=UNIDADE_LITO' 'O' 'Z=Ignorar']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Encoder\n",
    "NER_classes =  ['O', \n",
    "                'B=BACIA','I=BACIA',\n",
    "                'B=UNIDADE_CRONO', 'I=UNIDADE_CRONO', \n",
    "                'B=UNIDADE_LITO', 'I=UNIDADE_LITO',\n",
    "                'B=ROCHA', 'I=ROCHA',\n",
    "                'B=CAMPO', 'I=CAMPO',\n",
    "                'B=FLUIDODATERRA_i', 'I=FLUIDODATERRA_i',\n",
    "                'B=POÇO', 'I=POÇO',\n",
    "                'B=FLUIDO', 'I=FLUIDO',\n",
    "                'B=TEXTURA', 'I=TEXTURA', \n",
    "                'B=ESTRUTURA_FÍSICA', 'I=ESTRUTURA_FÍSICA', \n",
    "                'B=NÃOCONSOLID', 'I=NÃOCONSOLID',\n",
    "                'B=EVENTO_PETRO', 'I=EVENTO_PETRO',\n",
    "                'B=FLUIDODATERRA_o', 'I=FLUIDODATERRA_o',\n",
    "                'B=ELEMENTO_PETRO', 'I=ELEMENTO_PETRO',\n",
    "                'Z=Ignorar'] #,\n",
    "                #'B=TIPO_POROSIDADE', 'I=TIPO_POROSIDADE',\n",
    "                #'B=POÇO_R', 'I=POÇO_R',\n",
    "                #'B=POÇO_T', 'I=POÇO_T',\n",
    "                #'B=POÇO_Q', 'I=POÇO_Q']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(NER_classes)\n",
    "print(le.classes_)\n",
    "le.transform(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7942abe4-b813-47ea-a0e1-516be5ccd229",
   "metadata": {},
   "source": [
    "Definindo função para preprocessar os arquivos em formato CONLLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458ea278-8635-4136-b263-6455fa7d6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_sentences(PetroNER):\n",
    "    \n",
    "    PetroNER_sentences = parse_incr(PetroNER)\n",
    "    \n",
    "    dataset_dict = {}\n",
    "    \n",
    "    for tokenlist in PetroNER_sentences:\n",
    "\n",
    "        ID = []\n",
    "        deps = []\n",
    "        deps_encod = []\n",
    "        upos = []\n",
    "        form = []\n",
    "        grafo = []\n",
    "    \n",
    "        for tok in tokenlist:\n",
    "            if tok['upos'] != '_':\n",
    "                # Verificando se tem a anotação 'grafo'\n",
    "                try:\n",
    "                    grafo.append(tok['misc']['grafo'])\n",
    "                except:\n",
    "                    grafo.append(None)\n",
    "                    \n",
    "                # Verificando se a classe de entidade está na lista de interesse\n",
    "                try:\n",
    "                    deps_encod.append(le.transform([tok['deps']])[0])\n",
    "                    deps.append(tok['deps'])\n",
    "                except:\n",
    "                    deps_encod.append(le.transform(['O'])[0])\n",
    "                    deps.append('O')\n",
    "                \n",
    "                #deps_encod.append(le.transform([tok['deps']])[0])\n",
    "                #deps.append(tok['deps'])    \n",
    "                ID.append(tok['id'])\n",
    "                upos.append(tok['upos'])\n",
    "                form.append(tok['form'])\n",
    "        \n",
    "        dataset_dict[tokenlist.metadata['sent_id']] = {'id':ID, \n",
    "                                                       'deps': deps,\n",
    "                                                       'deps_encod':deps_encod, \n",
    "                                                       'upos': upos, \n",
    "                                                       'form': form,\n",
    "                                                       'grafo': grafo}\n",
    "\n",
    "    return pd.DataFrame(dataset_dict).T.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3acb321-60c2-4693-9f36-ed0b8307ca6f",
   "metadata": {},
   "source": [
    "Função para criar o dicionário vazio, no formato para receber as inferências dos modelos (todos os modelos, não apenas do NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94234888-60da-4ffc-8fce-5e0ccc6c5c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para receber as predições\n",
    "\n",
    "def empty_dic(dataset_pred):\n",
    "\n",
    "    pred_dic = {}\n",
    "\n",
    "    for n in range(len(dataset_pred)):\n",
    "        pred_dic[dataset_pred['index'][n]] = {'NER': {'Sent_original':None,\n",
    "                                                      'Sent_processadas':None,\n",
    "                                                      'Entities':None,\n",
    "                                                      'Classes': None,\n",
    "                                                      'Grafo': None,\n",
    "                                                      'Embedding': None\n",
    "                                                      },\n",
    "                                              'RE': None}                      \n",
    "    \n",
    "    return (pred_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8980cbf-4ee4-48ca-b935-d4b24bd52bd0",
   "metadata": {},
   "source": [
    "Carregar o mesmo tokenizador usado no modelo NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "069e6bd0-cc02-4e21-91bf-9e7d8f97ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o modelo pretreinado a ser usado\n",
    "model_checkpoint = \"neuralmind/bert-large-portuguese-cased\" #\"neuralmind/bert-base-portuguese-cased\" #'bert-base-multilingual-cased' # \"neuralmind/bert-large-portuguese-cased\" \n",
    "# Tamano máximo da sentença\n",
    "max_length=512\n",
    "\n",
    "# Carregar o tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30cfdae-4850-42c0-955b-a81b4236fcf6",
   "metadata": {},
   "source": [
    "Carregando modelo de NER treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d75d6d9-0c39-42c4-807f-5805b5b28dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_model = tf.keras.models.load_model(\"../Named Entity Recognition/Model_NER.h5\",\n",
    "                                       compile=False, \n",
    "                                       custom_objects={\"TFBertModel\": TFBertModel})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7ff2d-716e-4f0f-80b5-8e03460ec9dc",
   "metadata": {},
   "source": [
    "Função para receber o dataset pre-processado e realizar a inferênia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940a7d51-ffff-430b-90eb-f580c4c4acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_NER(dataset_pred):\n",
    "    \n",
    "    X_pred = tokenizer(list(dataset_pred['form'].values),\n",
    "                     truncation=True,\n",
    "                     is_split_into_words=True,\n",
    "                     padding=\"max_length\",\n",
    "                     max_length=max_length)\n",
    "    \n",
    "    # predizendo NER\n",
    "    pred = NER_model.predict([tf.convert_to_tensor(X_pred['input_ids']),\n",
    "                              #tf.convert_to_tensor(X_pred['token_type_ids']),\n",
    "                              tf.convert_to_tensor(X_pred['attention_mask'])]) \n",
    "\n",
    "    # Convertendo o vetor de saída em labels\n",
    "    pred_labels = np.argmax(pred, axis=2)\n",
    "    labels = le.inverse_transform(pred_labels.reshape(-1)).reshape(-1, 512)\n",
    "    \n",
    "    return (X_pred['input_ids'], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee807c8-904d-4351-b11a-ad9340c7dbfe",
   "metadata": {},
   "source": [
    "Função para pos-processar cada sentença após a inferência, de forma que possam ser usada nas etapas posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "183d7132-2535-4fed-b6e5-b33b11632ed4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pos_process_sentences(X_pred_input_ids, labels):\n",
    "    NER = [] # lista com entidades\n",
    "    classe = []\n",
    "    new_NER = []  # lista para receber tok de entidades durante o parser \n",
    "    sentence_orig = [] # lista para receber tokens da sentença\n",
    "    sentence_NER = [] # lista para receber sentenças anotadas com a entidade\n",
    "    NER_on = False # variável para indicar se está fazendo o parser por uma entidade\n",
    "    \n",
    "    #Sentença tokenizada\n",
    "    toks = tokenizer.tokenize(tokenizer.decode(X_pred_input_ids))\n",
    "\n",
    "    #iterando por cada token\n",
    "    for n in range(max_length):\n",
    "        # Pra o loop quando encontrar '[SEP]'\n",
    "        if toks[n] == '[SEP]':\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            #Ignorar o token '[CLS]' \n",
    "            if toks[n] != '[CLS]':\n",
    "                # Ao longo o parser vai gravando a sentença original e nas sentenças marcadas com o NER\n",
    "                sentence_orig.append(X_pred_input_ids[n])\n",
    "                \n",
    "                for sent in range(len(sentence_NER)):\n",
    "                    sentence_NER[sent].append(X_pred_input_ids[n])\n",
    "                \n",
    "                # Verificando se uma entidade está iniciando\n",
    "                if labels[n][0] == 'B':\n",
    "                    # Se uma entidade já está sendo processada, a entidade anterior deve ser finalizada e iniciar a nova\n",
    "                    if NER_on:\n",
    "                        NER.append(tokenizer.decode(new_NER))\n",
    "                        sentence_NER[-1] = tokenizer('[' + classe[-1] + '] ' +  NER[-1] + ' | ')['input_ids'][1:-1] + sentence_NER[-1] + tokenizer('[ / E ]')['input_ids'][1:-1]\n",
    "                        new_NER = []\n",
    "                        classe.append(labels[n][2:])\n",
    "                        new_NER.append(X_pred_input_ids[n])\n",
    "                        sentence_NER.append(sentence_orig)\n",
    "                        sentence_NER[-1] = sentence_NER[-1][:-1] + tokenizer('[E]')['input_ids'][1:-1] + [sentence_NER[-1][-1]]\n",
    "                    else:\n",
    "                        NER_on = True\n",
    "                        classe.append(labels[n][2:])\n",
    "                        new_NER.append(X_pred_input_ids[n])\n",
    "                        sentence_NER.append(sentence_orig)\n",
    "                        sentence_NER[-1] = sentence_NER[-1][:-1] + tokenizer('[E]')['input_ids'][1:-1] + [sentence_NER[-1][-1]]\n",
    "                            \n",
    "                \n",
    "                # Verificando se uma entidade está no meio e adicionar na lista 'new_NER'       \n",
    "                if labels[n][0] == 'I':\n",
    "                    if NER_on:\n",
    "                        new_NER.append(X_pred_input_ids[n])\n",
    "                            \n",
    "                # Verificar ser o token não é entidade e caso anteriormente uma entidade estivesse sendo processada, adicionar na lista 'new_NER'  \n",
    "\n",
    "                if labels[n][0] == 'O':\n",
    "                    if NER_on:\n",
    "                        NER.append(tokenizer.decode(new_NER))\n",
    "                        sentence_NER[-1] = tokenizer('[' + classe[-1] + '] ' +  NER[-1] + ' | ')['input_ids'][1:-1] + sentence_NER[-1] + tokenizer('[ / E ]')['input_ids'][1:-1] + [sentence_NER[-1][-1]]\n",
    "                        new_NER = []\n",
    "                        NER_on = False\n",
    "\n",
    "    if NER_on:\n",
    "        NER.append(tokenizer.decode(new_NER))\n",
    "        sentence_NER[-1] = tokenizer('[' + classe[-1] + '] ' +  NER[-1] + ' | ')['input_ids'][1:-1] + sentence_NER[-1] + tokenizer('[ / E ]')['input_ids'][1:-1]\n",
    "        new_NER = []\n",
    "        NER_on = False\n",
    "        \n",
    "    if NER == []:\n",
    "        return ([[],[],[],[]])\n",
    "    \n",
    "    else:\n",
    "\n",
    "        sentence_orig = tokenizer.decode(sentence_orig)\n",
    "        new_sentence_NER = []\n",
    "        for new_sent in sentence_NER:\n",
    "            new_sentence_NER.append(tokenizer.decode(new_sent))\n",
    "        sentence_NER = new_sentence_NER\n",
    "\n",
    "        return ([NER, classe, sentence_orig, sentence_NER])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26eed1d-c69f-48f8-b66e-c0d074c0f0df",
   "metadata": {},
   "source": [
    "Função para processar todas as sentenças de um arquivo e retornar o dicionário preenchido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83f58b61-a319-4af0-8e52-6b357e9f589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_dictionary(X_pred_input_ids, labels, dataset_pred, pred_dic):\n",
    "    \n",
    "    for n in range(len(labels)):\n",
    "        sent_dic = pos_process_sentences(X_pred_input_ids[n], labels[n])\n",
    "\n",
    "        pred_dic[dataset_pred['index'][n]]['NER']['Entities'] = sent_dic[0]\n",
    "        pred_dic[dataset_pred['index'][n]]['NER']['Classes'] = sent_dic[1]\n",
    "        pred_dic[dataset_pred['index'][n]]['NER']['Sent_original'] = sent_dic[2]\n",
    "        pred_dic[dataset_pred['index'][n]]['NER']['Sent_processadas'] = sent_dic[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a1b1f-cc82-406b-9126-90092d4921a3",
   "metadata": {},
   "source": [
    "### Predição"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e217b-b8c8-4b51-8d98-79b26c0130ac",
   "metadata": {},
   "source": [
    "**Apenas para predição de documentos novos**  \n",
    "Limpando arquivo Conllu (retirando quebra de linha das sentenças) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80298824-b6b5-4906-94f7-fa8fb5fe23f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterando pelos arquivos da pasta 'path_conllu'\n",
    "# path_conllu = \"../../Corpora/Predicao/Documentos_conllu/\"  # Documentos novos\n",
    "\n",
    "# for file in os.listdir(path_conllu):\n",
    "#     filename = os.fsdecode(file)\n",
    "#     if file.endswith(\".conllu\"):\n",
    "#         print(filename)\n",
    "        \n",
    "#         # Mantendo apenas as linhas que começam com # ou contém tabulação\n",
    "#         lines = []\n",
    "#         with open(path_conllu + filename, \"r\", encoding=\"utf-8\") as f:\n",
    "#             for l in f:\n",
    "#                 if (\"\\t\" in l) or (l.startswith(\"#\")):\n",
    "#                     lines.append(l)\n",
    "        \n",
    "#         #Reescrevendo o arquivo\n",
    "#         with open(path_conllu + filename, 'w') as f:\n",
    "#             for l in lines:\n",
    "#                 if l.startswith(\"# sent_id\"):\n",
    "#                     f.write(f\"\\n{l}\")\n",
    "#                 else:\n",
    "#                     f.write(f\"{l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733dd76a-36b3-42e8-bf5f-a5e3b5eb9285",
   "metadata": {},
   "source": [
    "Iterar por todas os arquivos da pasta \"path_conllu\", realizar a predição e salvar o dicionário com as predições na pasta \"path_json\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc432f8d-1be4-4902-a997-3d6d5efd88fd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petroner-uri-teste.conllu\n"
     ]
    }
   ],
   "source": [
    "# Documentos novos\n",
    "# path_conllu = \"../../Corpora/Predicao/Documentos_conllu/\"\n",
    "# path_json = \"../../Corpora/Predicao/Prediction_json/\"\n",
    "\n",
    "# PetroNER-teste para avaliação do pipeline\n",
    "path_conllu = \"../../Corpora/Predicao - avaliação/Documentos_conllu/\"\n",
    "path_json = \"../../Corpora/Predicao - avaliação/Prediction_json/\"\n",
    "\n",
    "for file in os.listdir(path_conllu):\n",
    "    filename = os.fsdecode(file)\n",
    "    if file.endswith(\".conllu\"):\n",
    "        print(filename)\n",
    "        pred = open(path_conllu + filename, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "        dataset_pred = pre_process_sentences(pred)\n",
    "\n",
    "        pred_dic = empty_dic(dataset_pred)\n",
    "        \n",
    "        X_pred_input_ids, labels = pred_NER(dataset_pred)\n",
    "        \n",
    "        pred_dictionary(X_pred_input_ids, labels, dataset_pred, pred_dic)\n",
    "              \n",
    "        with open(path_json + filename[:-7] + '.json', 'w+') as f:\n",
    "            json.dump(pred_dic, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "721994db-637b-4955-99a4-f4bd1c02ba85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
