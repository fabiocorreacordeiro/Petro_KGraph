{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51dc40b-1583-4de2-834a-b05e1e832636",
   "metadata": {},
   "source": [
    "### Notebook para predizer as Relações presentes nos novos documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5d05d8-a452-448e-9ba8-061ec301103c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Senha:  ··········\n"
     ]
    }
   ],
   "source": [
    "# Configurando Proxy\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "chave  = os.getenv('USER')\n",
    "senha  = getpass('Senha: ')\n",
    "\n",
    "os.environ['HTTP_PROXY']  = f'http://{chave}:{senha}@inet-sys.petrobras.com.br:804'\n",
    "os.environ['HTTPS_PROXY'] = f'http://{chave}:{senha}@inet-sys.petrobras.com.br:804'\n",
    "os.environ['NO_PROXY']    = '127.0.0.1, localhost, petrobras.com.br, petrobras.biz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b6e084c-cf74-4e8a-9a93-f30b04462d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from owlready2 import *\n",
    "from transformers import AutoTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376d51dc-6d5a-4d2c-b05b-71ea999d0142",
   "metadata": {},
   "source": [
    "### Carregando Ontologia - PetroKGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c92b69d-d35a-425b-9d0f-61b45f6876d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_ontology(\"http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Documentos Novos\n",
    "# onto_name = \"OntoGeoLogicaInstanciasRelacoes\"\n",
    "# onto = get_ontology(\"../../KnowledgeGraph/OntoGeoLogicaInstanciasRelacoes.owl\")\n",
    "\n",
    "# PetroNER-teste para avaliação do pipeline\n",
    "onto_name = \"OntoGeoLogicaInstanciasRelacoes\"\n",
    "onto = get_ontology(\"../../KnowledgeGraph/OntoGeoLogicaEntidadesNomeadas.owl\")\n",
    "\n",
    "onto.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff4f22a-2718-43ae-a21c-261abdb92c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain e range das relações que serão extraídas\n",
    "domain_range = [('POÇO', 'UNIDADE_LITO'), ('UNIDADE_LITO', 'POÇO'), #'crosses'\n",
    "                ('UNIDADE_LITO', 'ROCHA'), ('ROCHA', 'UNIDADE_LITO'), #'constituted_by'\n",
    "                ('UNIDADE_LITO', 'NÃOCONSOLID'), ('NÃOCONSOLID', 'UNIDADE_LITO'), #'constituted_by'\n",
    "                ('UNIDADE_LITO', 'FLUIDODATERRA_i'), ('FLUIDODATERRA_i', 'UNIDADE_LITO'), #'constituted_by'\n",
    "                ('UNIDADE_LITO', 'FLUIDODATERRA_o'), ('FLUIDODATERRA_o', 'UNIDADE_LITO'), #'constituted_by'\n",
    "                ('UNIDADE_LITO', 'UNIDADE_CRONO'), ('UNIDADE_CRONO', 'UNIDADE_LITO'), #'has_age'\n",
    "                ('UNIDADE_LITO', 'UNIDADE_CRONO'), ('UNIDADE_CRONO', 'UNIDADE_LITO'), #'has_age'\n",
    "                ('UNIDADE_LITO', 'BACIA'), ('BACIA', 'UNIDADE_LITO'), # 'located_in'\n",
    "                ('POÇO', 'BACIA'), ('BACIA', 'POÇO'), # 'located_in'\n",
    "                ('POÇO', 'CAMPO'), ('CAMPO', 'POÇO'), # 'located_in'\n",
    "                ('CAMPO', 'BACIA'), ('BACIA', 'CAMPO'), # 'located_in'\n",
    "                ('UNIDADE_CRONO', 'UNIDADE_CRONO'), #'participates_in'\n",
    "                ('UNIDADE_LITO', 'UNIDADE_LITO'), # 'part_of\n",
    "                ('UNIDADE_CRONO', 'UNIDADE_CRONO')] #'temporal_relation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74bc533-748e-4bd2-a74e-d58ad3308c5c",
   "metadata": {},
   "source": [
    "### Carregando o modelo de RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "013a94cf-4d32-4f1e-9e7d-5757cae85a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o modelo pretreinado usado no treinamento\n",
    "model_checkpoint = 'neuralmind/bert-base-portuguese-cased'  #'bert-base-multilingual-cased' #'monilouise/ner_news_portuguese'\n",
    "# Tamano máximo da sentença\n",
    "max_length=512\n",
    "\n",
    "# Carregar o tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc15136-081a-4290-a100-6467eb6d37de",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_model = tf.keras.models.load_model(\"../Relation Extraction/Model_RE.h5\",\n",
    "                                       compile=False, \n",
    "                                       custom_objects={\"TFBertModel\": TFBertModel})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49fe64-8bff-4cae-8281-0d4f85b10f54",
   "metadata": {},
   "source": [
    "Usando as mesmas labels e encoder usado no treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be021eb-ca5d-42b2-8df1-d7148bd7ded9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['constituted_by' 'crosses' 'has_age' 'located_in' 'no_relation' 'part_of']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Encoder\n",
    "Rel =  ['no_relation',\n",
    "        'constituted_by',\n",
    "        'crosses',\n",
    "        'has_age',\n",
    "        'located_in',\n",
    "        'part_of']\n",
    "# 'participates_in' não teve nenhua anotação  \n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(Rel)\n",
    "print(le.classes_)\n",
    "le.transform(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a5f46-bdba-41d3-80ae-63fdd992936a",
   "metadata": {},
   "source": [
    "### Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d70eaf0-b51f-4bd6-b359-785c8457ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para receber uma lista de URI e retornar os pares de URIs e pares de sentenças processadas\n",
    "\n",
    "def avaliar_URI(Sentencas_dic):\n",
    "    \n",
    "    # Listas para receber pares sentenças processadas e pares de URI\n",
    "    Sent_processadas = []\n",
    "    grafo = []\n",
    "    classes = []\n",
    "    \n",
    "    lista_URI = Sentencas_dic['NER']['Grafo']\n",
    "    lista_Classes = Sentencas_dic['NER']['Classes']\n",
    "    lista_Sent_processadas = Sentencas_dic['NER']['Sent_processadas']\n",
    "    \n",
    "    # Verifica se a lista não tem URI\n",
    "    if lista_URI == None :\n",
    "        return (None, None, None)\n",
    "    \n",
    "    else:\n",
    "        # Filtrando None da lista\n",
    "        lista_URI_sem_None = list(filter(None, lista_URI))\n",
    "    \n",
    "        # Verificando se a sentença tem apenas uma entidade com URI\n",
    "        if len(lista_URI_sem_None) < 2:\n",
    "            return (None, None, None)\n",
    "        else:\n",
    "\n",
    "            # Iterando pelos pares de URI:\n",
    "            for i in range (len(lista_URI) - 1):\n",
    "                # Verificando se a URI é None\n",
    "                if lista_URI[i] == None:\n",
    "                    pass\n",
    "                else:\n",
    "                    for j in range(i+1, len(lista_URI)):\n",
    "                        if lista_URI[j] == None:\n",
    "                            pass\n",
    "                        else:\n",
    "                            Sent_processadas.append((lista_Sent_processadas[i], lista_Sent_processadas[j]))\n",
    "                            grafo.append((lista_URI[i], lista_URI[j]))\n",
    "                            classes.append((lista_Classes[i], lista_Classes[j]))\n",
    "                            \n",
    "            return (grafo, Sent_processadas, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3260de9-0d59-476a-b228-7f30ed39e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations_between_uris(uri_1, uri_2): \n",
    "    #funcao que acessa a ontologia e procura relacao entre URIs\n",
    "    dict_relation_uris = {}\n",
    "    #Pega as relacoes que a URI1 tem\n",
    "    relation_query_results = list(default_world.sparql(\"\"\"\n",
    "            SELECT DISTINCT ?rel\n",
    "            WHERE{?uri ?rel ?obj\n",
    "                 FILTER(contains(str(?rel), \"http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#\"))\n",
    "                 FILTER (contains(str(?uri), \"\"\" + '\"' + uri_1 + '\"' + \"\"\"))\n",
    "                 }\n",
    "            \"\"\"))\n",
    "    \n",
    "    relations_str = []\n",
    "    for relation_uris in relation_query_results:\n",
    "        relations_str.append(str(relation_uris[0]).rsplit(\".\",1)[-1])\n",
    "        \n",
    "    # Para cada tipo de relação procura se existe match entre URI1 e URI2\n",
    "    for relation in relations_str:\n",
    "        relation_between_words = list(default_world.sparql(\"\"\"\n",
    "                PREFIX prefix: <http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#>\n",
    "                SELECT distinct ?y ?x2\n",
    "                WHERE{?y prefix:\"\"\" +  relation  +  \"\"\" ?x1\n",
    "\n",
    "                      FILTER (contains(str(?y), \"\"\" + '\"' + uri_1  + '\"' + \"\"\"))        \n",
    "\n",
    "                      ?x2 rdf:type ?j                                   \n",
    "                      FILTER (contains(str(?x2), \"\"\" + '\"' + uri_2  + '\"' + \"\"\"))\n",
    "\n",
    "                      FILTER ( ?x2 = ?x1 )\n",
    "                    }\n",
    "                \"\"\"))\n",
    "        dict_relation_uris[relation] = relation_between_words\n",
    "    return dict_relation_uris\n",
    "\n",
    "def go_through_relations(uri1,uri2):\n",
    "    relation_uris = get_relations_between_uris(uri1, uri2)            \n",
    "    if relation_uris != {}: #talvez exista relacao entre URIs, dicionario pode vir vazio -> []\n",
    "        for x, y in relation_uris.items():#procurar por relacao\n",
    "            if y != []: #existe alguma relacao\n",
    "#                 print(x)\n",
    "                return x\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e2ff23a-1957-4327-9897-931efc3f58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que recebe um par de sentenças (em formato tupla) com TAG \"[E][\\E]\" e retorna uma sentença com TAGs par RE (\"[E1][\\E1] ...[E2][\\E2]\"\n",
    "def processar_TAG_RE(sent_par):\n",
    "    \n",
    "    # Separando as anotações de classe\n",
    "    sent_1 = sent_par[0].split(' | ')[1]\n",
    "    sent_2 = sent_par[1].split(' | ')[1]\n",
    "    \n",
    "    # Substituindo as TAGs\n",
    "    sent_1 = sent_1.replace('[ E ]', '[ E1 ]').replace('[ / E ]', '[ / E1 ]')\n",
    "    sent_2 = sent_2.replace('[ E ]', '[ E2 ]').replace('[ / E ]', '[ / E2 ]')\n",
    "    \n",
    "    # Ajustando os fragmentos de texto\n",
    "    corte_1 = sent_1.find('[ / E1 ]') + 8\n",
    "    corte_2 = sent_2.find('[ E2 ]')\n",
    "\n",
    "    inicio = sent_1[:corte_1]\n",
    "    meio = sent_2[corte_1 - 15 : corte_2]\n",
    "    fim = sent_2[corte_2:]\n",
    "    \n",
    "    return(inicio + ' ' + meio + fim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53be8177-2454-4704-b1b2-7456fe64732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que recebe uma lista de sentenças e retorna a predição da relação.\n",
    "\n",
    "def RE_predict(list_of_sents):\n",
    "    #Tokenização das sentenças\n",
    "    sents_tok= dict(tokenizer(list_of_sents, \n",
    "                              truncation=True,\n",
    "                              is_split_into_words=False,\n",
    "                              padding=\"max_length\",\n",
    "                              max_length=max_length))\n",
    "    sents_ids = tf.convert_to_tensor(sents_tok['input_ids'])\n",
    "    sents_mask = tf.convert_to_tensor(sents_tok['attention_mask'])\n",
    "    # Usando os ids e mask dos tokens no modelo treinado previamente\n",
    "    pred = RE_model.predict([sents_ids, sents_mask])\n",
    "    \n",
    "    pred_label = le.inverse_transform(np.argmax(pred, axis=1))\n",
    "    \n",
    "    return (pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadea115-09f2-46af-ad06-b9caf435cb82",
   "metadata": {},
   "source": [
    "### Iterando pelos arquivos de predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a65422f3-421d-4e40-bd8d-f56c025e6bac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petroner-uri-teste.json\n"
     ]
    }
   ],
   "source": [
    "# Documentos Novos\n",
    "# path_json = \"../../Corpora/Predicao/Prediction_json/\"\n",
    "\n",
    "# PetroNER-teste para avaliação do pipeline\n",
    "path_json = \"../../Corpora/Predicao - avaliação/Prediction_json/\"\n",
    "\n",
    "new_rels = []\n",
    "\n",
    "# Iterando por cada arquivo\n",
    "for file in os.listdir(path_json):\n",
    "    filename = os.fsdecode(file)\n",
    "    if file.endswith(\".json\"):\n",
    "        print(filename)\n",
    "        \n",
    "        with open(path_json + filename, 'r') as f:\n",
    "            pred_dic = json.load(f)\n",
    "        \n",
    "        sentence_key = pred_dic.keys()\n",
    "        \n",
    "        # Iterando por cada sentença\n",
    "        for key in sentence_key:\n",
    "            Sentencas_dic = pred_dic[key]#['NER']['Sent_processadas']\n",
    "            rels = []\n",
    "            \n",
    "            # recuperando pares de URI e de sentenças processadas\n",
    "            par_grafo, par_sent, par_classes = avaliar_URI(Sentencas_dic)\n",
    "            if par_grafo != None:\n",
    "                \n",
    "                for n in range(len(par_grafo)):\n",
    "                    \n",
    "                    # Verificar se o domain e range das classes são validos para umas relações desejadas\n",
    "                    if par_classes[n] in domain_range:\n",
    "                    \n",
    "                        # Verificar se as duas entidades são diferentes\n",
    "                        if par_grafo[n][0] != par_grafo[n][1]:\n",
    "                            \n",
    "                            # Verificar se a relação já existe no PetroKGraph\n",
    "                            rel = go_through_relations(par_grafo[n][0], par_grafo[n][1])\n",
    "                            if rel != None:\n",
    "                                rels.append((par_grafo[n][0], rel, par_grafo[n][1], 'from PetroKGraph'))\n",
    "                                pred_dic[key]['RE'] = list(set(rels))\n",
    "                            \n",
    "                            # Verificar a relação inversa no PetroKGraph\n",
    "                            else:\n",
    "                                rel = go_through_relations(par_grafo[n][1], par_grafo[n][0])\n",
    "                                if rel != None:\n",
    "                                    rels.append((par_grafo[n][1], rel, par_grafo[n][0], 'from PetroKGraph'))\n",
    "                                    pred_dic[key]['RE'] = list(set(rels))\n",
    "                                \n",
    "                                # Usar modelo para identificar Relação\n",
    "                                else:\n",
    "\n",
    "                                    # Processa sentença e prediz relação\n",
    "                                    sent_RE_process = processar_TAG_RE(par_sent[n])\n",
    "                                    RE_pred = RE_predict([sent_RE_process])\n",
    "                                    \n",
    "                                    # Verificar se achou relação\n",
    "                                    if RE_pred != ['no_relation']:\n",
    "                                        rels.append((par_grafo[n][0], RE_pred[0], par_grafo[n][1], 'from PetroRE'))\n",
    "                                        new_rels.append((par_grafo[n][0], RE_pred[0], par_grafo[n][1], 'from PetroRE'))\n",
    "                                        pred_dic[key]['RE'] = list(set(rels))\n",
    "                                    \n",
    "                                    # Tentar achar relação no sentido inverso\n",
    "                                    else: \n",
    "                                        # Processa sentença e prediz relação\n",
    "                                        sent_RE_process = processar_TAG_RE((par_sent[n][1], par_sent[n][0]))\n",
    "                                        RE_pred = RE_predict([sent_RE_process])\n",
    "                                        \n",
    "                                        # Verificar se achou relação\n",
    "                                        if RE_pred != ['no_relation']:\n",
    "                                            rels.append((par_grafo[n][1], RE_pred[0], par_grafo[n][0],'from PetroRE'))\n",
    "                                            new_rels.append((par_grafo[n][0], RE_pred[0], par_grafo[n][1], 'from PetroRE'))\n",
    "                           \n",
    "        #Salvando o arquivo JSON\n",
    "        with open(path_json + filename, 'w+') as f:\n",
    "            json.dump(pred_dic, f)\n",
    "            \n",
    "            \n",
    "# Salvando arquivo com novas relações\n",
    "# path_entities = \"../../Corpora/Predicao/Prediction_graph/\"  # Documentos Novos\n",
    "path_entities = \"../../Corpora/Predicao - avaliação/Prediction_graph/\"  # PetroNER-teste para avaliação do pipeline\n",
    "filename_entities = \"New_relations\"\n",
    "\n",
    "#Salvando o arquivo as novas entidades\n",
    "with open(path_entities + filename_entities, 'w+') as f:\n",
    "    json.dump(list(set(new_rels)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb9451-560e-4fe0-9b75-63e822874614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
