{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!python main.py -i './docs/' -o './output/' -d 'cpu' -m './result_21_06_2021/multiclass_bert_model/' -s 5 -f 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://petroles.puc-rio.ai/downloads/Models/NER/Multiclass/result_21_06_2021.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, os\n",
    "\n",
    "list_zips = ['./result_21_06_2021.zip']\n",
    "list_folders = ['./result_21_06_2021']\n",
    "\n",
    "for path_to_zip_file, directory_to_extract_to in zip(list_zips,list_folders):\n",
    "    if not os.path.exists(directory_to_extract_to):\n",
    "        os.makedirs(directory_to_extract_to)\n",
    "    else:\n",
    "        continue\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user langdetect stanza transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (BertConfig, BertForTokenClassification,\n",
    "                                  BertTokenizer)\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from IPython.core.debugger import set_trace\n",
    "import torch.nn.functional as F\n",
    "from langdetect import detect\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stanza\n",
    "import torch\n",
    "import nltk\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class BertNer(BertForTokenClassification):\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, valid_ids=None):\n",
    "        sequence_output = self.bert(input_ids, token_type_ids, attention_mask, head_mask=None)[0]\n",
    "        batch_size,max_len,feat_dim = sequence_output.shape\n",
    "        valid_output = torch.zeros(batch_size,max_len,feat_dim,dtype=torch.float32,device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        for i in range(batch_size):\n",
    "            jj = -1\n",
    "            for j in range(max_len):\n",
    "                    if valid_ids[i][j].item() == 1:\n",
    "                        jj += 1\n",
    "                        valid_output[i][jj] = sequence_output[i][j]\n",
    "        sequence_output = self.dropout(valid_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits\n",
    "\n",
    "class InputFeatures(object):\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, valid_positions, tokens):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.valid_positions = valid_positions\n",
    "        self.tokens = tokens\n",
    "\n",
    "class Ner:\n",
    "\n",
    "    def __init__(self,model_dir: str):\n",
    "        self.model , self.tokenizer, self.model_config = self.load_model(model_dir)\n",
    "        self.label_map = self.model_config[\"label_map\"]\n",
    "        self.max_seq_length = self.model_config[\"max_seq_length\"]\n",
    "        self.label_map = {int(k):v for k,v in self.label_map.items()}\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        try:\n",
    "            if self.device == 'cuda':\n",
    "                self.nlp = stanza.Pipeline('pt',processors='tokenize',use_gpu=True) # initialize Portuguese neural pipeline\n",
    "            else:\n",
    "                self.nlp = stanza.Pipeline('pt',processors='tokenize') # initialize Portuguese neural pipeline\n",
    "        except:\n",
    "            stanza.download('pt') # download Portuguese model\n",
    "            if self.device == 'cuda':\n",
    "                self.nlp = stanza.Pipeline('pt',processors='tokenize',use_gpu=True) # initialize Portuguese neural pipeline\n",
    "            else:\n",
    "                self.nlp = stanza.Pipeline('pt',processors='tokenize') # initialize Portuguese neural pipeline\n",
    "\n",
    "    def load_model(self, model_dir: str, model_config: str = \"model_config.json\"):\n",
    "        model_config = os.path.join(model_dir,model_config)\n",
    "        model_config = json.load(open(model_config))\n",
    "        model = BertNer.from_pretrained(model_dir)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_dir)#, do_lower_case=model_config[\"do_lower\"])\n",
    "        return model, tokenizer, model_config\n",
    "\n",
    "    def tokenize(self, sent):\n",
    "        \"\"\" tokenize input\"\"\"\n",
    "        words = [token.text for token in sent]\n",
    "        tokens = []\n",
    "        valid_positions = []\n",
    "        for i, word in enumerate(words):\n",
    "            token = self.tokenizer.tokenize(word)\n",
    "            tokens.extend(token)\n",
    "            for i in range(len(token)):\n",
    "                if i == 0:\n",
    "                    valid_positions.append(1)\n",
    "                else:\n",
    "                    valid_positions.append(0)\n",
    "        return tokens, valid_positions\n",
    "    \n",
    "    def get_sents(self, text:str):\n",
    "        doc = self.nlp(text)\n",
    "        new_sents = []\n",
    "        sentences = doc.sentences\n",
    "        for sent in sentences:\n",
    "\n",
    "            if len(sent.tokens)>self.max_seq_length:\n",
    "                aux = []\n",
    "                for idx, token in enumerate(sent.tokens):\n",
    "                    aux.append(token)\n",
    "                    if ((idx+1)%(self.max_seq_length)-150) == 0:\n",
    "                        new_sents.append(aux)\n",
    "                        aux = []\n",
    "            else:\n",
    "                new_sents.append(sent.tokens)\n",
    "        return new_sents\n",
    "        \n",
    "    def preprocess(self, text: str):\n",
    "        \"\"\" preprocess \"\"\"\n",
    "        print('Start stanza tokenizer...',end='     ')\n",
    "        text_tokenized = self.get_sents(text)\n",
    "        self.sents = text_tokenized\n",
    "        print('Done!')\n",
    "        \n",
    "        features = []   \n",
    "        for sent in text_tokenized:\n",
    "            tokens, valid_positions = self.tokenize(sent)\n",
    "            # insert \"[CLS]\"\n",
    "            tokens.insert(0,\"[CLS]\")\n",
    "            valid_positions.insert(0,1)\n",
    "            # insert \"[SEP]\"\n",
    "            tokens.append(\"[SEP]\")\n",
    "            valid_positions.append(1)\n",
    "            segment_ids = []\n",
    "            for i in range(len(tokens)):\n",
    "                segment_ids.append(0)\n",
    "            input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            input_mask = [1] * len(input_ids)\n",
    "            while len(input_ids) < self.max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "                valid_positions.append(0)\n",
    "            \n",
    "            features.append(\n",
    "                InputFeatures(input_ids=input_ids[:self.max_seq_length],\n",
    "                              input_mask=input_mask[:self.max_seq_length],\n",
    "                              segment_ids=segment_ids[:self.max_seq_length],\n",
    "                              valid_positions=valid_positions[:self.max_seq_length],\n",
    "                              tokens= sent[:self.max_seq_length]))\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def get_predict(self, logits, logits_label, valid_ids):\n",
    "        list_labels = []\n",
    "        for i, (l, l_label) in enumerate(zip(logits,logits_label)):\n",
    "            logits_confidence = [values[label].item() for values,label in zip(l,l_label)]\n",
    "            logits = []\n",
    "            pos = 0\n",
    "            for index,mask in enumerate(valid_ids[i]):\n",
    "                if index == 0:\n",
    "                    continue\n",
    "                if mask == 1:\n",
    "                    logits.append((logits_label[i][index-pos],logits_confidence[index-pos]))\n",
    "                else:\n",
    "                    pos += 1\n",
    "\n",
    "            list_labels.append([(self.label_map[label],confidence) for label,confidence in logits])\n",
    "        return list_labels\n",
    "    \n",
    "    def get_output(self, all_tokens, all_labels):\n",
    "        search = True\n",
    "        flag = 0\n",
    "        out = []\n",
    "        #transforms_labels = lambda x: x if x=='O' else x[2:]\n",
    "        for idx, (tokens, labels) in enumerate(zip(all_tokens,all_labels)):\n",
    "            #list_label = [transforms_labels(label[0]) for label in labels if label[0]!='[SEP]']\n",
    "            list_label = [label[0] for label in labels if label[0]!='[SEP]']\n",
    "            \n",
    "            for index, (word, label) in enumerate(zip(tokens,list_label)):\n",
    "                if label != 'O' and search:\n",
    "                    contador = index\n",
    "                    while label[2:] == list_label[contador+1][2:] and list_label[contador+1] != '0':\n",
    "                        contador +=1\n",
    "                    total = contador - index +1\n",
    "                    flag = total + index\n",
    "                    if total > 0:\n",
    "                        wordesss = []\n",
    "                        wordStartChar = []\n",
    "                        wordEndChar = []\n",
    "                        for word, label in zip(tokens[index:contador+1], list_label[index:contador+1]):\n",
    "                            try:\n",
    "                                wordStartChar.append(word.start_char)\n",
    "                                wordEndChar.append(word.end_char)\n",
    "                                wordesss.append(word.text)\n",
    "                            except Exception as e: print(e)\n",
    "\n",
    "                        out.append((' '.join(map(str,wordesss)),min(wordStartChar),max(wordEndChar),label[2:]))\n",
    "                    else:\n",
    "                        total = 0\n",
    "                    search = False\n",
    "                if index >= flag:\n",
    "                    search = True\n",
    "\n",
    "        return pd.DataFrame(out,columns=['TEXT','START','END','LABEL'])  \n",
    "    \n",
    "    def get_output_2(self, all_tokens, all_labels):\n",
    "        out = []\n",
    "        #transforms_labels = lambda x: x if x=='O' else x[2:]\n",
    "        for idx, (tokens, labels) in enumerate(zip(all_tokens,all_labels)):\n",
    "            #list_label = [transforms_labels(label[0]) for label in labels if label[0]!='[SEP]']\n",
    "            list_label = [label[0] for label in labels if label[0]!='[SEP]']\n",
    "            \n",
    "            for word, label in zip(tokens,list_label):\n",
    "                if label != 'O':\n",
    "                    out.append((word.text,word.start_char,word.end_char,label))\n",
    "\n",
    "        return pd.DataFrame(out,columns=['TEXT','START','END','LABEL'])\n",
    "    \n",
    "    \n",
    "    def predict(self, text: str, batch_size=2):\n",
    "        test_features = self.preprocess(text)\n",
    "        #self.test_features = test_features\n",
    "        \n",
    "        all_input_ids = torch.tensor([f.input_ids for f in test_features],dtype=torch.long,device=self.device)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in test_features],dtype=torch.long,device=self.device)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in test_features],dtype=torch.long,device=self.device)\n",
    "        all_valid_positions = torch.tensor([f.valid_positions for f in test_features],dtype=torch.long,device=self.device)\n",
    "        all_tokens = [f.tokens for f in test_features]\n",
    "        \n",
    "        test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_valid_positions)\n",
    "        test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "        all_labels = []\n",
    "        for index, batch in enumerate(tqdm(test_dataloader, desc=f'Inference batch_size={batch_size}')):\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, valid_ids = batch\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(input_ids, segment_ids, input_mask, valid_ids)\n",
    "\n",
    "            logits = F.softmax(logits,dim=2)\n",
    "            logits_label = torch.argmax(logits,dim=2)\n",
    "            logits_label = logits_label.detach().cpu().numpy()\n",
    "            \n",
    "            all_labels += self.get_predict(logits, logits_label, valid_ids)\n",
    "        \n",
    "        try:\n",
    "            out = self.get_output(all_tokens, all_labels)\n",
    "        except:\n",
    "            out = self.get_output_2(all_tokens, all_labels)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./teste.txt', 'r', encoding='utf-16') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 10:55:52 WARNING: Language pt package default expects mwt, which has been added\n",
      "2022-02-17 10:55:52 INFO: Loading these models for language: pt (Portuguese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | bosque  |\n",
      "| mwt       | bosque  |\n",
      "=======================\n",
      "\n",
      "2022-02-17 10:55:52 INFO: Use device: cpu\n",
      "2022-02-17 10:55:52 INFO: Loading: tokenize\n",
      "2022-02-17 10:55:52 INFO: Loading: mwt\n",
      "2022-02-17 10:55:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "model = Ner('../datasets/result_pos_tag_portuguese/dataset_docs/multiclass_bert_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start stanza tokenizer...     Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference batch_size=2: 100%|██████████| 257/257 [16:51<00:00,  3.94s/it]\n"
     ]
    }
   ],
   "source": [
    "output = model.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SIDIPLA</td>\n",
       "      <td>6351</td>\n",
       "      <td>6358</td>\n",
       "      <td>uniCRONO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SIDIPLA</td>\n",
       "      <td>8241</td>\n",
       "      <td>8248</td>\n",
       "      <td>LIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>estuarino</td>\n",
       "      <td>8971</td>\n",
       "      <td>8980</td>\n",
       "      <td>uniCRONO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>salmão</td>\n",
       "      <td>18577</td>\n",
       "      <td>18583</td>\n",
       "      <td>LIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arenque</td>\n",
       "      <td>19549</td>\n",
       "      <td>19556</td>\n",
       "      <td>LIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ictioplâncton</td>\n",
       "      <td>28337</td>\n",
       "      <td>28350</td>\n",
       "      <td>LIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Grande</td>\n",
       "      <td>31511</td>\n",
       "      <td>31517</td>\n",
       "      <td>CAMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>estuarino</td>\n",
       "      <td>31903</td>\n",
       "      <td>31912</td>\n",
       "      <td>BAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>miragaia</td>\n",
       "      <td>33074</td>\n",
       "      <td>33082</td>\n",
       "      <td>LIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>camarão</td>\n",
       "      <td>33279</td>\n",
       "      <td>33286</td>\n",
       "      <td>CAMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>guatucupa</td>\n",
       "      <td>33492</td>\n",
       "      <td>33501</td>\n",
       "      <td>CAMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Patos</td>\n",
       "      <td>34997</td>\n",
       "      <td>35002</td>\n",
       "      <td>BAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>São</td>\n",
       "      <td>39283</td>\n",
       "      <td>39286</td>\n",
       "      <td>BAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SIDIPLA</td>\n",
       "      <td>41040</td>\n",
       "      <td>41047</td>\n",
       "      <td>uniCRONO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SIDIPLA</td>\n",
       "      <td>44441</td>\n",
       "      <td>44448</td>\n",
       "      <td>uniCRONO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SIDIPLA</td>\n",
       "      <td>46044</td>\n",
       "      <td>46051</td>\n",
       "      <td>LIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Porto</td>\n",
       "      <td>46864</td>\n",
       "      <td>46869</td>\n",
       "      <td>BAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>acamada</td>\n",
       "      <td>49942</td>\n",
       "      <td>49949</td>\n",
       "      <td>CAMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Brevoortia</td>\n",
       "      <td>50251</td>\n",
       "      <td>50261</td>\n",
       "      <td>LIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lycgro</td>\n",
       "      <td>50366</td>\n",
       "      <td>50372</td>\n",
       "      <td>uniCRONO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SIDIPLA</td>\n",
       "      <td>52446</td>\n",
       "      <td>52453</td>\n",
       "      <td>LIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Patos</td>\n",
       "      <td>52701</td>\n",
       "      <td>52706</td>\n",
       "      <td>BAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>plâncton</td>\n",
       "      <td>54327</td>\n",
       "      <td>54335</td>\n",
       "      <td>LIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>estuarino</td>\n",
       "      <td>54586</td>\n",
       "      <td>54595</td>\n",
       "      <td>uniCRONO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Cassino</td>\n",
       "      <td>55419</td>\n",
       "      <td>55426</td>\n",
       "      <td>CAMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Praia do</td>\n",
       "      <td>55794</td>\n",
       "      <td>55802</td>\n",
       "      <td>BAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CETESB.</td>\n",
       "      <td>56625</td>\n",
       "      <td>56632</td>\n",
       "      <td>BAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>SãoPaulo</td>\n",
       "      <td>56699</td>\n",
       "      <td>56707</td>\n",
       "      <td>BAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>São Paulo</td>\n",
       "      <td>56775</td>\n",
       "      <td>56784</td>\n",
       "      <td>BAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SOARES.</td>\n",
       "      <td>60479</td>\n",
       "      <td>60486</td>\n",
       "      <td>BAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>São Paulo</td>\n",
       "      <td>63440</td>\n",
       "      <td>63449</td>\n",
       "      <td>BAC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TEXT  START    END     LABEL\n",
       "0         SIDIPLA   6351   6358  uniCRONO\n",
       "1         SIDIPLA   8241   8248       LIT\n",
       "2       estuarino   8971   8980  uniCRONO\n",
       "3          salmão  18577  18583       LIT\n",
       "4         arenque  19549  19556       LIT\n",
       "5   ictioplâncton  28337  28350       LIT\n",
       "6          Grande  31511  31517      CAMP\n",
       "7       estuarino  31903  31912       BAC\n",
       "8        miragaia  33074  33082       LIT\n",
       "9         camarão  33279  33286      CAMP\n",
       "10      guatucupa  33492  33501      CAMP\n",
       "11          Patos  34997  35002       BAC\n",
       "12            São  39283  39286       BAC\n",
       "13        SIDIPLA  41040  41047  uniCRONO\n",
       "14        SIDIPLA  44441  44448  uniCRONO\n",
       "15        SIDIPLA  46044  46051       LIT\n",
       "16          Porto  46864  46869       BAC\n",
       "17        acamada  49942  49949      CAMP\n",
       "18     Brevoortia  50251  50261       LIT\n",
       "19         lycgro  50366  50372  uniCRONO\n",
       "20        SIDIPLA  52446  52453       LIT\n",
       "21          Patos  52701  52706       BAC\n",
       "22       plâncton  54327  54335       LIT\n",
       "23      estuarino  54586  54595  uniCRONO\n",
       "24        Cassino  55419  55426      CAMP\n",
       "25       Praia do  55794  55802       BAC\n",
       "26        CETESB.  56625  56632       BAC\n",
       "27       SãoPaulo  56699  56707       BAC\n",
       "28      São Paulo  56775  56784       BAC\n",
       "29        SOARES.  60479  60486       BAC\n",
       "30      São Paulo  63440  63449       BAC"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('teste.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data =pd.read_csv('../result_pos_tag_portuguese/dataset_sent/test.csv',sep='\\t',index_col=0)\n",
    "grupos = data.groupby('sentence')\n",
    "grps = []\n",
    "for key in grupos.groups.keys():\n",
    "    try:\n",
    "        grp = grupos.get_group(key)\n",
    "        text = ' '.join(grp['0'].to_list())\n",
    "        out = model.predict(text,batch_size=1)\n",
    "        grp['preds'] = [a[0] for a in out[1][0][:-1]]\n",
    "        grps.append(grp)\n",
    "        print(key)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat(grps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['BAC'] = df1['1'].apply(lambda x: 1 if 'BAC' in x  else 0)\n",
    "df1['LIT'] = df1['1'].apply(lambda x: 1 if 'LIT' in x else 0)\n",
    "df1['CAMP'] = df1['1'].apply(lambda x: 1 if 'CAMP' in x else 0)\n",
    "df1['uniCRONO'] = df1['1'].apply(lambda x: 1 if 'uniCRONO' in x  else 0)\n",
    "\n",
    "df1['BAC_Pred'] = df1['preds'].apply(lambda x: 1 if 'BAC' in x  else 0)\n",
    "df1['LIT_Pred'] = df1['preds'].apply(lambda x: 1 if 'LIT' in x else 0)\n",
    "df1['CAMP_Pred'] = df1['preds'].apply(lambda x: 1 if 'CAMP' in x else 0)\n",
    "df1['uniCRONO_Pred'] = df1['preds'].apply(lambda x: 1 if 'uniCRONO' in x  else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "f1 = [f1_score(df1['BAC'], df1['BAC_Pred'], average='macro'),\n",
    "      f1_score(df1['LIT'], df1['LIT_Pred'], average='macro'),\n",
    "      f1_score(df1['CAMP'], df1['CAMP_Pred'], average='macro'),\n",
    "      f1_score(df1['uniCRONO'], df1['uniCRONO_Pred'], average='macro')\n",
    "     ]\n",
    "\n",
    "precision = [precision_score(df1['BAC'], df1['BAC_Pred'], average='macro'),\n",
    "             precision_score(df1['LIT'], df1['LIT_Pred'], average='macro'),\n",
    "             precision_score(df1['CAMP'], df1['CAMP_Pred'], average='macro'),\n",
    "             precision_score(df1['uniCRONO'], df1['uniCRONO_Pred'], average='macro')\n",
    "            ]\n",
    "\n",
    "recall = [recall_score(df1['BAC'], df1['BAC_Pred'], average='macro'),\n",
    "          recall_score(df1['LIT'], df1['LIT_Pred'], average='macro'),\n",
    "          recall_score(df1['CAMP'], df1['CAMP_Pred'], average='macro'),\n",
    "          recall_score(df1['uniCRONO'], df1['uniCRONO_Pred'], average='macro')\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# set width of bar\n",
    "barWidth = 0.25\n",
    "fig = plt.subplots(figsize =(12, 8))\n",
    " \n",
    " \n",
    "# Set position of bar on X axis\n",
    "br1 = np.arange(len(f1))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "br3 = [x + barWidth for x in br2]\n",
    " \n",
    "# Make the plot\n",
    "plt.bar(br1, f1, color ='r', width = barWidth,\n",
    "        edgecolor ='grey', label ='f1')\n",
    "plt.bar(br2, precision, color ='g', width = barWidth,\n",
    "        edgecolor ='grey', label ='precision')\n",
    "plt.bar(br3, recall, color ='b', width = barWidth,\n",
    "        edgecolor ='grey', label ='recall')\n",
    " \n",
    "# Adding Xticks\n",
    "plt.xlabel('Entidade', fontweight ='bold', fontsize = 15)\n",
    "plt.ylabel('Score', fontweight ='bold', fontsize = 15)\n",
    "plt.xticks([r + barWidth for r in range(len(f1))],\n",
    "        ['BAC','LIT','CAMP','uniCRONO'])\n",
    " \n",
    "plt.legend()\n",
    "plt.ylim([0,1.2])\n",
    "plt.grid(True,alpha=0.7,linestyle='--',)\n",
    "plt.yticks(np.arange(13)*0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
