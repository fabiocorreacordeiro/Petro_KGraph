{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b123e6a-b815-4290-b947-cd3c1c05ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import *\n",
    "import gensim\n",
    "#import random\n",
    "#import unicodedata\n",
    "#import re\n",
    "#import numpy as np\n",
    "#import pickle\n",
    "#from sklearn import svm\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#from sklearn.metrics import f1_score\n",
    "#from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a965a5cb-c302-4a3b-aaab-d0d3dc8b8b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Senha:  ········\n"
     ]
    }
   ],
   "source": [
    "# Configurando Proxy\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "chave  = os.getenv('USER')\n",
    "senha  = getpass('Senha: ')\n",
    "\n",
    "os.environ['HTTP_PROXY']  = f'http://{chave}:{senha}@inet-sys.petrobras.com.br:804'\n",
    "os.environ['HTTPS_PROXY'] = f'http://{chave}:{senha}@inet-sys.petrobras.com.br:804'\n",
    "os.environ['NO_PROXY']    = '127.0.0.1, localhost, petrobras.com.br, petrobras.biz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a626fd6a-b2bb-49f2-be34-5b6a24cff14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://nexus.petrobras.com.br/nexus/repository/pypi-all/simple/\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement owl2vec_star (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for owl2vec_star\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install owl2vec_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02afc3-ae33-44f5-8704-6ffb98eaad5b",
   "metadata": {},
   "source": [
    "### Carregando modelos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4daa96-9c83-4db5-9aba-ce8fc8d24819",
   "metadata": {},
   "source": [
    "Carregando ontologia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef545ae-9284-45d6-b468-a76c4feaa796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OntoGeoLógica populada (OntoGeoLogicaPovoadaInstanciasRelacoes.owl)\n",
    "onto = get_ontology(\"../../KnowledgeGraph/OntoGeoLogicaPovoadaInstanciasRelacoes.owl\")\n",
    "#onto = get_ontology(\"../../KnowledgeGraph/OntoGeoLogicaLito.owl\")\n",
    "onto.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c870b-ad16-4e76-bd65-624341129f4c",
   "metadata": {},
   "source": [
    "Carregando modelo Gensin Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1164f1cb-247d-4f4c-b8fb-a8d116425072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Word2Vec - PetroVec híbrido 100d \n",
    "model_w2v = gensim.models.Word2Vec.load(\"../../Embeddings/PetroVec/Petrovec-híbrido-100/Publico-Hibrido(Completo+NILC).txt.model\")\n",
    "#model_w2v = gensim.models.Word2Vec.load(\"../../Embeddings/PetroVec/Petrovec-OeG-100/publico-COMPLETO-100.txt.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e9ba9-9088-4a47-b19f-3e571edd0f98",
   "metadata": {},
   "source": [
    "### Treinando modelo PetroOntoVec (OWL2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b609fa1-cbb1-498a-a221-e7c997a296c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Access the ontology ...\n",
      "\n",
      "Calculate the ontology projection ...\n",
      "\n",
      "Extract classes and individuals ...\n",
      "\n",
      "Extract axioms ...\n",
      "\n",
      "Extract annotations ...\n",
      "\n",
      "Generate URI document ...\n",
      "Extracted 44972 walks for 976 seed entities\n",
      "Extracted 3675 axiom sentences\n",
      "\n",
      "Generate literal document ...\n",
      "Extracted 2 annotation sentences\n",
      "\n",
      "Generate mixture document ...\n",
      "URI_Doc: 48647, Lit_Doc: 48649, Mix_Doc: 48647\n",
      "Time for document construction: 32.85531497001648 seconds\n",
      "\n",
      "Train the language model ...\n",
      "Time for learning the language model: 15.832857370376587 seconds\n",
      "Model saved. Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\upe2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#has_beginning\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#has_end\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_contains\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_during\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_in\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_finished_by\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_finishes\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_started_by\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#interval_starts\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic subclass of/subproperty of, involving:\n",
      "  http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#temporal_reference_system_used\n",
      "\n",
      "INFO: There are 7191 triples in the ontology\n",
      "INFO: Creating ontology graph projection...\n",
      "INFO: \tExtracting subsumption triples\n",
      "INFO: \t\tTime extracting subsumption: 1.919051170349121 seconds \n",
      "INFO: \tExtracting equivalence triples\n",
      "INFO: \t\tTime extracting equivalences: 0.1406252384185791 seconds \n",
      "INFO: \tExtracting class membership triples.\n",
      "INFO: \t\tTime extracting class membership: 1.8519539833068848 seconds \n",
      "INFO: \tExtracting sameAs triples\n",
      "INFO: \t\tTime extracting sameAs: 0.010971546173095703 seconds \n",
      "INFO: \tExtracting triples associated to after\n",
      "INFO: \t\tTime extracting triples for property: 0.17066597938537598 seconds \n",
      "INFO: \tExtracting triples associated to age_of\n",
      "INFO: \t\tTime extracting triples for property: 0.15359926223754883 seconds \n",
      "INFO: \tExtracting triples associated to before\n",
      "INFO: \t\tTime extracting triples for property: 0.1486060619354248 seconds \n",
      "INFO: \tExtracting triples associated to carrier_of\n",
      "INFO: \t\tTime extracting triples for property: 0.16858410835266113 seconds \n",
      "INFO: \tExtracting triples associated to characteristic_of\n",
      "INFO: \t\tTime extracting triples for property: 0.162369966506958 seconds \n",
      "INFO: \tExtracting triples associated to concretizes\n",
      "INFO: \t\tTime extracting triples for property: 0.17853927612304688 seconds \n",
      "INFO: \tExtracting triples associated to constituted_by\n",
      "INFO: \t\tTime extracting triples for property: 0.16656923294067383 seconds \n",
      "INFO: \tExtracting triples associated to contains_process\n",
      "INFO: \t\tTime extracting triples for property: 0.14857864379882812 seconds \n",
      "INFO: \tExtracting triples associated to denotes\n",
      "INFO: \t\tTime extracting triples for property: 0.14361929893493652 seconds \n",
      "INFO: \tExtracting triples associated to derives_from\n",
      "INFO: \t\tTime extracting triples for property: 0.14860200881958008 seconds \n",
      "INFO: \tExtracting triples associated to derives_into\n",
      "INFO: \t\tTime extracting triples for property: 0.16108989715576172 seconds \n",
      "INFO: \tExtracting triples associated to disposition_of\n",
      "INFO: \t\tTime extracting triples for property: 0.1650705337524414 seconds \n",
      "INFO: \tExtracting triples associated to function_of\n",
      "INFO: \t\tTime extracting triples for property: 0.14826059341430664 seconds \n",
      "INFO: \tExtracting triples associated to generated_by\n",
      "INFO: \t\tTime extracting triples for property: 0.17418122291564941 seconds \n",
      "INFO: \tExtracting triples associated to generated_in\n",
      "INFO: \t\tTime extracting triples for property: 0.16752386093139648 seconds \n",
      "INFO: \tExtracting triples associated to generically_depends_on\n",
      "INFO: \t\tTime extracting triples for property: 0.15915703773498535 seconds \n",
      "INFO: \tExtracting triples associated to has_2D_boundary\n",
      "INFO: \t\tTime extracting triples for property: 0.16456389427185059 seconds \n",
      "INFO: \tExtracting triples associated to has_age\n",
      "INFO: \t\tTime extracting triples for property: 0.15163087844848633 seconds \n",
      "INFO: \tExtracting triples associated to has_beginning\n",
      "INFO: \t\tTime extracting triples for property: 0.2423229217529297 seconds \n",
      "INFO: \tExtracting triples associated to has_characteristic\n",
      "INFO: \t\tTime extracting triples for property: 0.2044541835784912 seconds \n",
      "INFO: \tExtracting triples associated to has_date-time_description\n",
      "INFO: \t\tTime extracting triples for property: 0.1705474853515625 seconds \n",
      "INFO: \tExtracting triples associated to has_disposition\n",
      "INFO: \t\tTime extracting triples for property: 0.1804361343383789 seconds \n",
      "INFO: \tExtracting triples associated to has_duration\n",
      "INFO: \t\tTime extracting triples for property: 0.16394352912902832 seconds \n",
      "INFO: \tExtracting triples associated to has_duration_description\n",
      "INFO: \t\tTime extracting triples for property: 0.1655561923980713 seconds \n",
      "INFO: \tExtracting triples associated to has_end\n",
      "INFO: \t\tTime extracting triples for property: 0.23038816452026367 seconds \n",
      "INFO: \tExtracting triples associated to has_function\n",
      "INFO: \t\tTime extracting triples for property: 0.14464068412780762 seconds \n",
      "INFO: \tExtracting triples associated to has_measurement_datum\n",
      "INFO: \t\tTime extracting triples for property: 0.161576509475708 seconds \n",
      "INFO: \tExtracting triples associated to has_measurement_unit_label\n",
      "INFO: \t\tTime extracting triples for property: 0.1700286865234375 seconds \n",
      "INFO: \tExtracting triples associated to has_member\n",
      "INFO: \t\tTime extracting triples for property: 0.18950772285461426 seconds \n",
      "INFO: \tExtracting triples associated to has_part\n",
      "INFO: \t\tTime extracting triples for property: 0.16858506202697754 seconds \n",
      "INFO: \tExtracting triples associated to has_participant\n",
      "INFO: \t\tTime extracting triples for property: 0.17108845710754395 seconds \n",
      "INFO: \tExtracting triples associated to has_quality\n",
      "INFO: \t\tTime extracting triples for property: 0.15113091468811035 seconds \n",
      "INFO: \tExtracting triples associated to has_role\n",
      "INFO: \t\tTime extracting triples for property: 0.14072036743164062 seconds \n",
      "INFO: \tExtracting triples associated to has_temporal_duration\n",
      "INFO: \t\tTime extracting triples for property: 0.14759540557861328 seconds \n",
      "INFO: \tExtracting triples associated to has_time\n",
      "INFO: \t\tTime extracting triples for property: 0.14461326599121094 seconds \n",
      "INFO: \tExtracting triples associated to has_time_instant_inside\n",
      "INFO: \t\tTime extracting triples for property: 0.16556000709533691 seconds \n",
      "INFO: \tExtracting triples associated to has_time_stamp\n",
      "INFO: \t\tTime extracting triples for property: 0.14661383628845215 seconds \n",
      "INFO: \tExtracting triples associated to in_date-time_description\n",
      "INFO: \t\tTime extracting triples for property: 0.15358924865722656 seconds \n",
      "INFO: \tExtracting triples associated to in_time_position\n",
      "INFO: \t\tTime extracting triples for property: 0.1720738410949707 seconds \n",
      "INFO: \tExtracting triples associated to in_time_zone\n",
      "INFO: \t\tTime extracting triples for property: 0.16256976127624512 seconds \n",
      "INFO: \tExtracting triples associated to interval_after\n",
      "INFO: \t\tTime extracting triples for property: 0.16755270957946777 seconds \n",
      "INFO: \tExtracting triples associated to interval_before\n",
      "INFO: \t\tTime extracting triples for property: 0.16008853912353516 seconds \n",
      "INFO: \tExtracting triples associated to interval_contains\n",
      "INFO: \t\tTime extracting triples for property: 0.26828646659851074 seconds \n",
      "INFO: \tExtracting triples associated to interval_disjoint\n",
      "INFO: \t\tTime extracting triples for property: 0.15560650825500488 seconds \n",
      "INFO: \tExtracting triples associated to interval_during\n",
      "INFO: \t\tTime extracting triples for property: 0.21344566345214844 seconds \n",
      "INFO: \tExtracting triples associated to interval_equals\n",
      "INFO: \t\tTime extracting triples for property: 0.15256762504577637 seconds \n",
      "INFO: \tExtracting triples associated to interval_finished_by\n",
      "INFO: \t\tTime extracting triples for property: 0.2179889678955078 seconds \n",
      "INFO: \tExtracting triples associated to interval_finishes\n",
      "INFO: \t\tTime extracting triples for property: 0.22338390350341797 seconds \n",
      "INFO: \tExtracting triples associated to interval_in\n",
      "INFO: \t\tTime extracting triples for property: 0.24286675453186035 seconds \n",
      "INFO: \tExtracting triples associated to interval_meets\n",
      "INFO: \t\tTime extracting triples for property: 0.4797995090484619 seconds \n",
      "INFO: \tExtracting triples associated to interval_met_by\n",
      "INFO: \t\tTime extracting triples for property: 0.555525541305542 seconds \n",
      "INFO: \tExtracting triples associated to interval_overlapped_by\n",
      "INFO: \t\tTime extracting triples for property: 0.17553377151489258 seconds \n",
      "INFO: \tExtracting triples associated to interval_overlaps\n",
      "INFO: \t\tTime extracting triples for property: 0.1954796314239502 seconds \n",
      "INFO: \tExtracting triples associated to interval_started_by\n",
      "INFO: \t\tTime extracting triples for property: 0.27586841583251953 seconds \n",
      "INFO: \tExtracting triples associated to interval_starts\n",
      "INFO: \t\tTime extracting triples for property: 0.4218714237213135 seconds \n",
      "INFO: \tExtracting triples associated to is_about\n",
      "INFO: \t\tTime extracting triples for property: 0.194685697555542 seconds \n",
      "INFO: \tExtracting triples associated to is_concretized_as\n",
      "INFO: \t\tTime extracting triples for property: 0.19348573684692383 seconds \n",
      "INFO: \tExtracting triples associated to is_duration_of\n",
      "INFO: \t\tTime extracting triples for property: 0.2204139232635498 seconds \n",
      "INFO: \tExtracting triples associated to located_in\n",
      "INFO: \t\tTime extracting triples for property: 0.2094428539276123 seconds \n",
      "INFO: \tExtracting triples associated to location_of\n",
      "INFO: \t\tTime extracting triples for property: 0.2104482650756836 seconds \n",
      "INFO: \tExtracting triples associated to member_of\n",
      "INFO: \t\tTime extracting triples for property: 0.16356325149536133 seconds \n",
      "INFO: \tExtracting triples associated to occurs_in\n",
      "INFO: \t\tTime extracting triples for property: 0.15558743476867676 seconds \n",
      "INFO: \tExtracting triples associated to part_of\n",
      "INFO: \t\tTime extracting triples for property: 0.20097875595092773 seconds \n",
      "INFO: \tExtracting triples associated to participates_in\n",
      "INFO: \t\tTime extracting triples for property: 0.1795206069946289 seconds \n",
      "INFO: \tExtracting triples associated to quality_of\n",
      "INFO: \t\tTime extracting triples for property: 0.1625690460205078 seconds \n",
      "INFO: \tExtracting triples associated to realized_in\n",
      "INFO: \t\tTime extracting triples for property: 0.16855335235595703 seconds \n",
      "INFO: \tExtracting triples associated to realizes\n",
      "INFO: \t\tTime extracting triples for property: 0.17553234100341797 seconds \n",
      "INFO: \tExtracting triples associated to role_of\n",
      "INFO: \t\tTime extracting triples for property: 0.16755414009094238 seconds \n",
      "INFO: \tExtracting triples associated to temporal_position\n",
      "INFO: \t\tTime extracting triples for property: 0.19946861267089844 seconds \n",
      "INFO: \tExtracting triples associated to temporal_reference_system_used\n",
      "INFO: \t\tTime extracting triples for property: 0.1932687759399414 seconds \n",
      "INFO: \tExtracting triples associated to temporal_unit_type\n",
      "INFO: \t\tTime extracting triples for property: 0.21096563339233398 seconds \n",
      "INFO: \tExtracting triples associated to 2D_boundary_of\n",
      "INFO: \t\tTime extracting triples for property: 0.17898035049438477 seconds \n",
      "INFO: \tExtracting data property assertions\n",
      "INFO: \t\tTime extracting data property assertions: 0.5186212062835693 seconds \n",
      "INFO: \tExtracting complex equivalence axioms\n",
      "INFO: \t\tTime extracting complex equivalence axioms: 1.2733654975891113 seconds \n",
      "INFO: \tExtracting annotations.\n",
      "INFO: \t\tTime extracting annotations: 0.4428410530090332 seconds \n",
      "INFO: Projection created into a Graph object (RDFlib library)\n",
      "INFO: Projection saved into turtle file: ./cache\\projection.ttl\n",
      "INFO: collecting all words and their counts\n",
      "INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO: PROGRESS: at sentence #10000, processed 61215 words, keeping 1969 word types\n",
      "INFO: PROGRESS: at sentence #20000, processed 121974 words, keeping 2459 word types\n",
      "INFO: PROGRESS: at sentence #30000, processed 182422 words, keeping 2838 word types\n",
      "INFO: PROGRESS: at sentence #40000, processed 243050 words, keeping 3120 word types\n",
      "INFO: PROGRESS: at sentence #50000, processed 303612 words, keeping 3350 word types\n",
      "INFO: PROGRESS: at sentence #60000, processed 364418 words, keeping 3606 word types\n",
      "INFO: PROGRESS: at sentence #70000, processed 425139 words, keeping 3808 word types\n",
      "INFO: PROGRESS: at sentence #80000, processed 485700 words, keeping 4014 word types\n",
      "INFO: PROGRESS: at sentence #90000, processed 546683 words, keeping 4182 word types\n",
      "INFO: PROGRESS: at sentence #100000, processed 607661 words, keeping 4340 word types\n",
      "INFO: PROGRESS: at sentence #110000, processed 668494 words, keeping 4488 word types\n",
      "INFO: PROGRESS: at sentence #120000, processed 729318 words, keeping 4602 word types\n",
      "INFO: PROGRESS: at sentence #130000, processed 790273 words, keeping 4704 word types\n",
      "INFO: PROGRESS: at sentence #140000, processed 851003 words, keeping 4838 word types\n",
      "INFO: collected 4881 word types from a corpus of 887027 raw words and 145943 sentences\n",
      "INFO: Loading a fresh vocabulary\n",
      "INFO: effective_min_count=1 retains 4881 unique words (100% of original 4881, drops 0)\n",
      "INFO: effective_min_count=1 leaves 887027 word corpus (100% of original 887027, drops 0)\n",
      "INFO: deleting the raw counts dictionary of 4881 items\n",
      "INFO: sample=0.001 downsamples 61 most-common words\n",
      "INFO: downsampling leaves estimated 506254 word corpus (57.1% of prior 887027)\n",
      "INFO: estimated required memory for 4881 words and 100 dimensions: 6345300 bytes\n",
      "INFO: resetting layer weights\n",
      "INFO: training model with 8 workers on 4881 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=25 window=5\n",
      "INFO: EPOCH 1 - PROGRESS: at 71.04% examples, 348307 words/s, in_qsize 15, out_qsize 0\n",
      "DEBUG: job loop exiting, total 89 jobs\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO: EPOCH - 1 : training on 887027 raw words (506182 effective words) took 1.4s, 365318 effective words/s\n",
      "INFO: EPOCH 2 - PROGRESS: at 68.77% examples, 345499 words/s, in_qsize 15, out_qsize 0\n",
      "DEBUG: job loop exiting, total 89 jobs\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO: EPOCH - 2 : training on 887027 raw words (506875 effective words) took 1.4s, 357331 effective words/s\n",
      "INFO: EPOCH 3 - PROGRESS: at 64.28% examples, 324428 words/s, in_qsize 15, out_qsize 0\n",
      "DEBUG: job loop exiting, total 89 jobs\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO: EPOCH - 3 : training on 887027 raw words (506379 effective words) took 1.4s, 349659 effective words/s\n",
      "INFO: EPOCH 4 - PROGRESS: at 67.64% examples, 341436 words/s, in_qsize 15, out_qsize 0\n",
      "DEBUG: job loop exiting, total 89 jobs\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO: EPOCH - 4 : training on 887027 raw words (505944 effective words) took 1.3s, 376537 effective words/s\n",
      "INFO: EPOCH 5 - PROGRESS: at 75.53% examples, 382115 words/s, in_qsize 15, out_qsize 0\n",
      "DEBUG: job loop exiting, total 89 jobs\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG: worker exiting, processed 13 jobs\n",
      "INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO: EPOCH - 5 : training on 887027 raw words (506413 effective words) took 1.2s, 407582 effective words/s\n",
      "INFO: EPOCH 6 - PROGRESS: at 77.77% examples, 388971 words/s, in_qsize 15, out_qsize 0\n",
      "DEBUG: job loop exiting, total 89 jobs\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO: EPOCH - 6 : training on 887027 raw words (506296 effective words) took 1.2s, 422134 effective words/s\n",
      "INFO: EPOCH 7 - PROGRESS: at 73.28% examples, 367802 words/s, in_qsize 15, out_qsize 0\n",
      "DEBUG: job loop exiting, total 89 jobs\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO: EPOCH - 7 : training on 887027 raw words (505654 effective words) took 1.2s, 405271 effective words/s\n",
      "INFO: EPOCH 8 - PROGRESS: at 75.52% examples, 381415 words/s, in_qsize 15, out_qsize 0\n",
      "DEBUG: job loop exiting, total 89 jobs\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO: EPOCH - 8 : training on 887027 raw words (505706 effective words) took 1.2s, 406802 effective words/s\n",
      "INFO: EPOCH 9 - PROGRESS: at 64.29% examples, 314144 words/s, in_qsize 15, out_qsize 0\n",
      "DEBUG: job loop exiting, total 89 jobs\n",
      "DEBUG: worker exiting, processed 9 jobs\n",
      "INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG: worker exiting, processed 13 jobs\n",
      "INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO: EPOCH - 9 : training on 887027 raw words (506247 effective words) took 1.6s, 321470 effective words/s\n",
      "INFO: EPOCH 10 - PROGRESS: at 64.25% examples, 321117 words/s, in_qsize 15, out_qsize 0\n",
      "DEBUG: job loop exiting, total 89 jobs\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "DEBUG: worker exiting, processed 10 jobs\n",
      "INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG: worker exiting, processed 12 jobs\n",
      "INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG: worker exiting, processed 11 jobs\n",
      "INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO: EPOCH - 10 : training on 887027 raw words (506163 effective words) took 1.5s, 345707 effective words/s\n",
      "INFO: training on a 8870270 raw words (5061859 effective words) took 13.8s, 367310 effective words/s\n",
      "INFO: saving Word2Vec object under ./cache\\outputontology.embeddings, separately None\n",
      "INFO: not storing attribute vectors_norm\n",
      "INFO: not storing attribute cum_table\n",
      "DEBUG: {'uri': './cache\\\\outputontology.embeddings', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n",
      "INFO: saved ./cache\\outputontology.embeddings\n",
      "INFO: storing 4881x100 projection weights into ./cache\\outputontology.embeddings.txt\n",
      "DEBUG: {'uri': './cache\\\\outputontology.embeddings.txt', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}\n"
     ]
    }
   ],
   "source": [
    "# Se necessário retreinar, ajustar os hiperparâmetros do arquivo default.cfg\n",
    "!owl2vec_star standalone --config_file default.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3e6ed-a5ff-404f-822d-ad259c07c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando modelo PetroOntoVec \n",
    "#model_owl2v = gensim.models.Word2Vec.load(\"cache_dim100/outputontology.embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd90c7-43b3-4a47-9267-f59c818fe3e3",
   "metadata": {},
   "source": [
    "### Verificando os modelos carregados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29865918-1b6f-4336-9e4f-acb4c892d329",
   "metadata": {},
   "source": [
    "Número de classes e indivíduos na ontologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4bd23f4-5820-4ff0-ab31-5df1c42ad529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de classes:  321\n",
      "Número de indivíduos:  646\n"
     ]
    }
   ],
   "source": [
    "classes = list(onto.classes())\n",
    "print('Número de classes: ', len(list(onto.classes())))\n",
    "\n",
    "individuals = list(onto.individuals())\n",
    "print('Número de indivíduos: ', len(list(onto.individuals())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf3f16-3716-45dd-a934-3dbdcbbc2080",
   "metadata": {},
   "source": [
    "Número de vetores no modelo PetroOntoVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe65e9-0f51-4131-a2b8-03f46fd9e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Número de Vetores: ', len(model_owl2v.wv.vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0623ac34-20ec-4e9e-a058-77d36e892a7c",
   "metadata": {},
   "source": [
    "Número de vetores no modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7faa5986-be00-404d-845b-ad96c7008444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Vetores:  440692\n"
     ]
    }
   ],
   "source": [
    "print('Número de Vetores: ', len(model_w2v.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de29d5f-659c-49cb-88da-ff8f88b443c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648f754-00ca-4ac3-b68a-739d181b7f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee2e35-8fc3-44bb-80b3-84638c0474d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ef35974-27ca-414d-8745-585de5805a57",
   "metadata": {},
   "source": [
    "Carregando modelo Gensin OWL2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e68282-0c64-4a0d-b7db-5308765b6fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se necessário retreinar, ajustar os hiperparâmetros do arquivo default.cfg\n",
    "#!owl2vec_star standalone --config_file default.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef4fd41-b3cb-4a23-bf55-7ecf55bbbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo OWL2Vec - \n",
    "model_owl2v = gensim.models.Word2Vec.load(\"cache_dim100/outputontology.embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710f6fde-8636-40be-8574-10800dfb750f",
   "metadata": {},
   "source": [
    "### Verificando os modelos carregados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ad19a8-d4e0-4f60-8a99-d1f4586f483e",
   "metadata": {},
   "source": [
    "Número de classes e indivíduos na ontologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61d6d1e-b1c6-4db9-9ae5-76ccb3a17dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de classes:  699\n",
      "Número de indivíduos:  65073\n"
     ]
    }
   ],
   "source": [
    "classes = list(onto.classes())\n",
    "print('Número de classes: ', len(list(onto.classes())))\n",
    "\n",
    "individuals = list(onto.individuals())\n",
    "print('Número de indivíduos: ', len(list(onto.individuals())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4f09c-de24-4bed-b8f2-f05e6eff12e8",
   "metadata": {},
   "source": [
    "Número de vetores no modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d85fcfe-3a4c-4bd8-b73a-245945a892d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Vetores:  183121\n"
     ]
    }
   ],
   "source": [
    "print('Número de Vetores: ', len(model_w2v.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc58462-2420-41c5-af09-ca68abac02b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Vetores:  664223\n"
     ]
    }
   ],
   "source": [
    "print('Número de Vetores: ', len(model_owl2v.wv.vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f87cad-e7c7-4d24-ab23-40c26352e181",
   "metadata": {},
   "source": [
    "Usando SPARQL para verificar todas as labels presentes na Ontologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28cd4dbf-f133-4530-9898-44c5e5a5d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando SPARQL para criar uma lista com todas as labels da ontologia\n",
    "all_labels = list(default_world.sparql(\"\"\"\n",
    "                    SELECT ?y\n",
    "                    { ?x rdfs:label ?y . }\n",
    "                    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e23fd5a9-1966-4746-a288-3c11e0de6f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\upe2\\OneDrive - PETROBRAS\\Documents\\Repositorios\\Graph2Vec\\Corpora\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\upe2\\OneDrive - PETROBRAS\\Documents\\Repositorios\\Graph2Vec\n"
     ]
    }
   ],
   "source": [
    "# Carregar funções usadas no preprocessamento do PetroVec\n",
    "%cd Corpora \n",
    "%run _PreProcessamento.ipynb\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57053eb2-fcfa-4e1e-a081-44392abd3183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para preprocessar as labels (usando os mesmos processamentos feitos no PetroVec\n",
    "def process_label (label):\n",
    "    new_label = eliminaCaracteresAcentuados(label.strip())\n",
    "    new_label = eliminaCaracteresEspeciais(new_label)\n",
    "    new_label = new_label.replace('-',' ').strip()\n",
    "    new_label = new_label.replace(' ', '_')\n",
    "    \n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a78f8a60-eee2-4f4b-971e-e44943cf3cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320637"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessar todas as labels da ontologia\n",
    "labels_process = []\n",
    "for labels in all_labels:\n",
    "    labels_process.append(process_label (labels[0]))\n",
    "    \n",
    "all_labels = labels_process\n",
    "len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "291e686f-67e4-4dbb-8892-2266d725c210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3556"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separar tadas as labels que são MWEs\n",
    "labels_mwe = []\n",
    "\n",
    "for label in all_labels:\n",
    "    label_split = label.split(\"_\")\n",
    "    \n",
    "    # Incluir labels com mais de uma palavra na lista de MWE\n",
    "    if len(label_split) > 1:\n",
    "        labels_mwe.append(label)\n",
    "    \n",
    "    # Incluir labels com hífen na lista de MWE\n",
    "    # tratar apenas label com mais de 6 caracteres\n",
    "    \n",
    "    if '-' in label and len(label)>6:\n",
    "        labels_mwe.append(label.replace('-',' ').strip())\n",
    "\n",
    "len(labels_mwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a68524a8-2d08-48cf-9a84-07c3bdbcb079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando lista de MWE\n",
    "\n",
    "#with open('Ontologia_MWE.txt', 'w') as my_list_file:\n",
    "#   #looping over the each ist element\n",
    "#    for element in labels_mwe:\n",
    "#         #writing to file line by line\n",
    "#        my_list_file.write('%s,\\n' % element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8e728-e5cc-4e38-8dd4-5dfc516ecc55",
   "metadata": {},
   "source": [
    "### Amostrando um indivíduo na ontologia e verificando os seus vetores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "016be119-c78d-4244-98eb-022d41cb6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amostrando uma classe como exemplo\n",
    "class_exe = random.sample(list(onto.classes()), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e380a8b-cc63-4696-aeb8-a02e59d2faa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#orthogneiss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-6.00110948e-01, -1.61211073e-01,  5.04836082e-01,  3.61882120e-01,\n",
       "        6.25488818e-01, -3.19106020e-02,  1.89580858e-01,  2.29084045e-01,\n",
       "       -2.74820358e-01, -1.21875226e-01, -2.36282364e-01, -7.73441136e-01,\n",
       "        2.79481292e-01, -2.45097131e-01, -1.30058467e-01,  2.45569929e-01,\n",
       "        3.18507910e-01, -3.37837249e-01, -2.49643907e-01,  4.01288718e-01,\n",
       "       -3.95967573e-01, -3.78667444e-01,  1.39991507e-01, -8.01694870e-01,\n",
       "        4.52305198e-01,  9.62272227e-01,  5.11983097e-01,  7.59303212e-01,\n",
       "        5.33758514e-02,  4.64791842e-02, -7.14596868e-01, -2.33741701e-02,\n",
       "       -5.12573659e-01, -8.32998380e-02,  9.33511853e-01,  3.90009433e-02,\n",
       "        3.80734086e-01,  4.22426283e-01,  9.10681710e-02,  4.04166549e-01,\n",
       "       -7.92386085e-02,  3.00150737e-02,  8.29342484e-01, -8.98407921e-02,\n",
       "       -4.24109221e-01, -1.92860607e-02, -3.30838680e-01,  1.91459700e-01,\n",
       "        1.07665055e-01,  6.09852135e-01, -3.25916708e-01, -7.56468177e-01,\n",
       "       -1.98026612e-01, -4.02617455e-01, -6.51894212e-02, -1.07360333e-01,\n",
       "        3.87794107e-01,  7.44679034e-01, -2.76895851e-01,  2.49712437e-01,\n",
       "        3.19288939e-01,  2.85725057e-01, -1.04048979e+00,  3.53130281e-01,\n",
       "        5.54592311e-01, -1.19569965e-01, -8.05441618e-01,  3.04446012e-01,\n",
       "       -1.48259029e-01,  6.01100802e-01, -2.85012752e-01,  7.49695778e-01,\n",
       "       -1.65061116e-01,  1.64345399e-01,  5.99434555e-01, -1.21877468e+00,\n",
       "        1.77451104e-01, -3.30788642e-01,  4.26099449e-01,  4.97056752e-01,\n",
       "        7.21395433e-01, -1.02064037e+00,  5.16985655e-01, -6.24264419e-01,\n",
       "        3.60100091e-01,  2.69979108e-02,  7.76038051e-01, -3.01974624e-01,\n",
       "        3.62876624e-01, -4.78186943e-02,  1.65784750e-02,  7.33867347e-01,\n",
       "       -5.48201203e-01,  4.07618284e-01,  4.39739197e-01, -1.85152799e-01,\n",
       "       -6.87209598e-04, -4.73824173e-01,  1.59965247e-01,  2.01303616e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identificando a URI da classe amostrada e o vetor OWL2Vec dessa URI\n",
    "URI = class_exe[0].iri\n",
    "print(URI)\n",
    "model_owl2v.wv.get_vector(URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ea78aad-3d17-4238-9a4f-93690b3466db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ortognaisse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.76559997, -0.28431433,  0.7339332 ,  0.15310739, -0.50298667,\n",
       "        1.0293398 , -0.9089291 ,  0.49154428, -1.2829138 , -0.15538073,\n",
       "        0.3361676 ,  0.32258713,  0.30236015, -0.8525175 , -0.7339676 ,\n",
       "        0.06812439,  0.17258033, -0.19034281, -0.20487294,  0.26412642,\n",
       "       -0.451745  , -0.9720001 ,  0.25856522,  1.1118947 , -0.6019822 ,\n",
       "       -1.7088734 , -0.41614413,  0.21912332, -0.35121897, -0.2268106 ,\n",
       "       -0.33338425, -0.6406187 ,  0.08953464, -0.130068  ,  0.753258  ,\n",
       "        0.3120481 , -0.4733411 , -0.22071823, -0.10844773, -0.09456688,\n",
       "       -0.33868232, -1.0161653 ,  0.24288413, -0.18177363, -1.495666  ,\n",
       "        0.19142503,  1.0515566 , -0.15632537, -0.22854537, -0.15199544,\n",
       "       -1.0895354 , -0.37172064,  0.03618393, -0.23148187,  0.47784925,\n",
       "        0.12662435,  0.07635535, -0.27507645,  0.15070257,  0.29046848,\n",
       "       -0.01753431, -0.11168765,  0.6066513 , -0.4230402 , -0.88321024,\n",
       "        0.09547697,  0.0416657 ,  0.5976002 , -0.46985018,  0.8824965 ,\n",
       "        0.23093377, -0.6511477 , -0.44517413,  1.3551307 ,  0.56418484,\n",
       "        0.09101202, -0.6423294 ,  0.3574061 , -0.3542215 ,  0.349466  ,\n",
       "       -0.76349264, -1.0077294 ,  0.34426734,  0.6325573 , -0.36365414,\n",
       "        0.5969225 , -0.7201777 , -0.1367889 ,  0.5292688 , -0.01051815,\n",
       "       -0.25053123, -0.7794415 , -0.8212474 , -0.02445046, -0.18144135,\n",
       "       -0.25754073, -0.34455305, -0.96707666, -0.41669068, -0.4059016 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identificando a primeira label registrada dessa classe (a classe pode ter mais de um label)\n",
    "label = class_exe[0].label[0]\n",
    "label = process_label (label)\n",
    "print (label)\n",
    "\n",
    "model_w2v.wv.get_vector(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e24e2-7799-4ec5-bfbd-360d5c3845ff",
   "metadata": {},
   "source": [
    "... agora amostrando os indivíduos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a94d7b07-0152-41a0-8bfa-00170a0521ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amostrando um indivíduo como exemplo\n",
    "indi_exe = random.sample(list(onto.individuals()), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4387ffa5-9b46-47e6-995c-e8ef71712b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#poco_023759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.2680754e-02, -1.2634169e-01,  7.8132734e-02,  2.7866840e-01,\n",
       "        1.0791692e+00, -4.3170649e-01,  1.1441602e+00,  9.0137370e-02,\n",
       "       -6.6200590e-01,  1.3214651e+00,  9.9946260e-01, -4.5151815e-01,\n",
       "        6.8166763e-02, -1.6324931e-01, -5.0619972e-01,  2.7296829e-01,\n",
       "       -9.9822134e-02, -2.3251947e-02, -3.4956235e-01,  9.0259171e-01,\n",
       "       -1.0445242e+00,  3.8250020e-01,  4.5530146e-01, -1.3840871e+00,\n",
       "       -2.3262878e-01,  2.4052285e-02,  2.8535134e-01,  6.4405310e-01,\n",
       "       -1.3261826e-01, -4.2854633e-02, -9.1252410e-01,  3.9106798e-01,\n",
       "       -1.5311350e-01,  2.1662063e-05,  1.0602155e+00, -2.1051055e-01,\n",
       "        1.0095196e+00, -3.8435972e-01,  3.8413680e-01,  5.1050729e-01,\n",
       "        3.4314841e-02,  2.4037187e-01,  4.4898387e-02, -1.8570779e-01,\n",
       "       -6.6938442e-01,  2.7082554e-01,  9.4244309e-02, -2.7958006e-01,\n",
       "        5.1705837e-01,  4.2862356e-01, -1.1038110e+00, -3.3376420e-01,\n",
       "        8.4476852e-01,  4.4922513e-01,  1.9626659e-01,  9.0328828e-02,\n",
       "       -2.0352970e-01,  8.2098506e-02, -3.5042897e-01,  3.9986879e-01,\n",
       "        9.0661526e-02,  8.9334631e-01, -1.1126684e+00, -3.7346748e-01,\n",
       "       -5.0488453e-02,  6.3985571e-02, -9.2994928e-01,  9.9841797e-01,\n",
       "       -2.0298269e-02,  4.5133926e-02, -1.3766595e+00,  7.6776600e-01,\n",
       "        1.0297291e-01, -1.4751615e-01, -2.1211554e-01, -1.0861309e+00,\n",
       "       -4.3727621e-01,  6.4418837e-02, -1.1619454e-01, -5.9741658e-01,\n",
       "        1.5407707e-01, -9.8119950e-01,  1.5011494e+00, -1.8388024e-01,\n",
       "        1.1112747e+00, -4.1878438e-01,  2.7882859e-02,  2.1125366e-01,\n",
       "        2.4377635e-01, -1.7003918e-01, -5.4641355e-02,  5.6816574e-02,\n",
       "        2.6664713e-01,  1.6233858e-01,  8.3531976e-02,  1.3368019e-01,\n",
       "        3.8294798e-01, -1.6118529e-01, -7.9576887e-02, -1.9206712e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identificando a URI do indivíduo amostrado e o vetor OWL2Vec dessa URI\n",
    "URI = indi_exe[0].iri\n",
    "print(URI)\n",
    "model_owl2v.wv.get_vector(URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fb132ff-b4ed-42f9-a059-816e2eccb9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7rep34\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word '7rep34' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9340\\3370153675.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel_w2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Graph2Vec\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Graph2Vec\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word '7rep34' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# Identificando a primeira label registrada dessa classe (a classe pode ter mais de um label)\n",
    "label = indi_exe[0].label[0]\n",
    "label = process_label (label)\n",
    "print(label)\n",
    "\n",
    "model_w2v.wv.get_vector(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f9d98-511a-4ebc-bc01-a45a65089c59",
   "metadata": {},
   "source": [
    "### Criando dataset para treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf41d51-098b-4c39-8e0f-83e648071c1b",
   "metadata": {},
   "source": [
    "Queremos treinar um modelo que identifique se uma determinada palavra é label de uma determinada URI. Para isso iremos montar um dataset de treinamento por:\n",
    "- Pares de vetores onde o primeiro vetor é o vetor da URI proveniente do modelo OWL2Vec e o segundo é o vetor da label no modelo PetroVec. (positive sample)\n",
    "- Pares de vetores onde o primeiro vetor é o vetor da URI proveniente do modelo OWL2Vec e o segundo é um vetor amostrado aleatóriamente do modelo PetroVec. (easy negative sample)\n",
    "- Pares de vetores onde o primeiro vetor é o vetor da URI proveniente do modelo OWL2Vec e o segundo é um vetor do modelos PetroVec similar ao vetor da label, mas que pertença à um conceito diferente da URI. (hard negative sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95653f09-3ea3-4d14-a863-e8016e447a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que recebe um indivíduo ou classe da ontologia e retorna pares de positive sample \n",
    "# (Uma lista contendo tuplas. As tuplas são compostas por URI, o vetor da URI (OWL2Vec), uma das labels e o vetor da label (PetroVec))\n",
    "\n",
    "def positive_sample(exe):\n",
    "    pairs = []\n",
    "    \n",
    "    # Obtendo o vetor da URI\n",
    "    URI = exe.iri\n",
    "    URI_vector = model_owl2v.wv.get_vector(URI) \n",
    "    \n",
    "    # Iterando sobre todos as label\n",
    "    URI_labels = exe.label\n",
    "    for l in URI_labels:\n",
    "        # Pular caso não encontre o vetor da label no modelo PetroVec\n",
    "        try:\n",
    "            label = process_label(l)\n",
    "            #obtendo o vetor da label no modelo PetroVec\n",
    "            label_vector = model_w2v.wv.get_vector(label)\n",
    "            # Adicionando o os dois vetores na lista pairs\n",
    "            pairs.append((URI, URI_vector, label, label_vector))\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "178717fc-b075-403d-9241-52203a9dba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sample_easy(exe):\n",
    "    #Função que recebe um indivíduo ou classe da ontologia e retorna um par de negative sample easy\n",
    "    # (Uma lista contendo tuplas. As tuplas são compostas por URI, o vetor da URI (OWL2Vec), \n",
    "    # uma labels amostrada aleatóriamente e o vetor da label (PetroVec))\n",
    "       \n",
    "    # Obtendo o vetor da URI\n",
    "    URI = exe.iri\n",
    "    URI_vector = model_owl2v.wv.get_vector(URI) \n",
    "    \n",
    "    # Iterando sobre todos as label\n",
    "    URI_labels = exe.label\n",
    "    labels_process = []\n",
    "\n",
    "    # preprocessando as labels\n",
    "    for l in URI_labels:\n",
    "        labels_process.append(process_label(l))\n",
    "    \n",
    "    while True:\n",
    "        # Amostrar aleatóriamente uma palavra do vocabulário do PetroVec\n",
    "        sample = random.sample(list(model_w2v.wv.vocab), 1)[0]\n",
    "        #Verificar se a amostra pertence a lista de labels\n",
    "        if sample not in labels_process:\n",
    "            sample_vector = model_w2v.wv.get_vector(sample)\n",
    "            pair = (URI, URI_vector, sample, sample_vector)\n",
    "            return(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f136717e-fcec-4e76-93c5-5e8375f6638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sample_hard(positive_labels, n_hard):\n",
    "    #Função que recebe uma lista de positive labels e número de negative hard samples desejado e \n",
    "    # retorna pares de negative sample hard\n",
    "    # (Uma lista contendo tuplas. As tuplas são compostas por uma labels similar (negative hard) e o vetor da label (PetroVec))\n",
    "    # A label similar foi selecionada entre as labels mais similares usando o modelo PetroVec\n",
    "    \n",
    "    # Obtendo o vetor da URI\n",
    "    #URI = exe.iri\n",
    "    #URI_vector = model_owl2v.wv.get_vector(URI) \n",
    "    \n",
    "    # Iterando sobre todos as label\n",
    "    #URI_labels = exe.label\n",
    "    #labels_process = []\n",
    "\n",
    "    # preprocessando as labels\n",
    "    #for l in URI_labels:\n",
    "    #    labels_process.append(process_label(l))\n",
    "        \n",
    "    #Lista para receber os negative samples hard\n",
    "    pair = []\n",
    "   \n",
    "    # Iterando sobre as positive labels\n",
    "    for label in positive_labels:\n",
    "        # Identificar palavras similares no modelo PetroVec\n",
    "        similar = model_w2v.wv.most_similar(label, topn=100)\n",
    "        \n",
    "        # Iterar entre as palavras similares e verifica se elas fazem parte da Ontologia\n",
    "        # mas não fazem parte das URI_labels\n",
    "        i_hard = 0     #Contador de negative hard sample\n",
    "        for sim in similar:\n",
    "            if sim[0] not in positive_labels:\n",
    "                if sim[0] in all_labels:\n",
    "                    sim_vector = model_w2v.wv.get_vector(sim[0])\n",
    "                    pair.append((sim[0], sim_vector))\n",
    "                    i_hard = i_hard + 1\n",
    "                    # Verifica se alcançou o númer de negative hard sample desejado\n",
    "                    if i_hard == n_hard:\n",
    "                        break\n",
    "    return(pair)\n",
    "        \n",
    "    \n",
    "#    n = 0\n",
    "#    while n < 30:\n",
    "#        n = n + 1\n",
    "        \n",
    "#        try:\n",
    "#            # Amostrar uma das labels (pelo menos uma tem que existir no vocablário do PetroVec)\n",
    "#            sample_label = random.sample(labels_process, 1)[0]\n",
    "#            # Identificar palavras similares no modelo PetroVec\n",
    "#            similar = model_w2v.wv.most_similar(sample_label, topn=100)\n",
    "#            similar = random.sample(similar, 100)\n",
    "            \n",
    "#            # Iterar entre as palavras similares e verifica se elas fazem parte da Ontologia\n",
    "#            # mas não fazem parte das URI_labels\n",
    "#            for sim in similar:\n",
    "#                if sim[0] not in labels_process:\n",
    "#                    if sim[0] in all_labels:\n",
    "#                        sim_vector = model_w2v.wv.get_vector(sim[0])\n",
    "#                        pair = (URI, URI_vector, sim[0], sim_vector)\n",
    "#                        return(pair)\n",
    "\n",
    "#        except:       \n",
    "#            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc72e3b0-aab5-483a-92a6-032f773da21f",
   "metadata": {},
   "source": [
    "### Iterar pelas classes e indivíduos para montar o dataset de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed803925-06dc-47b0-8a4b-38ba6d00ead0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de nós avaliados:  100\n",
      "Numero de labels no dataset:  48\n",
      "--------------------------\n",
      "Número de nós avaliados:  200\n",
      "Numero de labels no dataset:  116\n",
      "--------------------------\n",
      "Número de nós avaliados:  300\n",
      "Numero de labels no dataset:  177\n",
      "--------------------------\n",
      "Número de nós avaliados:  400\n",
      "Numero de labels no dataset:  231\n",
      "--------------------------\n",
      "Número de nós avaliados:  500\n",
      "Numero de labels no dataset:  248\n",
      "--------------------------\n",
      "Número de nós avaliados:  600\n",
      "Numero de labels no dataset:  250\n",
      "--------------------------\n",
      "Número de nós avaliados:  700\n",
      "Numero de labels no dataset:  254\n",
      "--------------------------\n",
      "Número de nós avaliados:  800\n",
      "Numero de labels no dataset:  308\n",
      "--------------------------\n",
      "Número de nós avaliados:  900\n",
      "Numero de labels no dataset:  358\n",
      "--------------------------\n",
      "Número de nós avaliados:  1000\n",
      "Numero de labels no dataset:  402\n",
      "--------------------------\n",
      "Número de nós avaliados:  1100\n",
      "Numero de labels no dataset:  402\n",
      "--------------------------\n",
      "Número de nós avaliados:  1200\n",
      "Numero de labels no dataset:  426\n",
      "--------------------------\n",
      "Número de nós avaliados:  1300\n",
      "Numero de labels no dataset:  466\n",
      "--------------------------\n",
      "Número de nós avaliados:  1400\n",
      "Numero de labels no dataset:  518\n",
      "--------------------------\n",
      "Número de nós avaliados:  1500\n",
      "Numero de labels no dataset:  542\n",
      "--------------------------\n",
      "Número de nós avaliados:  1600\n",
      "Numero de labels no dataset:  598\n",
      "--------------------------\n",
      "Número de nós avaliados:  1700\n",
      "Numero de labels no dataset:  668\n",
      "--------------------------\n",
      "Número de nós avaliados:  1800\n",
      "Numero de labels no dataset:  716\n",
      "--------------------------\n",
      "Número de nós avaliados:  1900\n",
      "Numero de labels no dataset:  736\n",
      "--------------------------\n",
      "Número de nós avaliados:  2000\n",
      "Numero de labels no dataset:  773\n",
      "--------------------------\n",
      "Número de nós avaliados:  2100\n",
      "Numero de labels no dataset:  807\n",
      "--------------------------\n",
      "Número de nós avaliados:  2200\n",
      "Numero de labels no dataset:  853\n",
      "--------------------------\n",
      "Número de nós avaliados:  2300\n",
      "Numero de labels no dataset:  932\n",
      "--------------------------\n",
      "Número de nós avaliados:  2400\n",
      "Numero de labels no dataset:  996\n",
      "--------------------------\n",
      "Número de nós avaliados:  2500\n",
      "Numero de labels no dataset:  1063\n",
      "--------------------------\n",
      "Número de nós avaliados:  2600\n",
      "Numero de labels no dataset:  1138\n",
      "--------------------------\n",
      "Número de nós avaliados:  2700\n",
      "Numero de labels no dataset:  1209\n",
      "--------------------------\n",
      "Número de nós avaliados:  2800\n",
      "Numero de labels no dataset:  1268\n",
      "--------------------------\n",
      "Número de nós avaliados:  2900\n",
      "Numero de labels no dataset:  1321\n",
      "--------------------------\n",
      "Número de nós avaliados:  3000\n",
      "Numero de labels no dataset:  1365\n",
      "--------------------------\n",
      "Número de nós avaliados:  3100\n",
      "Numero de labels no dataset:  1391\n",
      "--------------------------\n",
      "Número de nós avaliados:  3200\n",
      "Numero de labels no dataset:  1423\n",
      "--------------------------\n",
      "Número de nós avaliados:  3300\n",
      "Numero de labels no dataset:  1480\n",
      "--------------------------\n",
      "Número de nós avaliados:  3400\n",
      "Numero de labels no dataset:  1504\n",
      "--------------------------\n",
      "Número de nós avaliados:  3500\n",
      "Numero de labels no dataset:  1525\n",
      "--------------------------\n",
      "Número de nós avaliados:  3600\n",
      "Numero de labels no dataset:  1540\n",
      "--------------------------\n",
      "Número de nós avaliados:  3700\n",
      "Numero de labels no dataset:  1545\n",
      "--------------------------\n",
      "Número de nós avaliados:  3800\n",
      "Numero de labels no dataset:  1573\n",
      "--------------------------\n",
      "Número de nós avaliados:  3900\n",
      "Numero de labels no dataset:  1591\n",
      "--------------------------\n",
      "Número de nós avaliados:  4000\n",
      "Numero de labels no dataset:  1618\n",
      "--------------------------\n",
      "Número de nós avaliados:  4100\n",
      "Numero de labels no dataset:  1618\n",
      "--------------------------\n",
      "Número de nós avaliados:  4200\n",
      "Numero de labels no dataset:  1651\n",
      "--------------------------\n",
      "Número de nós avaliados:  4300\n",
      "Numero de labels no dataset:  1677\n",
      "--------------------------\n",
      "Número de nós avaliados:  4400\n",
      "Numero de labels no dataset:  1699\n",
      "--------------------------\n",
      "Número de nós avaliados:  4500\n",
      "Numero de labels no dataset:  1699\n",
      "--------------------------\n",
      "Número de nós avaliados:  4600\n",
      "Numero de labels no dataset:  1720\n",
      "--------------------------\n",
      "Número de nós avaliados:  4700\n",
      "Numero de labels no dataset:  1747\n",
      "--------------------------\n",
      "Número de nós avaliados:  4800\n",
      "Numero de labels no dataset:  1767\n",
      "--------------------------\n",
      "Número de nós avaliados:  4900\n",
      "Numero de labels no dataset:  1773\n",
      "--------------------------\n",
      "Número de nós avaliados:  5000\n",
      "Numero de labels no dataset:  1811\n",
      "--------------------------\n",
      "Número de nós avaliados:  5100\n",
      "Numero de labels no dataset:  1829\n",
      "--------------------------\n",
      "Número de nós avaliados:  5200\n",
      "Numero de labels no dataset:  1861\n",
      "--------------------------\n",
      "Número de nós avaliados:  5300\n",
      "Numero de labels no dataset:  1903\n",
      "--------------------------\n",
      "Número de nós avaliados:  5400\n",
      "Numero de labels no dataset:  1947\n",
      "--------------------------\n",
      "Número de nós avaliados:  5500\n",
      "Numero de labels no dataset:  1974\n",
      "--------------------------\n",
      "Número de nós avaliados:  5600\n",
      "Numero de labels no dataset:  1990\n",
      "--------------------------\n",
      "Número de nós avaliados:  5700\n",
      "Numero de labels no dataset:  2022\n",
      "--------------------------\n",
      "Número de nós avaliados:  5800\n",
      "Numero de labels no dataset:  2068\n",
      "--------------------------\n",
      "Número de nós avaliados:  5900\n",
      "Numero de labels no dataset:  2082\n",
      "--------------------------\n",
      "Número de nós avaliados:  6000\n",
      "Numero de labels no dataset:  2105\n",
      "--------------------------\n",
      "Número de nós avaliados:  6100\n",
      "Numero de labels no dataset:  2128\n",
      "--------------------------\n",
      "Número de nós avaliados:  6200\n",
      "Numero de labels no dataset:  2129\n",
      "--------------------------\n",
      "Número de nós avaliados:  6300\n",
      "Numero de labels no dataset:  2129\n",
      "--------------------------\n",
      "Número de nós avaliados:  6400\n",
      "Numero de labels no dataset:  2129\n",
      "--------------------------\n",
      "Número de nós avaliados:  6500\n",
      "Numero de labels no dataset:  2129\n",
      "--------------------------\n",
      "Número de nós avaliados:  6600\n",
      "Numero de labels no dataset:  2139\n",
      "--------------------------\n",
      "Número de nós avaliados:  6700\n",
      "Numero de labels no dataset:  2143\n",
      "--------------------------\n",
      "Número de nós avaliados:  6800\n",
      "Numero de labels no dataset:  2175\n",
      "--------------------------\n",
      "Número de nós avaliados:  6900\n",
      "Numero de labels no dataset:  2213\n",
      "--------------------------\n",
      "Número de nós avaliados:  7000\n",
      "Numero de labels no dataset:  2213\n",
      "--------------------------\n",
      "Número de nós avaliados:  7100\n",
      "Numero de labels no dataset:  2226\n",
      "--------------------------\n",
      "Número de nós avaliados:  7200\n",
      "Numero de labels no dataset:  2246\n",
      "--------------------------\n",
      "Número de nós avaliados:  7300\n",
      "Numero de labels no dataset:  2262\n",
      "--------------------------\n",
      "Número de nós avaliados:  7400\n",
      "Numero de labels no dataset:  2311\n",
      "--------------------------\n",
      "Número de nós avaliados:  7500\n",
      "Numero de labels no dataset:  2339\n",
      "--------------------------\n",
      "Número de nós avaliados:  7600\n",
      "Numero de labels no dataset:  2367\n",
      "--------------------------\n",
      "Número de nós avaliados:  7700\n",
      "Numero de labels no dataset:  2387\n",
      "--------------------------\n",
      "Número de nós avaliados:  7800\n",
      "Numero de labels no dataset:  2410\n",
      "--------------------------\n",
      "Número de nós avaliados:  7900\n",
      "Numero de labels no dataset:  2434\n",
      "--------------------------\n",
      "Número de nós avaliados:  8000\n",
      "Numero de labels no dataset:  2450\n",
      "--------------------------\n",
      "Número de nós avaliados:  8100\n",
      "Numero de labels no dataset:  2472\n",
      "--------------------------\n",
      "Número de nós avaliados:  8200\n",
      "Numero de labels no dataset:  2478\n",
      "--------------------------\n",
      "Número de nós avaliados:  8300\n",
      "Numero de labels no dataset:  2491\n",
      "--------------------------\n",
      "Número de nós avaliados:  8400\n",
      "Numero de labels no dataset:  2512\n",
      "--------------------------\n",
      "Número de nós avaliados:  8500\n",
      "Numero de labels no dataset:  2526\n",
      "--------------------------\n",
      "Número de nós avaliados:  8600\n",
      "Numero de labels no dataset:  2534\n",
      "--------------------------\n",
      "Número de nós avaliados:  8700\n",
      "Numero de labels no dataset:  2543\n",
      "--------------------------\n",
      "Número de nós avaliados:  8800\n",
      "Numero de labels no dataset:  2547\n",
      "--------------------------\n",
      "Número de nós avaliados:  8900\n",
      "Numero de labels no dataset:  2561\n",
      "--------------------------\n",
      "Número de nós avaliados:  9000\n",
      "Numero de labels no dataset:  2576\n",
      "--------------------------\n",
      "Número de nós avaliados:  9100\n",
      "Numero de labels no dataset:  2593\n",
      "--------------------------\n",
      "Número de nós avaliados:  9200\n",
      "Numero de labels no dataset:  2612\n",
      "--------------------------\n",
      "Número de nós avaliados:  9300\n",
      "Numero de labels no dataset:  2616\n",
      "--------------------------\n",
      "Número de nós avaliados:  9400\n",
      "Numero de labels no dataset:  2627\n",
      "--------------------------\n",
      "Número de nós avaliados:  9500\n",
      "Numero de labels no dataset:  2639\n",
      "--------------------------\n",
      "Número de nós avaliados:  9600\n",
      "Numero de labels no dataset:  2639\n",
      "--------------------------\n",
      "Número de nós avaliados:  9700\n",
      "Numero de labels no dataset:  2639\n",
      "--------------------------\n",
      "Número de nós avaliados:  9800\n",
      "Numero de labels no dataset:  2641\n",
      "--------------------------\n",
      "Número de nós avaliados:  9900\n",
      "Numero de labels no dataset:  2648\n",
      "--------------------------\n",
      "Número de nós avaliados:  10000\n",
      "Numero de labels no dataset:  2655\n",
      "--------------------------\n",
      "Número de nós avaliados:  10100\n",
      "Numero de labels no dataset:  2674\n",
      "--------------------------\n",
      "Número de nós avaliados:  10200\n",
      "Numero de labels no dataset:  2681\n",
      "--------------------------\n",
      "Número de nós avaliados:  10300\n",
      "Numero de labels no dataset:  2689\n",
      "--------------------------\n",
      "Número de nós avaliados:  10400\n",
      "Numero de labels no dataset:  2700\n",
      "--------------------------\n",
      "Número de nós avaliados:  10500\n",
      "Numero de labels no dataset:  2706\n",
      "--------------------------\n",
      "Número de nós avaliados:  10600\n",
      "Numero de labels no dataset:  2711\n",
      "--------------------------\n",
      "Número de nós avaliados:  10700\n",
      "Numero de labels no dataset:  2721\n",
      "--------------------------\n",
      "Número de nós avaliados:  10800\n",
      "Numero de labels no dataset:  2729\n",
      "--------------------------\n",
      "Número de nós avaliados:  10900\n",
      "Numero de labels no dataset:  2729\n",
      "--------------------------\n",
      "Número de nós avaliados:  11000\n",
      "Numero de labels no dataset:  2729\n",
      "--------------------------\n",
      "Número de nós avaliados:  11100\n",
      "Numero de labels no dataset:  2740\n",
      "--------------------------\n",
      "Número de nós avaliados:  11200\n",
      "Numero de labels no dataset:  2746\n",
      "--------------------------\n",
      "Número de nós avaliados:  11300\n",
      "Numero de labels no dataset:  2753\n",
      "--------------------------\n",
      "Número de nós avaliados:  11400\n",
      "Numero de labels no dataset:  2767\n",
      "--------------------------\n",
      "Número de nós avaliados:  11500\n",
      "Numero de labels no dataset:  2769\n",
      "--------------------------\n",
      "Número de nós avaliados:  11600\n",
      "Numero de labels no dataset:  2771\n",
      "--------------------------\n",
      "Número de nós avaliados:  11700\n",
      "Numero de labels no dataset:  2773\n",
      "--------------------------\n",
      "Número de nós avaliados:  11800\n",
      "Numero de labels no dataset:  2775\n",
      "--------------------------\n",
      "Número de nós avaliados:  11900\n",
      "Numero de labels no dataset:  2777\n",
      "--------------------------\n",
      "Número de nós avaliados:  12000\n",
      "Numero de labels no dataset:  2779\n",
      "--------------------------\n",
      "Número de nós avaliados:  12100\n",
      "Numero de labels no dataset:  2779\n",
      "--------------------------\n",
      "Número de nós avaliados:  12200\n",
      "Numero de labels no dataset:  2783\n",
      "--------------------------\n",
      "Número de nós avaliados:  12300\n",
      "Numero de labels no dataset:  2785\n",
      "--------------------------\n",
      "Número de nós avaliados:  12400\n",
      "Numero de labels no dataset:  2785\n",
      "--------------------------\n",
      "Número de nós avaliados:  12500\n",
      "Numero de labels no dataset:  2788\n",
      "--------------------------\n",
      "Número de nós avaliados:  12600\n",
      "Numero de labels no dataset:  2791\n",
      "--------------------------\n",
      "Número de nós avaliados:  12700\n",
      "Numero de labels no dataset:  2791\n",
      "--------------------------\n",
      "Número de nós avaliados:  12800\n",
      "Numero de labels no dataset:  2791\n",
      "--------------------------\n",
      "Número de nós avaliados:  12900\n",
      "Numero de labels no dataset:  2791\n",
      "--------------------------\n",
      "Número de nós avaliados:  13000\n",
      "Numero de labels no dataset:  2791\n",
      "--------------------------\n",
      "Número de nós avaliados:  13100\n",
      "Numero de labels no dataset:  2791\n",
      "--------------------------\n",
      "Número de nós avaliados:  13200\n",
      "Numero de labels no dataset:  2791\n",
      "--------------------------\n",
      "Número de nós avaliados:  13300\n",
      "Numero de labels no dataset:  2791\n",
      "--------------------------\n",
      "Número de nós avaliados:  13400\n",
      "Numero de labels no dataset:  2791\n",
      "--------------------------\n",
      "Número de nós avaliados:  13500\n",
      "Numero de labels no dataset:  2792\n",
      "--------------------------\n",
      "Número de nós avaliados:  13600\n",
      "Numero de labels no dataset:  2793\n",
      "--------------------------\n",
      "Número de nós avaliados:  13700\n",
      "Numero de labels no dataset:  2793\n",
      "--------------------------\n",
      "Número de nós avaliados:  13800\n",
      "Numero de labels no dataset:  2793\n",
      "--------------------------\n",
      "Número de nós avaliados:  13900\n",
      "Numero de labels no dataset:  2793\n",
      "--------------------------\n",
      "Número de nós avaliados:  14000\n",
      "Numero de labels no dataset:  2794\n",
      "--------------------------\n",
      "Número de nós avaliados:  14100\n",
      "Numero de labels no dataset:  2794\n",
      "--------------------------\n",
      "Número de nós avaliados:  14200\n",
      "Numero de labels no dataset:  2794\n",
      "--------------------------\n",
      "Número de nós avaliados:  14300\n",
      "Numero de labels no dataset:  2798\n",
      "--------------------------\n",
      "Número de nós avaliados:  14400\n",
      "Numero de labels no dataset:  2808\n",
      "--------------------------\n",
      "Número de nós avaliados:  14500\n",
      "Numero de labels no dataset:  2829\n",
      "--------------------------\n",
      "Número de nós avaliados:  14600\n",
      "Numero de labels no dataset:  2893\n",
      "--------------------------\n",
      "Número de nós avaliados:  14700\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  14800\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  14900\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  15000\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  15100\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  15200\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  15300\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  15400\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  15500\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  15600\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  15700\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  15800\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  15900\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  16000\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  16100\n",
      "Numero de labels no dataset:  2895\n",
      "--------------------------\n",
      "Número de nós avaliados:  16200\n",
      "Numero de labels no dataset:  2902\n",
      "--------------------------\n",
      "Número de nós avaliados:  16300\n",
      "Numero de labels no dataset:  2906\n",
      "--------------------------\n",
      "Número de nós avaliados:  16400\n",
      "Numero de labels no dataset:  2907\n",
      "--------------------------\n",
      "Número de nós avaliados:  16500\n",
      "Numero de labels no dataset:  2907\n",
      "--------------------------\n",
      "Número de nós avaliados:  16600\n",
      "Numero de labels no dataset:  2911\n",
      "--------------------------\n",
      "Número de nós avaliados:  16700\n",
      "Numero de labels no dataset:  2912\n",
      "--------------------------\n",
      "Número de nós avaliados:  16800\n",
      "Numero de labels no dataset:  2912\n",
      "--------------------------\n",
      "Número de nós avaliados:  16900\n",
      "Numero de labels no dataset:  2912\n",
      "--------------------------\n",
      "Número de nós avaliados:  17000\n",
      "Numero de labels no dataset:  2912\n",
      "--------------------------\n",
      "Número de nós avaliados:  17100\n",
      "Numero de labels no dataset:  2912\n",
      "--------------------------\n",
      "Número de nós avaliados:  17200\n",
      "Numero de labels no dataset:  2912\n",
      "--------------------------\n",
      "Número de nós avaliados:  17300\n",
      "Numero de labels no dataset:  2912\n",
      "--------------------------\n",
      "Número de nós avaliados:  17400\n",
      "Numero de labels no dataset:  2912\n",
      "--------------------------\n",
      "Número de nós avaliados:  17500\n",
      "Numero de labels no dataset:  2912\n",
      "--------------------------\n",
      "Número de nós avaliados:  17600\n",
      "Numero de labels no dataset:  2912\n",
      "--------------------------\n",
      "Número de nós avaliados:  17700\n",
      "Numero de labels no dataset:  2912\n",
      "--------------------------\n",
      "Número de nós avaliados:  17800\n",
      "Numero de labels no dataset:  2912\n",
      "--------------------------\n",
      "Número de nós avaliados:  17900\n",
      "Numero de labels no dataset:  2914\n",
      "--------------------------\n",
      "Número de nós avaliados:  18000\n",
      "Numero de labels no dataset:  2928\n",
      "--------------------------\n",
      "Número de nós avaliados:  18100\n",
      "Numero de labels no dataset:  2929\n",
      "--------------------------\n",
      "Número de nós avaliados:  18200\n",
      "Numero de labels no dataset:  2930\n",
      "--------------------------\n",
      "Número de nós avaliados:  18300\n",
      "Numero de labels no dataset:  2935\n",
      "--------------------------\n",
      "Número de nós avaliados:  18400\n",
      "Numero de labels no dataset:  2936\n",
      "--------------------------\n",
      "Número de nós avaliados:  18500\n",
      "Numero de labels no dataset:  2936\n",
      "--------------------------\n",
      "Número de nós avaliados:  18600\n",
      "Numero de labels no dataset:  2936\n",
      "--------------------------\n",
      "Número de nós avaliados:  18700\n",
      "Numero de labels no dataset:  2936\n",
      "--------------------------\n",
      "Número de nós avaliados:  18800\n",
      "Numero de labels no dataset:  2936\n",
      "--------------------------\n",
      "Número de nós avaliados:  18900\n",
      "Numero de labels no dataset:  2936\n",
      "--------------------------\n",
      "Número de nós avaliados:  19000\n",
      "Numero de labels no dataset:  2936\n",
      "--------------------------\n",
      "Número de nós avaliados:  19100\n",
      "Numero de labels no dataset:  2936\n",
      "--------------------------\n",
      "Número de nós avaliados:  19200\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  19300\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  19400\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  19500\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  19600\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  19700\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  19800\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  19900\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  20000\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  20100\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  20200\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  20300\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  20400\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  20500\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  20600\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  20700\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  20800\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  20900\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  21000\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  21100\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  21200\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  21300\n",
      "Numero de labels no dataset:  2937\n",
      "--------------------------\n",
      "Número de nós avaliados:  21400\n",
      "Numero de labels no dataset:  2938\n",
      "--------------------------\n",
      "Número de nós avaliados:  21500\n",
      "Numero de labels no dataset:  2938\n",
      "--------------------------\n",
      "Número de nós avaliados:  21600\n",
      "Numero de labels no dataset:  2945\n",
      "--------------------------\n",
      "Número de nós avaliados:  21700\n",
      "Numero de labels no dataset:  2947\n",
      "--------------------------\n",
      "Número de nós avaliados:  21800\n",
      "Numero de labels no dataset:  2948\n",
      "--------------------------\n",
      "Número de nós avaliados:  21900\n",
      "Numero de labels no dataset:  2948\n",
      "--------------------------\n",
      "Número de nós avaliados:  22000\n",
      "Numero de labels no dataset:  2948\n",
      "--------------------------\n",
      "Número de nós avaliados:  22100\n",
      "Numero de labels no dataset:  2948\n",
      "--------------------------\n",
      "Número de nós avaliados:  22200\n",
      "Numero de labels no dataset:  2948\n",
      "--------------------------\n",
      "Número de nós avaliados:  22300\n",
      "Numero de labels no dataset:  2950\n",
      "--------------------------\n",
      "Número de nós avaliados:  22400\n",
      "Numero de labels no dataset:  2950\n",
      "--------------------------\n",
      "Número de nós avaliados:  22500\n",
      "Numero de labels no dataset:  2950\n",
      "--------------------------\n",
      "Número de nós avaliados:  22600\n",
      "Numero de labels no dataset:  2950\n",
      "--------------------------\n",
      "Número de nós avaliados:  22700\n",
      "Numero de labels no dataset:  2951\n",
      "--------------------------\n",
      "Número de nós avaliados:  22800\n",
      "Numero de labels no dataset:  2951\n",
      "--------------------------\n",
      "Número de nós avaliados:  22900\n",
      "Numero de labels no dataset:  2951\n",
      "--------------------------\n",
      "Número de nós avaliados:  23000\n",
      "Numero de labels no dataset:  2951\n",
      "--------------------------\n",
      "Número de nós avaliados:  23100\n",
      "Numero de labels no dataset:  2951\n",
      "--------------------------\n",
      "Número de nós avaliados:  23200\n",
      "Numero de labels no dataset:  2951\n",
      "--------------------------\n",
      "Número de nós avaliados:  23300\n",
      "Numero de labels no dataset:  2951\n",
      "--------------------------\n",
      "Número de nós avaliados:  23400\n",
      "Numero de labels no dataset:  2951\n",
      "--------------------------\n",
      "Número de nós avaliados:  23500\n",
      "Numero de labels no dataset:  2951\n",
      "--------------------------\n",
      "Número de nós avaliados:  23600\n",
      "Numero de labels no dataset:  2956\n",
      "--------------------------\n",
      "Número de nós avaliados:  23700\n",
      "Numero de labels no dataset:  2958\n",
      "--------------------------\n",
      "Número de nós avaliados:  23800\n",
      "Numero de labels no dataset:  2958\n",
      "--------------------------\n",
      "Número de nós avaliados:  23900\n",
      "Numero de labels no dataset:  2971\n",
      "--------------------------\n",
      "Número de nós avaliados:  24000\n",
      "Numero de labels no dataset:  2989\n",
      "--------------------------\n",
      "Número de nós avaliados:  24100\n",
      "Numero de labels no dataset:  2991\n",
      "--------------------------\n",
      "Número de nós avaliados:  24200\n",
      "Numero de labels no dataset:  3000\n",
      "--------------------------\n",
      "Número de nós avaliados:  24300\n",
      "Numero de labels no dataset:  3004\n",
      "--------------------------\n",
      "Número de nós avaliados:  24400\n",
      "Numero de labels no dataset:  3004\n",
      "--------------------------\n",
      "Número de nós avaliados:  24500\n",
      "Numero de labels no dataset:  3004\n",
      "--------------------------\n",
      "Número de nós avaliados:  24600\n",
      "Numero de labels no dataset:  3006\n",
      "--------------------------\n",
      "Número de nós avaliados:  24700\n",
      "Numero de labels no dataset:  3006\n",
      "--------------------------\n",
      "Número de nós avaliados:  24800\n",
      "Numero de labels no dataset:  3016\n",
      "--------------------------\n",
      "Número de nós avaliados:  24900\n",
      "Numero de labels no dataset:  3018\n",
      "--------------------------\n",
      "Número de nós avaliados:  25000\n",
      "Numero de labels no dataset:  3018\n",
      "--------------------------\n",
      "Número de nós avaliados:  25100\n",
      "Numero de labels no dataset:  3018\n",
      "--------------------------\n",
      "Número de nós avaliados:  25200\n",
      "Numero de labels no dataset:  3019\n",
      "--------------------------\n",
      "Número de nós avaliados:  25300\n",
      "Numero de labels no dataset:  3019\n",
      "--------------------------\n",
      "Número de nós avaliados:  25400\n",
      "Numero de labels no dataset:  3020\n",
      "--------------------------\n",
      "Número de nós avaliados:  25500\n",
      "Numero de labels no dataset:  3020\n",
      "--------------------------\n",
      "Número de nós avaliados:  25600\n",
      "Numero de labels no dataset:  3020\n",
      "--------------------------\n",
      "Número de nós avaliados:  25700\n",
      "Numero de labels no dataset:  3020\n",
      "--------------------------\n",
      "Número de nós avaliados:  25800\n",
      "Numero de labels no dataset:  3020\n",
      "--------------------------\n",
      "Número de nós avaliados:  25900\n",
      "Numero de labels no dataset:  3020\n",
      "--------------------------\n",
      "Número de nós avaliados:  26000\n",
      "Numero de labels no dataset:  3020\n",
      "--------------------------\n",
      "Número de nós avaliados:  26100\n",
      "Numero de labels no dataset:  3039\n",
      "--------------------------\n",
      "Número de nós avaliados:  26200\n",
      "Numero de labels no dataset:  3048\n",
      "--------------------------\n",
      "Número de nós avaliados:  26300\n",
      "Numero de labels no dataset:  3049\n",
      "--------------------------\n",
      "Número de nós avaliados:  26400\n",
      "Numero de labels no dataset:  3049\n",
      "--------------------------\n",
      "Número de nós avaliados:  26500\n",
      "Numero de labels no dataset:  3049\n",
      "--------------------------\n",
      "Número de nós avaliados:  26600\n",
      "Numero de labels no dataset:  3051\n",
      "--------------------------\n",
      "Número de nós avaliados:  26700\n",
      "Numero de labels no dataset:  3052\n",
      "--------------------------\n",
      "Número de nós avaliados:  26800\n",
      "Numero de labels no dataset:  3053\n",
      "--------------------------\n",
      "Número de nós avaliados:  26900\n",
      "Numero de labels no dataset:  3054\n",
      "--------------------------\n",
      "Número de nós avaliados:  27000\n",
      "Numero de labels no dataset:  3054\n",
      "--------------------------\n",
      "Número de nós avaliados:  27100\n",
      "Numero de labels no dataset:  3055\n",
      "--------------------------\n",
      "Número de nós avaliados:  27200\n",
      "Numero de labels no dataset:  3056\n",
      "--------------------------\n",
      "Número de nós avaliados:  27300\n",
      "Numero de labels no dataset:  3057\n",
      "--------------------------\n",
      "Número de nós avaliados:  27400\n",
      "Numero de labels no dataset:  3057\n",
      "--------------------------\n",
      "Número de nós avaliados:  27500\n",
      "Numero de labels no dataset:  3057\n",
      "--------------------------\n",
      "Número de nós avaliados:  27600\n",
      "Numero de labels no dataset:  3059\n",
      "--------------------------\n",
      "Número de nós avaliados:  27700\n",
      "Numero de labels no dataset:  3059\n",
      "--------------------------\n",
      "Número de nós avaliados:  27800\n",
      "Numero de labels no dataset:  3059\n",
      "--------------------------\n",
      "Número de nós avaliados:  27900\n",
      "Numero de labels no dataset:  3059\n",
      "--------------------------\n",
      "Número de nós avaliados:  28000\n",
      "Numero de labels no dataset:  3059\n",
      "--------------------------\n",
      "Número de nós avaliados:  28100\n",
      "Numero de labels no dataset:  3059\n",
      "--------------------------\n",
      "Número de nós avaliados:  28200\n",
      "Numero de labels no dataset:  3062\n",
      "--------------------------\n",
      "Número de nós avaliados:  28300\n",
      "Numero de labels no dataset:  3062\n",
      "--------------------------\n",
      "Número de nós avaliados:  28400\n",
      "Numero de labels no dataset:  3063\n",
      "--------------------------\n",
      "Número de nós avaliados:  28500\n",
      "Numero de labels no dataset:  3064\n",
      "--------------------------\n",
      "Número de nós avaliados:  28600\n",
      "Numero de labels no dataset:  3068\n",
      "--------------------------\n",
      "Número de nós avaliados:  28700\n",
      "Numero de labels no dataset:  3068\n",
      "--------------------------\n",
      "Número de nós avaliados:  28800\n",
      "Numero de labels no dataset:  3070\n",
      "--------------------------\n",
      "Número de nós avaliados:  28900\n",
      "Numero de labels no dataset:  3079\n",
      "--------------------------\n",
      "Número de nós avaliados:  29000\n",
      "Numero de labels no dataset:  3082\n",
      "--------------------------\n",
      "Número de nós avaliados:  29100\n",
      "Numero de labels no dataset:  3087\n",
      "--------------------------\n",
      "Número de nós avaliados:  29200\n",
      "Numero de labels no dataset:  3087\n",
      "--------------------------\n",
      "Número de nós avaliados:  29300\n",
      "Numero de labels no dataset:  3087\n",
      "--------------------------\n",
      "Número de nós avaliados:  29400\n",
      "Numero de labels no dataset:  3087\n",
      "--------------------------\n",
      "Número de nós avaliados:  29500\n",
      "Numero de labels no dataset:  3087\n",
      "--------------------------\n",
      "Número de nós avaliados:  29600\n",
      "Numero de labels no dataset:  3091\n",
      "--------------------------\n",
      "Número de nós avaliados:  29700\n",
      "Numero de labels no dataset:  3096\n",
      "--------------------------\n",
      "Número de nós avaliados:  29800\n",
      "Numero de labels no dataset:  3096\n",
      "--------------------------\n",
      "Número de nós avaliados:  29900\n",
      "Numero de labels no dataset:  3096\n",
      "--------------------------\n",
      "Número de nós avaliados:  30000\n",
      "Numero de labels no dataset:  3097\n",
      "--------------------------\n",
      "Número de nós avaliados:  30100\n",
      "Numero de labels no dataset:  3097\n",
      "--------------------------\n",
      "Número de nós avaliados:  30200\n",
      "Numero de labels no dataset:  3098\n",
      "--------------------------\n",
      "Número de nós avaliados:  30300\n",
      "Numero de labels no dataset:  3098\n",
      "--------------------------\n",
      "Número de nós avaliados:  30400\n",
      "Numero de labels no dataset:  3098\n",
      "--------------------------\n",
      "Número de nós avaliados:  30500\n",
      "Numero de labels no dataset:  3098\n",
      "--------------------------\n",
      "Número de nós avaliados:  30600\n",
      "Numero de labels no dataset:  3098\n",
      "--------------------------\n",
      "Número de nós avaliados:  30700\n",
      "Numero de labels no dataset:  3098\n",
      "--------------------------\n",
      "Número de nós avaliados:  30800\n",
      "Numero de labels no dataset:  3101\n",
      "--------------------------\n",
      "Número de nós avaliados:  30900\n",
      "Numero de labels no dataset:  3101\n",
      "--------------------------\n",
      "Número de nós avaliados:  31000\n",
      "Numero de labels no dataset:  3102\n",
      "--------------------------\n",
      "Número de nós avaliados:  31100\n",
      "Numero de labels no dataset:  3103\n",
      "--------------------------\n",
      "Número de nós avaliados:  31200\n",
      "Numero de labels no dataset:  3105\n",
      "--------------------------\n",
      "Número de nós avaliados:  31300\n",
      "Numero de labels no dataset:  3105\n",
      "--------------------------\n",
      "Número de nós avaliados:  31400\n",
      "Numero de labels no dataset:  3105\n",
      "--------------------------\n",
      "Número de nós avaliados:  31500\n",
      "Numero de labels no dataset:  3108\n",
      "--------------------------\n",
      "Número de nós avaliados:  31600\n",
      "Numero de labels no dataset:  3108\n",
      "--------------------------\n",
      "Número de nós avaliados:  31700\n",
      "Numero de labels no dataset:  3108\n",
      "--------------------------\n",
      "Número de nós avaliados:  31800\n",
      "Numero de labels no dataset:  3108\n",
      "--------------------------\n",
      "Número de nós avaliados:  31900\n",
      "Numero de labels no dataset:  3109\n",
      "--------------------------\n",
      "Número de nós avaliados:  32000\n",
      "Numero de labels no dataset:  3112\n",
      "--------------------------\n",
      "Número de nós avaliados:  32100\n",
      "Numero de labels no dataset:  3114\n",
      "--------------------------\n",
      "Número de nós avaliados:  32200\n",
      "Numero de labels no dataset:  3122\n",
      "--------------------------\n",
      "Número de nós avaliados:  32300\n",
      "Numero de labels no dataset:  3123\n",
      "--------------------------\n",
      "Número de nós avaliados:  32400\n",
      "Numero de labels no dataset:  3123\n",
      "--------------------------\n",
      "Número de nós avaliados:  32500\n",
      "Numero de labels no dataset:  3123\n",
      "--------------------------\n",
      "Número de nós avaliados:  32600\n",
      "Numero de labels no dataset:  3123\n",
      "--------------------------\n",
      "Número de nós avaliados:  32700\n",
      "Numero de labels no dataset:  3124\n",
      "--------------------------\n",
      "Número de nós avaliados:  32800\n",
      "Numero de labels no dataset:  3124\n",
      "--------------------------\n",
      "Número de nós avaliados:  32900\n",
      "Numero de labels no dataset:  3126\n",
      "--------------------------\n",
      "Número de nós avaliados:  33000\n",
      "Numero de labels no dataset:  3134\n",
      "--------------------------\n",
      "Número de nós avaliados:  33100\n",
      "Numero de labels no dataset:  3135\n",
      "--------------------------\n",
      "Número de nós avaliados:  33200\n",
      "Numero de labels no dataset:  3135\n",
      "--------------------------\n",
      "Número de nós avaliados:  33300\n",
      "Numero de labels no dataset:  3139\n",
      "--------------------------\n",
      "Número de nós avaliados:  33400\n",
      "Numero de labels no dataset:  3143\n",
      "--------------------------\n",
      "Número de nós avaliados:  33500\n",
      "Numero de labels no dataset:  3143\n",
      "--------------------------\n",
      "Número de nós avaliados:  33600\n",
      "Numero de labels no dataset:  3144\n",
      "--------------------------\n",
      "Número de nós avaliados:  33700\n",
      "Numero de labels no dataset:  3147\n",
      "--------------------------\n",
      "Número de nós avaliados:  33800\n",
      "Numero de labels no dataset:  3148\n",
      "--------------------------\n",
      "Número de nós avaliados:  33900\n",
      "Numero de labels no dataset:  3149\n",
      "--------------------------\n",
      "Número de nós avaliados:  34000\n",
      "Numero de labels no dataset:  3151\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "# Unir em uma lista as classes e indivíduos\n",
    "nodes = classes + individuals\n",
    "\n",
    "# Número de negative easy e hard para cada positive label encontrada no positive sample. E número de repetições do positive sample.\n",
    "p = 15\n",
    "n_easy = 5\n",
    "n_hard = 15\n",
    "\n",
    "# Lista para receber o dataset com labels e vetores\n",
    "node_dataset_labels = {}\n",
    "node_dataset_vectors = {}\n",
    "\n",
    "i = 0\n",
    "k = 0\n",
    "\n",
    "# Iterando pelos nós do grafo OWL amostrando os positive e negative sample\n",
    "for n in nodes:\n",
    "    i = i + 1\n",
    "\n",
    "\n",
    "    dataset_labels = {'URI':'',\n",
    "                      'positive':[],\n",
    "                      'negative_easy':[],\n",
    "                      'negative_hard':[]}\n",
    "    dataset_vectors = {'URI':'',\n",
    "                       'URI_label':'',\n",
    "                       'positive':[],\n",
    "                       'negative_easy':[],\n",
    "                       'negative_hard':[]}\n",
    "    \n",
    "    for s in positive_sample(n):\n",
    "        \n",
    "        dataset_labels['URI'] = s[0]\n",
    "        dataset_vectors['URI'] = s[0]\n",
    "        dataset_vectors['URI_label'] = s[1]\n",
    "\n",
    "        for npos in range(p):\n",
    "            dataset_labels['positive'] = dataset_labels['positive'] + [s[2]]\n",
    "            dataset_vectors['positive'] = dataset_vectors['positive'] + [s[3]]\n",
    "\n",
    "        for ne in range(n_easy):\n",
    "            neg_easy = negative_sample_easy(n)\n",
    "            dataset_labels['negative_easy'] = dataset_labels['negative_easy'] + [neg_easy[2]]\n",
    "            dataset_vectors['negative_easy'] = dataset_vectors['negative_easy'] + [neg_easy[3]]\n",
    "\n",
    "    for ns in negative_sample_hard(set(dataset_labels['positive']), n_hard):\n",
    "        dataset_labels['negative_hard'] = dataset_labels['negative_hard'] + [ns[0]]\n",
    "        dataset_vectors['negative_hard'] = dataset_vectors['negative_hard'] + [ns[1]]\n",
    "        \n",
    "    if dataset_labels['URI'] != '' and dataset_labels['negative_hard'] !=  []:\n",
    "        k = k + 1\n",
    "        node_dataset_labels[k] = dataset_labels\n",
    "        node_dataset_vectors[k] = dataset_vectors\n",
    "    \n",
    "    if i%100 == 0:                    \n",
    "        print('Número de nós avaliados: ', i)\n",
    "        print('Numero de labels no dataset: ', k)\n",
    "        print('--------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c18b8-b4c1-4221-8806-d3bf4d4eee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Número de nós avaliados:  34000\n",
    "Numero de labels no dataset:  3123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcbf5edb-b83b-4148-91c2-6f2a46373e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando e carregando dataset_labels\n",
    "with open(\"dataset_7_labels.bin\", \"wb\") as output:\n",
    "    pickle.dump(node_dataset_labels, output)\n",
    "    \n",
    "with open(\"dataset_7_labels.bin\", \"rb\") as data:\n",
    "    node_dataset_labels = pickle.load(data)\n",
    "    \n",
    "# Salvando e carregando dataset_vectors\n",
    "with open(\"dataset_7_vectors.bin\", \"wb\") as output:\n",
    "    pickle.dump(node_dataset_vectors, output)\n",
    "    \n",
    "with open(\"dataset_7_vectors.bin\", \"rb\") as data:\n",
    "    node_dataset_vectors = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ed02151-1ae0-4810-aaa6-da55448b3ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'URI': 'http://www.semanticweb.org/bg40/ontologies/2022/5/untitled-ontology-2#anhydrite',\n",
       " 'positive': ['anidrita',\n",
       "  'anidrita',\n",
       "  'anidrita',\n",
       "  'anidrita',\n",
       "  'anidrita',\n",
       "  'anidrita',\n",
       "  'anidrita',\n",
       "  'anidrita',\n",
       "  'anidrita',\n",
       "  'anidrita',\n",
       "  'anidrita',\n",
       "  'anidrita',\n",
       "  'anidrita',\n",
       "  'anidrita',\n",
       "  'anidrita'],\n",
       " 'negative_easy': ['skenea', 'ugn', 'encontrada', 'absoraao', 'cid33'],\n",
       " 'negative_hard': ['halita',\n",
       "  'gipsita',\n",
       "  'taquidrita',\n",
       "  'evaporito',\n",
       "  'carnalita',\n",
       "  'marga',\n",
       "  'pseudomorfos',\n",
       "  'calcario',\n",
       "  'dolomito',\n",
       "  'silvinita',\n",
       "  'bandada',\n",
       "  'nodular',\n",
       "  'membro_itaunas',\n",
       "  'concrecoes',\n",
       "  'criptocristalina']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_dataset_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e7699-0663-401f-8141-69533070d664",
   "metadata": {},
   "source": [
    "### Dividindo conjunto de treino, validação e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "957b3ebf-6d4a-421a-b5aa-4c48f45424a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificando os ID dos nós e eliminando duplicados. Dessa forma,\n",
    "#evitamos que label de um mesmo nó fiquem ao mesmo tempo no conjunto de treino e\n",
    "# nos conjuntos de validação ou teste.\n",
    "\n",
    "nodes = list(node_dataset_labels.keys())\n",
    "random.shuffle(nodes)\n",
    "\n",
    "# Divisão da base em treino, validação e teste\n",
    "p_train = 0.8\n",
    "p_valid = 0.0\n",
    "p_test = 1 - p_train - p_valid\n",
    "\n",
    "n_train = nodes[0:int(len(nodes) * p_train)]\n",
    "n_valid = nodes[int(len(nodes) * p_train) : int(len(nodes) * (p_train + p_valid))]\n",
    "n_test = nodes[int(len(nodes) * (p_train + p_valid)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d6a60d4-18ea-4cec-96b3-48a288744c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos iterar por todos os nós das bases de treino, validação e teste, para montar o datasets. \n",
    "# Os datasets serão compostos por pares de URI e label, com a respectiva indicação de se é um positive (1) ou negative (0)sample \n",
    "\n",
    "def criar_datasets(n_dataset):\n",
    "    \n",
    "    # Listas contendo positive(1)/negative(0), URI e label \n",
    "    labels = []\n",
    "    # Vetor da URI e vetor da label concatenada em uma única lista\n",
    "    vectors = []\n",
    "    # Valor target (0 ou 1) para os algoritmos de classificação \n",
    "    target = []\n",
    "    \n",
    "    for n in n_dataset:\n",
    "        \n",
    "        #Positive samples\n",
    "        for positive in node_dataset_labels[n]['positive']:\n",
    "            labels.append([1, node_dataset_labels[n]['URI'], positive])\n",
    "            \n",
    "        for positive_vector in node_dataset_vectors[n]['positive']:\n",
    "            vectors.append(list(np.concatenate((node_dataset_vectors[n]['URI_label'],\n",
    "                                                positive_vector), axis=None)))\n",
    "            #vectors.append([1, node_dataset_vectors[n]['URI_label'], positive_vector])\n",
    "            target.append(1)\n",
    "            \n",
    "        # Negative easy\n",
    "        for ne in node_dataset_labels[n]['negative_easy']:\n",
    "            labels.append([0, node_dataset_labels[n]['URI'], ne])\n",
    "            \n",
    "        for ne_vector in node_dataset_vectors[n]['negative_easy']:\n",
    "            vectors.append(list(np.concatenate((node_dataset_vectors[n]['URI_label'],\n",
    "                                                ne_vector), axis=None)))\n",
    "            #vectors.append([0, node_dataset_vectors[n]['URI_label'], ne_vector])\n",
    "            target.append(0)\n",
    "        \n",
    "        # Negative hard\n",
    "        for nh in node_dataset_labels[n]['negative_hard']:\n",
    "            labels.append([0, node_dataset_labels[n]['URI'], nh])\n",
    "            \n",
    "        for nh_vector in node_dataset_vectors[n]['negative_hard']:\n",
    "            vectors.append(list(np.concatenate((node_dataset_vectors[n]['URI_label'],\n",
    "                                                nh_vector), axis=None)))\n",
    "            #vectors.append([0, node_dataset_vectors[n]['URI_label'], nh_vector])\n",
    "            target.append(0)\n",
    "    \n",
    "    # Embaralhando as listas (mas mantendo a mesma ordem entre labels, vectors e target)\n",
    "    \n",
    "    return_shuffle = list(zip(labels, vectors, target))\n",
    "    random.shuffle(return_shuffle)\n",
    "    labels, vectors, target = zip(*return_shuffle)\n",
    "    \n",
    "    return(list(labels), list(vectors), list(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e4ce3e1-b240-4ee0-a47f-38a9fcf10631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os datasets\n",
    "train_labels, train_X, train_Y = criar_datasets(n_train)\n",
    "#valid_labels, valid_X, valid_Y = criar_datasets(n_valid)\n",
    "test_labels, test_X, test_Y = criar_datasets(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5d14a9a-8b10-4cd1-bb9a-1034a0a80b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de amostras para treino:  86643\n",
      "Quantidade de amostras para teste:  21911\n"
     ]
    }
   ],
   "source": [
    "print('Quantidade de amostras para treino: ', len(train_labels))\n",
    "#print('Quantidade de amostras para validação: ', len(valid_labels))\n",
    "print('Quantidade de amostras para teste: ', len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5846d65f-8a54-476a-9b73-40f2f26af93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino:\n",
      "Positive sample:  42075\n",
      "Negative easy sample:  14025.0\n",
      "Negative hard sample:  30543.0\n",
      "\n",
      " Teste:\n",
      "Positive sample:  10635\n",
      "Negative easy sample:  3545.0\n",
      "Negative hard sample:  7731.0\n"
     ]
    }
   ],
   "source": [
    "print('Treino:')\n",
    "\n",
    "ps = sum(train_Y)\n",
    "nes = ps/p * n_easy\n",
    "nhs = len(train_Y) - ps- nes\n",
    "print('Positive sample: ', ps)\n",
    "print('Negative easy sample: ', nes)\n",
    "print('Negative hard sample: ', nhs)\n",
    "\n",
    "#print('\\n Validação:')\n",
    "\n",
    "#ps = sum(valid_Y)\n",
    "#nes = ps/p * n_easy\n",
    "#nhs = len(valid_Y) - ps- nes\n",
    "#print('Positive sample: ', ps)\n",
    "#print('Negative easy sample: ', nes)\n",
    "#print('Negative hard sample: ', nhs)\n",
    "\n",
    "print('\\n Teste:')\n",
    "\n",
    "ps = sum(test_Y)\n",
    "nes = ps/p * n_easy\n",
    "nhs = len(test_Y) - ps- nes\n",
    "print('Positive sample: ', ps)\n",
    "print('Negative easy sample: ', nes)\n",
    "print('Negative hard sample: ', nhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c687cd-0fe7-4536-8f05-16f9da71095e",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Dataset 6**  \n",
    "2 positive sample, 1 negative easy, 1 negative hard  \n",
    "Quantidade de amostras para treino:  11116  \n",
    "Quantidade de amostras para validação:  0  \n",
    "Quantidade de amostras para teste:  2791  \n",
    "W2V - 100 dimensões  \n",
    "Owl2V - 100 dimensões  \n",
    "  \n",
    "Treino:  \n",
    "Positive sample:  5618  \n",
    "Negative easy sample:  2809  \n",
    "Negative hard sample:  2689  \n",
    "  \n",
    " Validação:  \n",
    "Positive sample:  0  \n",
    "Negative easy sample:  0  \n",
    "Negative hard sample:  0  \n",
    "  \n",
    " Teste:  \n",
    "Positive sample:  1410  \n",
    "Negative easy sample:  705  \n",
    "Negative hard sample:  676  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e6c2e6-2e6b-4255-ae11-47fa43f01936",
   "metadata": {},
   "source": [
    "### Treinando um modelo de aprendizado de máquina para identificar se um termo é label de uma URI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44431fc4-74fe-4612-a5ae-a59424d6017c",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39ce789e-ec76-4893-ba1f-7ac090be73d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC_clf = svm.SVC(probability=True)\n",
    "SVC_clf.fit(train_X, train_Y)\n",
    "SVC_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec0fe517-92f4-450a-8cf9-8d35d72974e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia:  0.6106065446579344\n",
      "F1 Score:  0.5302279484638256\n",
      "Precision (0-1):  [0.59538376 0.63969709]\n",
      "Recall (0-1):  [0.75948918 0.45275035]\n",
      "F1 (0-1):  [0.66749805 0.53022795]\n"
     ]
    }
   ],
   "source": [
    "test_pred = SVC_clf.predict(test_X)\n",
    "print(\"Acurácia: \", accuracy_score(test_Y, test_pred))\n",
    "prf = precision_recall_fscore_support(test_Y, test_pred, average=None)\n",
    "print(\"F1 Score: \", prf[2][1])\n",
    "print(\"Precision (0-1): \", prf[0])\n",
    "print(\"Recall (0-1): \", prf[1])\n",
    "print(\"F1 (0-1): \", prf[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f20342-74d4-441e-ae36-7580ada5fc46",
   "metadata": {},
   "source": [
    "**Dataset 6**  \n",
    "train easy and hard - test easy and hard  \n",
    "Acurácia: 0.6649946255822285      \n",
    "F1 Score: 0.687813021702838     \n",
    "\n",
    "train easy and hard - test easy  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  \n",
    "train easy and hard - test hard  \n",
    "Acurácia:    \n",
    "F1 Score:     \n",
    "  \n",
    "train hard - test hard  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  \n",
    "train hard - test easy and hard  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c31429-63e5-4f5d-8241-498ddd8f73fb",
   "metadata": {},
   "source": [
    "Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5ddeee6-9119-4d39-a4cf-72ffbdae8a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, early_stopping=True, hidden_layer_sizes=(5, 2),\n",
       "              max_iter=10000, random_state=1, solver='lbfgs')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLP_clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "MLP_clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, early_stopping=True, max_iter=10000)\n",
    "MLP_clf.fit(train_X, train_Y)\n",
    "MLP_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8dc9b29-73d6-475a-b805-a8a3f978ac1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia:  0.5741864816758706\n",
      "F1 Score:  0.6687965921192759\n",
      "Precision (0-1):  [0.72234918 0.53721129]\n",
      "Recall (0-1):  [0.2803299  0.88575458]\n",
      "F1 (0-1):  [0.40391004 0.66879659]\n"
     ]
    }
   ],
   "source": [
    "test_pred = MLP_clf.predict(test_X)\n",
    "print(\"Acurácia: \", accuracy_score(test_Y, test_pred))\n",
    "prf = precision_recall_fscore_support(test_Y, test_pred, average=None)\n",
    "print(\"F1 Score: \", prf[2][1])\n",
    "print(\"Precision (0-1): \", prf[0])\n",
    "print(\"Recall (0-1): \", prf[1])\n",
    "print(\"F1 (0-1): \", prf[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044eec8-1576-4329-8244-edfce734dd48",
   "metadata": {},
   "source": [
    "**Dataset 6**   \n",
    "train easy and hard - test easy and hard  \n",
    "Acurácia: 0.6671443926907918   \n",
    "F1 Score: 0.7315804680728113   \n",
    "  \n",
    "train easy and hard - test easy  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  \n",
    "train easy and hard - test hard  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  \n",
    "train hard - test hard  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  \n",
    "train hard - test easy and hard  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeec111-8826-4ec9-822f-0c3cb6898ef0",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7537822-f82e-4dc2-9c9b-bcb6a6733790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_clf = RandomForestClassifier(n_estimators=100)\n",
    "RF_clf.fit(train_X, train_Y)\n",
    "RF_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95810143-5e96-4aa3-ac8b-a5e5afc66b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia:  0.5171831500159737\n",
      "F1 Score:  0.011216001495466865\n",
      "Precision (0-1):  [0.51595185 0.9375    ]\n",
      "Recall (0-1):  [0.99964526 0.00564175]\n",
      "F1 (0-1):  [0.68061468 0.011216  ]\n"
     ]
    }
   ],
   "source": [
    "test_pred = RF_clf.predict(test_X)\n",
    "print(\"Acurácia: \", accuracy_score(test_Y, test_pred))\n",
    "prf = precision_recall_fscore_support(test_Y, test_pred, average=None)\n",
    "print(\"F1 Score: \", prf[2][1])\n",
    "print(\"Precision (0-1): \", prf[0])\n",
    "print(\"Recall (0-1): \", prf[1])\n",
    "print(\"F1 (0-1): \", prf[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ddc3c3-684c-4670-8df7-6c4c9c0e2c91",
   "metadata": {},
   "source": [
    "**Dataset 5**   \n",
    "train easy and hard - test easy and hard  \n",
    "Acurácia:  0.5775707631673236  \n",
    "F1 Score:  0.462870159453303  \n",
    "  \n",
    "train easy and hard - test easy  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  \n",
    "train easy and hard - test hard  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  \n",
    "train hard - test hard  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  \n",
    "train hard - test easy and hard  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a11e9-8253-4088-9a8d-e3bcfa319baf",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6934f9a7-dee6-4dc8-a045-c8f44d429b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0, solver='liblinear')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_clf = LogisticRegression(solver='liblinear', random_state=0)\n",
    "LR_clf.fit(train_X, train_Y)\n",
    "LR_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db700f99-9607-4699-83e7-b429cd0e8724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia:  0.5893386883300625\n",
      "F1 Score:  0.5556104306598182\n",
      "Precision (0-1):  [0.59261669 0.58514512]\n",
      "Recall (0-1):  [0.64632849 0.52891396]\n",
      "F1 (0-1):  [0.61830831 0.55561043]\n"
     ]
    }
   ],
   "source": [
    "test_pred = LR_clf.predict(test_X)\n",
    "print(\"Acurácia: \", accuracy_score(test_Y, test_pred))\n",
    "prf = precision_recall_fscore_support(test_Y, test_pred, average=None)\n",
    "print(\"F1 Score: \", prf[2][1])\n",
    "print(\"Precision (0-1): \", prf[0])\n",
    "print(\"Recall (0-1): \", prf[1])\n",
    "print(\"F1 (0-1): \", prf[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9e3408-c938-453d-b71c-908a94c188ae",
   "metadata": {},
   "source": [
    "**Dataset 5**  \n",
    "train easy and hard - test easy and hard  \n",
    "Acurácia: 0.6176997491938373     \n",
    "F1 Score: 0.6231013776050865     \n",
    "  \n",
    "train easy and hard - test easy  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  \n",
    "train easy and hard - test hard  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  \n",
    "train hard - test hard  \n",
    "Acurácia:    \n",
    "F1 Score:    \n",
    "  \n",
    "train hard - test easy and hard  \n",
    "Acurácia:    \n",
    "F1 Score:    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b6ad4-2eef-4286-8de0-6a6ffcce2191",
   "metadata": {},
   "source": [
    "### Próximos passos para melhorar o classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66863d40-9032-4433-a400-2d824eaba558",
   "metadata": {},
   "source": [
    "- Retreinar PetroVec considerando todas as labels da Ontologia como MWE\n",
    "- Testar tamanhos diferentes (provavelmente menores) dos vetores PetroVec e OWL2Vec\n",
    "- Testar outros modelos de vetorização de grafos (RDF2Vec, ...)\n",
    "- Testar uma quantidade maior de negative samples, principalmente o hard samples, e estratégia de data augmentation\n",
    "    \n",
    "- Criar dataset onde os casos de positive, negative hard e negative easy samples não sejam necessáriamente fixos por nó..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1649e2f-25d8-4c4f-b268-d5fef9205c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b57b2-245d-43a1-a2d7-2141ea091591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9786ae-0b75-4a37-be6f-6213c5ae41b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649991a7-7803-491e-b89b-b8e5525916ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a765c-3a7c-4ea7-8a4d-082148e8cea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
